<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AI 入门知识</title>
    <url>/2023/035.html</url>
    <content><![CDATA[<blockquote>
<p>学习心得：计算机视觉之算法基础与 OpenMMLab 入门<br>注：基于子豪兄的讲解，我重点发散了计算机视觉之算法基础的相关部分，主要参考的是我现有的笔记</p>
</blockquote>
<h2 id="机器学习的本质"><a href="#机器学习的本质" class="headerlink" title="机器学习的本质"></a>机器学习的本质</h2><h3 id="简单问题的学习"><a href="#简单问题的学习" class="headerlink" title="简单问题的学习"></a>简单问题的学习</h3><p>简单问题的学习过程是寻求最拟合方程的过程（以不同面积房价预测为例）</p>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://img-blog.csdnimg.cn/img_convert/c4ceecf25a107672517273f0a0dcd821.jpeg">
        
      </span></p>
<p>其中 $w$、$b$ 是 Learnable，代表了其对于预测精度的决定性，也代表了它是后期得到的，经优化确定之后模型就有了最好的预测能力</p>
<h3 id="非结构化问题的学习"><a href="#非结构化问题的学习" class="headerlink" title="非结构化问题的学习"></a>非结构化问题的学习</h3><ul>
<li>所习得的转换为多层非线性变换，实现对 DATA 的高层抽象</li>
<li>DATA 输入也会更复杂，需要更多数据预处理操作</li>
</ul>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://i.imgur.com/PqCAyzV.jpg">
        
      </span></p>
<h3 id="HIDEN-LAYER的CELL"><a href="#HIDEN-LAYER的CELL" class="headerlink" title="HIDEN LAYER的CELL"></a>HIDEN LAYER的CELL</h3><ul>
<li>在多层感知机中，隐层由图示的CELL组建而成，每一个 CELL 接受上一层的多个 CELL 输入（$S_n$），经过 Activation（激活函数，此处以 ReLU 为例），得到该 CELL 对下一层的输出（$S^{\prime}$）</li>
</ul>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://i.imgur.com/jxVB8Ba.jpg">
        
      </span></p>
<ul>
<li>Learnable：$w1$、$w2$… $wn$；$bias$（可去掉）</li>
<li>$o=\sum(ws)+bias$</li>
<li>$S^{\prime}=Activation(o)$</li>
</ul>
<h2 id="CV的基础任务"><a href="#CV的基础任务" class="headerlink" title="CV的基础任务"></a>CV的基础任务</h2><h3 id="非像素级"><a href="#非像素级" class="headerlink" title="非像素级"></a>非像素级</h3><h4 id="IMAGE-CLASSIFICTIOIN（分类）"><a href="#IMAGE-CLASSIFICTIOIN（分类）" class="headerlink" title="IMAGE CLASSIFICTIOIN（分类）"></a>IMAGE CLASSIFICTIOIN（分类）</h4><ul>
<li>一张图像中是否包含某种物体，<strong>对图像进行类别描述</strong>是 Image Classification 的主要研究内容</li>
</ul>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://pic2.zhimg.com/80/v2-ff27c83b868490fab2a9a309279e14cd_720w.webp">
        
      </span></p>
<ul>
<li>经典 CNN：AlexNet（2012），在其之后，有很多基于CNN的算法也在 ImageNet 上取得了特别好的成绩，比如 GoogleNet（2014）、VGGNet（2014）、ResNet（2015）以及 DenseNet（2016）等</li>
<li>常用公共数据集（数据复杂度递增）：<ul>
<li><strong>MNIST</strong>：60k 训练图像、10k 测试图像、10个类别、图像大小1×28×28、内容是0-9手写数字</li>
<li><strong>CIFAR-10</strong>：50k 训练图像、10k 测试图像、10个类别、图像大小3×32×32</li>
<li><strong>CIFAR-100</strong>：50k 训练图像、10k 测试图像、100个类别、图像大小3×32×32</li>
<li><strong>ImageNet</strong>：1.2M 训练图像、50k 验证图像、1k 个类别。2017年及之前，每年会举行基于 ImageNet 数据集的 ILSVRC 竞赛</li>
</ul>
</li>
</ul>
<h4 id="LOCALIZATION（定位）"><a href="#LOCALIZATION（定位）" class="headerlink" title="LOCALIZATION（定位）"></a>LOCALIZATION（定位）</h4><ul>
<li>在 Image Classification 的基础上，还想知道图像中的<strong>单个</strong>主体对象具体在图像的什么位置，通常是以包围盒（bounding box）的形式。网络带有<strong>两个输出头</strong>。一个分支用于做图像分类，另一个分支用于判断目标位置，即输出四个数字标记包围盒位置（例如中心点横纵坐标和包围盒长宽），<strong>该分支输出结果只有在分类分支判断不为“背景”时才使用</strong></li>
</ul>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://pic2.zhimg.com/80/v2-590292930c3e06a9dac9c3dcb798e33d_720w.webp">
        
      </span></p>
<h4 id="OBJECT-DETECTION（检测）"><a href="#OBJECT-DETECTION（检测）" class="headerlink" title="OBJECT DETECTION（检测）"></a>OBJECT DETECTION（检测）</h4><p>
        <span class="lazyload-img-span">
        <img data-src="https://pic1.zhimg.com/80/v2-e175e2ddab083943f9b07c59c72f6180_720w.webp">
        
      </span></p>
<ul>
<li>Object Detection 通常是从图像中输出<strong>多个</strong>目标的 Bounding Box 以及类别，同时完成了 Image Classification 和 Localization 。在 Localization 中，通常只有<strong>一个</strong>目标，而目标检测更一般化，其图像中出现的目标种类和数目都不定。近年来，目标检测研究趋势主要向更快、更有效的检测系统发展</li>
<li>经典算法：<ul>
<li><strong>two-stage</strong>：R-CNN（第一个高效模型）、Fast R-CNN、Faster R-CNN、R-FCN 等；</li>
<li><strong>one-stage</strong>：YOLO、SSD 等</li>
</ul>
</li>
<li>PASCAL VOC 包含20个类别。通常是用 VOC07 和 VOC12 的 trainval 并集作为训练，用 VOC07 的测试集作为测试。</li>
<li>常用公共数据集（数据复杂度递增）：<ul>
<li><strong>PASCAL VOC</strong>：20个类别。通常是用 VOC07 和 VOC12 的 trainval 并集作为训练，用 VOC07 的测试集作为测试</li>
<li><strong>MS COCO</strong>：COCO 比 VOC 更困难。80k 训练图像、40k 验证图像、20k 没有公开标记的测试图像(test-dev)，80个类别。通常是用80k 训练和35k 验证图像的并集作为训练，其余5k 图像作为验证，20k 测试图像用于线上测试</li>
</ul>
</li>
</ul>
<h3 id="像素级-细粒度级"><a href="#像素级-细粒度级" class="headerlink" title="像素级/细粒度级"></a>像素级/细粒度级</h3><h4 id="SEGMENTATION（分割）"><a href="#SEGMENTATION（分割）" class="headerlink" title="SEGMENTATION（分割）"></a>SEGMENTATION（分割）</h4><p>分割任务是将整个图像分成像素组，然后对其进行标记和分类，难度上比非像素级更大，特征更加复杂</p>
<h5 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h5><ul>
<li>语义分割试图在语义上理解图像中每个像素在<strong>大类</strong>上的从属（例如天空、汽车、摩托车等）。</li>
<li>基本思路：<ul>
<li><strong>二分类</strong>：我们将图像输入模型，得到和图像一样长宽的输出，其输出为单通道<strong>概率图</strong>，每个像素代表其属于第二类的可能性，进行二值化得到分割结果</li>
<li><strong>多分类</strong>：我们将图像输入模型，得到和图像一样长宽的输出，其输出为<strong>多通道</strong>，每个通道代表不同类别，本质是给每个类别一张二值图以得到多类分割结果</li>
</ul>
</li>
<li>经典算法：FCN（全卷积神经网络）、UNet、PSPNet、DeepLabV3 系列、UPerNet 等</li>
<li>常用公共数据集比较杂，涉及了遥感、医学影像、自动驾驶等各个专业和领域，体量庞大且专业性强，这里不做展示</li>
</ul>
<h5 id="实例分割"><a href="#实例分割" class="headerlink" title="实例分割"></a>实例分割</h5><ul>
<li>和语义分割的本质区别在于，语义分割是得到在大类上的从属关系，实例分割进一步区分<strong>大类中不同实体间的区别</strong>。比如，如果一群人打排球，语义分割和实例分割都会将其分割结果归类为「人」，但是语义分割的分割结果是一个大多边形把人都包起来（图1），而实例分割会给每个人一个多边形包起来（图2）</li>
</ul>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://pic1.zhimg.com/80/v2-f4a5453a5f9b93752a10d3e6beff39d8_720w.webp">
        
      </span></p>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://pic1.zhimg.com/80/v2-6167a115b5a01196e68e394cee412754_720w.webp">
        
      </span></p>
<h4 id="关键点检测"><a href="#关键点检测" class="headerlink" title="关键点检测"></a>关键点检测</h4><ul>
<li>提取分析对象的关键点，例如人脸的关键点有眼珠、眼角、鼻尖、嘴尖、下颚转折点等等，通过提取这些点的二维坐标就可以得到大概的线状、面状分布特征，如果能够提取三位点坐标，则可以引入深度特征，实现更加复杂的应用</li>
</ul>
<h3 id="为何分类是最基础的任务？"><a href="#为何分类是最基础的任务？" class="headerlink" title="为何分类是最基础的任务？"></a>为何分类是最基础的任务？</h3><ul>
<li>基于分类算法，可以在后面连接到各种其他任务的算法（例如检测、分割等），或者与其他算法头并行。总之，如果没有了分类，检测器的包围框将没有实际的参考价值，分割器的像素之间划分开的差异也没有实际意义，所以分类算法是一切人工智能算法的基础，分类领域的突破是下游任务发展的重要推动力</li>
</ul>
<h2 id="DEEPLEARNING算法的设计"><a href="#DEEPLEARNING算法的设计" class="headerlink" title="DEEPLEARNING算法的设计"></a>DEEPLEARNING算法的设计</h2><ul>
<li>朴素点说，深度学习模型的隐层可以专心负责特征提取，输入层可以任意输入各种模态的数据。声音数据可以经过滤波算法以及可视化算法转化成彩图和视觉算法共用输入层、语言数据可以切分为 Seg 序列，映射为一维 feature vector 作为 Token 输入 Transformer 算法……所以，DL 算法本质上是支持多模态的</li>
<li>但单纯将各种模态数据映射为统一的输入结构，容易损失数据特征，因此需要针对分析对象的性质和关键特征来设计模型的输入层，以及隐层中的特征提取方式</li>
<li>而视频和音频数据相比于单帧数据，带有时序特征，在设计上如何考虑多帧之间的时序关系是非常重要的，因此难度也要更高。常见的应用有：<ul>
<li><strong>片段引导</strong>：划分视频片段，并对应某类片段的受众，引导受众跳转到感兴趣片段</li>
<li><strong>片段查询</strong>：根据用户的自然语言描述，截取出符合描述条件的片段</li>
</ul>
</li>
</ul>
<h2 id="子豪兄总结的新颖且有前景的研究领域"><a href="#子豪兄总结的新颖且有前景的研究领域" class="headerlink" title="子豪兄总结的新颖且有前景的研究领域"></a>子豪兄总结的新颖且有前景的研究领域</h2><ul>
<li><strong>可解释性分析、显著性分析</strong>（<strong>兴趣排名No.3</strong>）</li>
<li><strong>图机器学习、图神经网络</strong>（<strong>兴趣排名No.1</strong>）</li>
<li>人工智能+VR AR 元宇宙</li>
<li><strong>轻量化压缩部署</strong>（<strong>兴趣排名No.2</strong>）</li>
<li>各行各业垂直细分应用</li>
<li>NERF</li>
<li>Diffusion</li>
<li>隐私计算、联邦学习、可信计算</li>
<li>AI 基础设施平台</li>
<li>预训练大模型</li>
</ul>
]]></content>
      <categories>
        <category>Computer Vision 计算机视觉</category>
      </categories>
  </entry>
  <entry>
    <title>GIS的应用</title>
    <url>/2021/0740930.html</url>
    <content><![CDATA[<h1 id="输入（蓝）、处理（橙）、输出（绿）："><a href="#输入（蓝）、处理（橙）、输出（绿）：" class="headerlink" title="输入（蓝）、处理（橙）、输出（绿）："></a>输入（蓝）、处理（橙）、输出（绿）：</h1><p>输入处理输出是最基本最简化的应用逻辑</p>
<p>
        <span class="lazyload-img-span">
        <img data-src="GIS%E7%9A%84%E5%BA%94%E7%94%A8/%E5%9B%BE1.png">
        
      </span></p>
<h1 id="GIS模型-具体行业模型（集成）"><a href="#GIS模型-具体行业模型（集成）" class="headerlink" title="GIS模型+具体行业模型（集成）"></a>GIS模型+具体行业模型（集成）</h1><ul>
<li>举例复杂应用：水务行业的排水管网 SWMM 模型（排水管网属于 GIS 模型，SWMM 属于税务行业模型）<ul>
<li>对应 GISer 的思想：<ul>
<li>给排水管建模</li>
<li>分层：根据用户需求分析得到的设备类型抽象图层：检查井、管线、汇水区……</li>
<li>抽象：<ul>
<li>检查井，点类型：坐标、高程……</li>
<li>管线，线类型：上下游井、埋深、管长、管径……<br>  □ 汇水区，面类型：面积……</li>
</ul>
</li>
</ul>
</li>
<li>针对思想的复杂应用构建：<ul>
<li>输入：<ul>
<li>检查井：坐标、高程……&lt;&lt;DEM 高程提取</li>
<li>管线：上下游井、埋深、管长、管径……&lt;&lt;上下游网络拓扑</li>
<li>汇水区：面积……&lt;&lt;小流域 Basin 划分工具</li>
</ul>
</li>
<li>处理：<ul>
<li>SWMM 模型引擎（和 GIS 关系不大，主要来源于行业模型）</li>
</ul>
</li>
<li>输出（行业用户对应需求）：<ul>
<li>检查井的水位变化序列的动态专题渲染</li>
<li>管线流量、充满度变化序列的动态专题渲染</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>GIS Experiments</category>
      </categories>
  </entry>
  <entry>
    <title>MMLAB PR 指南</title>
    <url>/2023/034.html</url>
    <content><![CDATA[<blockquote>
<p>ref：<br>贡献指南：<a href="https://mmengine.readthedocs.io/zh_CN/latest/notes/contributing.html">https://mmengine.readthedocs.io/zh_CN/latest/notes/contributing.html</a><br>代码规范：<a href="https://mmengine.readthedocs.io/zh_CN/latest/notes/code_style.html">https://mmengine.readthedocs.io/zh_CN/latest/notes/code_style.html</a></p>
</blockquote>
<h2 id="PR-描述规范"><a href="#PR-描述规范" class="headerlink" title="PR 描述规范"></a>PR 描述规范</h2><h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><ul>
<li><strong>sample</strong>： [Docs] Refine contribute.md</li>
<li>在开头使用英文括号描述修改的对象，常见修改对象有<table>
<thead>
<tr>
<th>对象</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td>Docs</td>
<td align="left">官方文档更新 可以是refine也可以补充</td>
</tr>
<tr>
<td>Feature</td>
<td align="left">新功能 新功能support对xxx的支持</td>
</tr>
<tr>
<td>Fix</td>
<td align="left">修复bug</td>
</tr>
<tr>
<td>WIP</td>
<td align="left">先提出来 等待开发完成 暂时不用review</td>
</tr>
</tbody></table>
</li>
</ul>
<h2 id="REFINE-DOCs"><a href="#REFINE-DOCs" class="headerlink" title="REFINE DOCs"></a>REFINE DOCs</h2><h3 id="代码快-to-提示框"><a href="#代码快-to-提示框" class="headerlink" title="代码快 to 提示框"></a>代码快 to 提示框</h3><ul>
<li>基于代码快的语法，通过特殊的标注实现代码快到提示框的转换<ul>
<li>注解{Note}<br>
        <span class="lazyload-img-span">
        <img data-src="https://i.imgur.com/mqtDx3A.jpg">
        
      </span></li>
<li>参见{SeeAlso}<br>
        <span class="lazyload-img-span">
        <img data-src="https://i.imgur.com/oNjmsqz.jpg">
        
      </span></li>
<li>警告{Warning}<br>
        <span class="lazyload-img-span">
        <img data-src="https://i.imgur.com/eJxjNl5.jpg">
        
      </span></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>For OpenMMLab</category>
      </categories>
  </entry>
  <entry>
    <title>一键锁定目标的检测算法</title>
    <url>/2023/031.html</url>
    <content><![CDATA[<blockquote>
<p>学习心得：计算机视觉之目标检测算法基础 注：基于 OpenMMLab 实战营的教授内容，做了有用的补充，会缺少一部分课上的内容</p>
</blockquote>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://picx.zhimg.com/80/v2-a447ea83ea84fcad99c59dab0634d7d7_720w.png?source=d16d100b">
        
      </span></p>
<h2 id="常用数据集"><a href="#常用数据集" class="headerlink" title="常用数据集"></a>常用数据集</h2><ul>
<li><strong>PASCAL VOC</strong>：20个类别。通常是用 VOC07 和 VOC12 的 trainval 并集作为训练，用 VOC07 的测试集作为测试</li>
<li><strong>MS COCO</strong>：COCO 比 VOC 更困难。80k 训练图像、40k 验证图像、20k 没有公开标记的测试图像(test-dev)，80个类别。通常是用80k 训练和35k 验证图像的并集作为训练，其余5k 图像作为验证，20k 测试图像用于线上测试</li>
</ul>
<h2 id="常用精度指标"><a href="#常用精度指标" class="headerlink" title="常用精度指标"></a>常用精度指标</h2><ul>
<li><strong>mAP (mean average precision)</strong> ：目标检测中的常用评价指标，计算方法如下。当预测的包围盒和真实包围盒的交并比大于某一阈值(通常为0.5)，则认为该预测正确。对每个类别，我们画出它的查准率-查全率(precision-recall)曲线，平均准确率是曲线下的面积。之后再对所有类别的平均准确率求平均，即可得到 mAP，其取值为[0, 100%]</li>
<li>**交并比(intersection over union, IoU)**：算法预测的包围盒和真实包围盒交集的面积除以这两个包围盒并集的面积，取值为[0, 1]。交并比度量了算法预测的包围盒和真实包围盒的接近程度，交并比越大，两个包围盒的重叠程度越高</li>
</ul>
<h2 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h2><ul>
<li>Object Detection 通常是从图像中输出多个目标的Bounding Box以及类别，同时完成了 Image Classification 和 Localization。在 Localization中，通常只有一个目标，而目标检测更一般化，其图像中出现的目标种类和数目都不定。近年来，目标检测研究趋势主要向更快、更有效的检测系统发展。目前检测算法主要分为 Two-Stage 和 One-Stage 两种算法。前者，先生成很多可能候选区，然后再对所有候选区进行分类和校准；后者，不生成各种候选区直接给出检测结果。前者以 R-CNN 为代表，后者以 YOLO 和 SSD 为典型</li>
</ul>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://picx.zhimg.com/80/v2-6260e70fbe1b5727ac2507dfad1834e0_720w.png?source=d16d100b">
        
      </span></p>
<h2 id="关于检测窗口的选择"><a href="#关于检测窗口的选择" class="headerlink" title="关于检测窗口的选择"></a>关于检测窗口的选择</h2><h3 id="滑动窗口法"><a href="#滑动窗口法" class="headerlink" title="滑动窗口法"></a>滑动窗口法</h3><ul>
<li>步骤：<ol>
<li>生成不同大小的窗口，在图片上设置步长滑动</li>
<li>每次滑动对窗口执行分类器，若概率超过某个阈值则认为检测到了物体，生成候选框</li>
<li>最后，在不同大小窗口生成的检测结果中，经过非极大值抑制（NMS）的方法进行筛选，确定最终检测到的物体</li>
</ol>
</li>
<li>缺点：滑动一次就要分类一次，效率太低了，缺少应用价值</li>
<li>优化方案：不在输入数据上滑窗，而是在尺寸更小单信息跟丰富的特征图上滑窗</li>
</ul>
<h3 id="SELECTIVE-SEARCH-选择性搜索"><a href="#SELECTIVE-SEARCH-选择性搜索" class="headerlink" title="SELECTIVE SEARCH 选择性搜索"></a>SELECTIVE SEARCH 选择性搜索</h3><ul>
<li>步骤：<ol>
<li>基于滑动窗口法，更改了候选框的搜索方法</li>
<li>首先对图像做分割算法，产生很多的子区域</li>
<li>然后根据子区域的相似性，如颜色、纹理、尺寸等，进行区域的合并，不断迭代下去</li>
<li>每次迭代的时候对不同子区域生成外接矩形，也就是提议框</li>
<li>最后同滑动窗口法</li>
</ol>
</li>
<li>缺点：效率依然不够高，时间成本太大</li>
<li>优化方案：PRN</li>
</ul>
<h3 id="RPN-区域候选网络"><a href="#RPN-区域候选网络" class="headerlink" title="RPN 区域候选网络"></a>RPN 区域候选网络</h3><ul>
<li>步骤：<ol>
<li>输入图像通过卷积神经网络，得到特征图</li>
<li>基于特征图运行，对于每个滑动窗口，生成一组特定的 Anchor（锚）</li>
<li>可能有很多盒子里没有任何物体，模型需要学习哪些 Anchor 可能有对象</li>
<li>Anchor 的定位和分类由回归层和分类器完成</li>
</ol>
</li>
</ul>
<h2 id="经典-TWO-STAGE算法"><a href="#经典-TWO-STAGE算法" class="headerlink" title="经典 TWO-STAGE算法"></a>经典 TWO-STAGE算法</h2><h3 id="R-CNN（SELECTIVE-SEARCH-CNN-SVM）"><a href="#R-CNN（SELECTIVE-SEARCH-CNN-SVM）" class="headerlink" title="R-CNN（SELECTIVE SEARCH + CNN + SVM）"></a>R-CNN（SELECTIVE SEARCH + CNN + SVM）</h3><ul>
<li>步骤：<ol>
<li>首先，对输入图像采用 Selective Search 生成大概1~2k 个候选框</li>
<li>每个候选区投入 CNN 提取特征（神经网络中卷积层不需要固定的出入尺寸，但全连接层需要，所以传统投入网络的时候可能会做crop裁切或者 warp 拉伸等操作）</li>
<li>特征送入 SVM 分类器判断分类</li>
<li>最后使用回归器对候选框位置进行精修</li>
</ol>
</li>
</ul>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://pic1.zhimg.com/80/v2-27ce1521962b06714fbecfbd40b733e0_720w.png?source=d16d100b">
        
      </span></p>
<ul>
<li>缺点<ul>
<li>重复的计算。虽然 R-CNN 不是在穷举，但是候选框太多，计算量依然会很大，而且候选框之间往往有大量重叠部分</li>
<li>训练很麻烦，候选区提取、分类、回归都是单独的代码块，需要分开运行，中间的数据还要单独存储</li>
</ul>
</li>
</ul>
<h3 id="SPP-NET（ROI-POOLING，SPP-空间金字塔池化）"><a href="#SPP-NET（ROI-POOLING，SPP-空间金字塔池化）" class="headerlink" title="SPP-NET（ROI POOLING，SPP 空间金字塔池化）"></a>SPP-NET（ROI POOLING，SPP 空间金字塔池化）</h3><ul>
<li>作者何凯明注意到，传统网络中由于全连接层的存在，通常要固定网络输入的尺寸，这种方法往往造成空间信息的损失和部分特征的放大或扭曲，于是他在卷积层的结尾创造了 SPP 空间金字塔池化。物体检测精度确实有所提高</li>
</ul>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://pic1.zhimg.com/80/v2-07e04149737cbfb59e0a52fb3e9a1ddf_720w.png?source=d16d100b">
        
      </span></p>
<ul>
<li>这一层对卷积层的输出做多种固定的池化，无论输入尺寸如何最后都会按照固定的比率池化，比如 W，H 变换为 W/n，H/n，于是就确保了全连接层接受的数量不变</li>
</ul>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://pica.zhimg.com/80/v2-ad4e42bec28dd18461e0de4a6a156c93_720w.png?source=d16d100b">
        
      </span></p>
<ul>
<li>步骤：<ol>
<li>首先通过 Selective Search 生成1~2k 个候选框，这一步和 R-CNN 一样</li>
<li>特征提取阶段就是和最大的区别了，由于 SPP 一次就可以对整张图片做全面的特征提取，所以效率大大增加</li>
<li>最后几步也适合 R-CNN 一样</li>
</ol>
</li>
<li>缺点同 R-CNN 类似</li>
</ul>
<h3 id="FAST-R-CNN（SELECTIVE-SEARCH-CNN-ROI）"><a href="#FAST-R-CNN（SELECTIVE-SEARCH-CNN-ROI）" class="headerlink" title="FAST R-CNN（SELECTIVE SEARCH + CNN + ROI）"></a>FAST R-CNN（SELECTIVE SEARCH + CNN + ROI）</h3><p>
        <span class="lazyload-img-span">
        <img data-src="https://picx.zhimg.com/80/v2-35dd28eb799ef2af85d99e0468671fcf_720w.png?source=d16d100b">
        
      </span></p>
<ul>
<li>传统 R-CNN 要生成不同尺寸滑动窗下的分类结果，因此每次投入分类器的都是固定的某一尺寸，需要对视窗进行 Resize 或 Warp。ROI pooling 就能避免这个问题</li>
<li>其过程和 SPP 的一部分类似，首先生将 region proposal 候选框划分为 H * W 的网格</li>
<li>对每个网格进行 MaxPooling，形成 H * W 大小的 feature maps。其优点就是提高了处理速度</li>
<li>Fast R-CNN 提出了多任务损失函数，将分类器损失和边框回归损失放在一起统一训练，最终输出对应的分类和边框坐标</li>
</ul>
<h3 id="R-FCN"><a href="#R-FCN" class="headerlink" title="R-FCN"></a>R-FCN</h3><p>
        <span class="lazyload-img-span">
        <img data-src="https://picx.zhimg.com/80/v2-c603f134d541bd4bd5facf0b4ebc8cf9_720w.png?source=d16d100b">
        
      </span></p>
<ul>
<li>R-FCN 是 Faster-RCNN 的改进，速度得到了很大提升，但精度提升不大。主要解决的问题是位置敏感性：一个图片在任何方向稍微经过裁切，都可以被分类器认出，然而检测任务不希望出现这种位置上的偏差。R-FCN 提出了 Position-sensitive score maps 位置敏感网络层来解决这个问题</li>
<li>位置敏感网络层维度 k * k * （C + 1），k 一般等于3，构成能够表达敏感位置（左上，正上，右上，正左，正中，正右，左下，正下，右下）的 Grid 格网编码结构</li>
</ul>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://picx.zhimg.com/80/v2-291ac6728f65eaf667750ce345d0b06f_720w.png?source=d16d100b">
        
      </span></p>
<h2 id="经典One-Stage算法"><a href="#经典One-Stage算法" class="headerlink" title="经典One-Stage算法"></a>经典One-Stage算法</h2><h3 id="YoLoV1"><a href="#YoLoV1" class="headerlink" title="YoLoV1"></a>YoLoV1</h3><p>
        <span class="lazyload-img-span">
        <img data-src="https://pica.zhimg.com/80/v2-9b24e325b437fb2440a66f63684cc96e_720w.png?source=d16d100b">
        
      <br>​</span></p>
<ul>
<li>步骤：<ol>
<li>输入一个图像，YoLoV1 会把图像看成一个 s * s 的栅格格网，如图中 s = 7 ，若某个物体的 ground truth 中心落在某个栅格，则这个栅格要负责该物体的预测</li>
<li>对于上述每个栅格，要预测回归两个 bounding box 的坐标及其含有对象的置信度，同时预测物体所属类别</li>
<li>根据上一步可以预测出的目标窗口，然后根据阈值去除可能性比较低的目标窗口，然后根据 NMS 去除冗余窗口</li>
<li>YoLoV1 最后的输出是7x7x30，这里是7x7代表输入图像的7x7栅格，维数30的前十个代表2个 bounding boxes， 每个 bounding box 要预测5个值</li>
</ol>
</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>YoLo 对相互靠得很近的物体、过于细小的物体检测效果不好，这是因为一个栅格只预测两个 bounding box，并且只属于一个类别</li>
<li>测试时，如果某类物体出现了奇葩的长宽比或者奇怪的角度时，泛化能力显著下降</li>
<li>由于损失函数的问题，定位误差是影响检测效果的主要原因，尤其是对于大物体小物体的处理还待加强</li>
</ul>
<h3 id="SSD（Single-shot-MultiBox-Detector）"><a href="#SSD（Single-shot-MultiBox-Detector）" class="headerlink" title="SSD（Single shot MultiBox Detector）"></a>SSD（Single shot MultiBox Detector）</h3><p>
        <span class="lazyload-img-span">
        <img data-src="https://picx.zhimg.com/80/v2-f0776451ff869de7aad947961279878b_720w.png?source=d16d100b">
        
      </span></p>
<ul>
<li>Single shot 表明了 SSD 和 YoLo 一样属于 One-Stage 算法，MultiBox 表明了 SSD 可以检测多个物体 </li>
</ul>
<h3 id="主要改进"><a href="#主要改进" class="headerlink" title="主要改进"></a>主要改进</h3><ul>
<li>使用了类似 RPN 中的 Anchor 锚点机制，增加 bounding box 多样性。卷积输出的 feature map，每个点对应为原图的一个区域的中心点。以这个点为中心，构造出6个宽高比例不同，大小不同的 Anchor（SSD 中称为 default box）。每个 anchor 对应4个位置参数(x,y,w,h)和21个类别概率</li>
</ul>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://pic1.zhimg.com/80/v2-c2197c05bb2a8598f0a3d52141cee802_720w.png?source=d16d100b">
        
      </span></p>
<ul>
<li>使用全卷积网络，效率得到提高</li>
<li>网络中间会生成多个阶段不同感受野的 Feature Maps 丰富特征多样性，因此 SSD 能够在不同感受野下进行不同的目标检测，实现多尺度预测，克服了 YoLo 在大物体小物体上的缺陷</li>
<li>步骤：<ol>
<li>卷积层。借鉴了 VGG16，惯用先经过 CNN 获得特征图，再进行定位和分类的方法。</li>
<li>目标检测层。这一层由五个卷积层和一个平均池化层构成。因为 SSD 认为目标检测中的物体只与周围信息相关，感受野不是全局的，所以去掉了全连接层</li>
<li>筛选层。与 YoLo 基本一致，先过滤类别概率低于阈值的 default box，再采用NMS筛掉重叠度较高的。不过，SSD 综合了各种 feature maps 上的 default box </li>
</ol>
</li>
<li>SSD 基本可以满足手机端的实时物体检测需求了，许多框架比如 Tensorflow 基于 MobileNet 在移动设备就使用了SSD算法实现。</li>
</ul>
<h2 id="近期受关注算法"><a href="#近期受关注算法" class="headerlink" title="近期受关注算法"></a>近期受关注算法</h2><h3 id="FCOS"><a href="#FCOS" class="headerlink" title="FCOS"></a>FCOS</h3><ul>
<li>FCOS 不使用 Anchor，而且整个结构都是纯粹的 CNN。先预测特征图上的各像素类别，再预测各点的 bbox 的大小和位置。</li>
<li>对于 Ambiguity 问题，FCOS 利用 FPN 和 Center Sampling 来解决。同时创新了 Center-Ness 分支帮助 NMS 抑制低质量框</li>
</ul>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://picx.zhimg.com/80/v2-fabf74e5cf48be22623183f9028c6a37_720w.png?source=d16d100b">
        
      </span></p>
<h3 id="CENTERNET"><a href="#CENTERNET" class="headerlink" title="CENTERNET"></a>CENTERNET</h3><ul>
<li>CenterNet 将检测任务的思路从以点为核心转换到以框为核心</li>
<li>基本思路：<ol>
<li>Resnet50 提取图像特征得到特征图</li>
<li>经过反卷积模块，三次上采样</li>
<li>输入三个分支进行预测，得到 Heatmap、预测框尺寸和中心点偏移量</li>
</ol>
</li>
</ul>
<h3 id="DETR-and-DEFORMABLE-DETR"><a href="#DETR-and-DEFORMABLE-DETR" class="headerlink" title="DETR and DEFORMABLE DETR"></a>DETR and DEFORMABLE DETR</h3><ul>
<li>DETR 第一次系统地考虑将 Transformer 引入图像检测任务，实现了端到端算法，本质是特征序列道框序列的流程</li>
</ul>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://picx.zhimg.com/80/v2-285fa40180bff4eb72d6fac1f1ac278c_720w.png?source=d16d100b">
        
      </span></p>
]]></content>
      <categories>
        <category>Computer Vision 计算机视觉</category>
      </categories>
  </entry>
  <entry>
    <title>写给 mmsegmentation 工具箱新手的避坑指南</title>
    <url>/2023/032.html</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我在 Windows 环境使用 MMClassification、MMDetection 都还算轻轻松松，但是走完 MMSegmentation 全流程之后，真的想感叹一句“踩了不少坑啊”，所以想把自己的遇坑经验凝练总结出来，写一个专门给新手无伤通关的避坑教程。</p>
<h2 id="Windows-配置环境的痛：mmcv-full"><a href="#Windows-配置环境的痛：mmcv-full" class="headerlink" title="Windows 配置环境的痛：mmcv-full"></a>Windows 配置环境的痛：mmcv-full</h2><p>在 v1.4.0 之前，mmcv-full 的安装没有针对 Windows 的现成预编译包，所以大部分新手会卡在 build MMCV 的过程中……这种情况下有两种解决方案。</p>
<h3 id="方案-1：新版编译版本自动安装"><a href="#方案-1：新版编译版本自动安装" class="headerlink" title="方案 1：新版编译版本自动安装"></a>方案 1：新版编译版本自动安装</h3><p>在 1.4.0 之后，MMCV 会跟上 PyTorch 版本更新 <a href="https://zhuanlan.zhihu.com/p/441653536"> Windows 环境下的mmcv-full预编译包</a>，但是可用的版本范围比较局限，依赖 PyTorch、CUDA、mmcv-full 低版本的炼丹师自然就不适合这种安装方式了（看方案2），下面是以 PyTorch1.11.0、 CUDA11.3 为例的安装命令。</p>
<ul>
<li>一句命令安装 <code>mmcv-full</code>，下载速度还是不错的</li>
</ul>
<pre><code class="PowerShell">pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.11/index.html
</code></pre>
<h3 id="方案-2：手动操作"><a href="#方案-2：手动操作" class="headerlink" title="方案 2：手动操作"></a>方案 2：手动操作</h3><p>如果你不希望更新到新版 MMSegmentation 或者 MMCV，也可以尝试手动安装，下面以在 GPU+CPU 双环境运行的目标来安装 <code>mmcv-full</code>，参考了<a href="https://mmcv.readthedocs.io/zh_CN/latest/get_started/build.html#id1">官方文档</a>，所有命令行运行在 <code>powershell</code>，使用 <code>cmd</code> 的炼丹师需要注意两个命令行的命令差异。</p>
<ul>
<li>创建虚拟环境</li>
</ul>
<pre><code class="PowerShell">conda create --name mmcv python=3.7 # 经测试，3.6, 3.7, 3.8 也能通过
conda activate mmcv # 确保做任何操作前先激活环境
</code></pre>
<ul>
<li>进入一个临时文件路径，克隆 <code>mmcv-full</code> 源码</li>
</ul>
<pre><code class="PowerShell">git clone https://github.com/open-mmlab/mmcv.git
cd mmcv # 进入项目文件夹
</code></pre>
<ul>
<li>安装依赖</li>
</ul>
<p>所有依赖中，也安装了 <code>ninja</code> 库用于加快最后编译的速度</p>
<pre><code class="PowerShell">pip install -r requirements.txt
# 建议使用镜像加速 =pip install -r requirements.txt -i https://pypi.douban.com/simple
</code></pre>
<ul>
<li>配置编译环境</li>
</ul>
<p>安装 <code>Microsoft Visual Studio Community 2017/2019/......</code> ，确保环境变量中的 <code>Path</code> 存在编译所需的值。以 VS2019 Community 为例：<code>C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\Hostx86\x64</code> 。</p>
<ul>
<li>编译安装 <code>mmcv-full</code></li>
</ul>
<pre><code class="PowerShell">$env:MMCV_WITH_OPS = 1
$env:MAX_JOBS = 8 # 根据可用的CPU和内存量进行设置
python setup.py build_ext # 如果成功, 将会自动弹出来编译 flow_warp
python setup.py develop # 执行安装
</code></pre>
<ul>
<li>检测是否安装成功</li>
</ul>
<pre><code class="PowerShell">pip list # 使用anaconda的话，也可以在openmmlab依赖的虚拟环境下 =conda list
</code></pre>
<h2 id="数据集自定义类别"><a href="#数据集自定义类别" class="headerlink" title="数据集自定义类别"></a>数据集自定义类别</h2><h3 id="更改-CLASSES-和-num-classes"><a href="#更改-CLASSES-和-num-classes" class="headerlink" title="更改 CLASSES 和 num_classes"></a>更改 CLASSES 和 num_classes</h3><p>不敢调试的新手炼丹师首次面对 <code>mmseg</code> 的项目可能无所适从，因此也很难养成自己编写数据集加载代码的习惯。其实能搜到很多水平不一的资料教你编辑现有的数据集加载方式（比如常见的 <code>ADEDataset</code>），修改 <code>CLASSES</code>，然后设置 <code>num_classes</code>，可能更改完发现编辑后的代码根本没应用上，网络 decoder 不断吐槽你 <code>num_classes</code> 不对，然后你又去检查手里的数据集……其实是因为认识较浅，下面展示更合理的走通指南：</p>
<ul>
<li><p>选择好模型后，先把相关联配置文件里的全部 <code>num_classes</code> 设置好值，比如经典 ADE 数据集提取并划分了 150 个实例类， <code>num_classes</code> 就是 <code>150</code>，计入 <code>num_classes</code> 的所有类的名称下一步都要写入 <code>CLASSES</code>。（背景类未算入 150，下一节会讲解为什么）</p>
</li>
<li><p>下面，进入 <code>mmseg</code> 项目下的 <code>mmseg``/``datasets</code>，以遥感语义分割任务为例新建 py 文件 <code>uavdataset.py</code>， 继承自 <code>custom.py</code> 中的 <code>CustomDataset</code>，然后开始实现自己的数据集……在定义 <code>CLASSES</code>的时候， tuple 初始化为自己类名的集合即可（比如关于街区 block、农田 field 和其他利用地 notused 的遥感语义分割任务），用于上色的 <code>PALETTE</code> 也可以用类似的方式配置（配置格式：[<em>R</em>, <em>G</em>, <em>B</em>]）。</p>
</li>
</ul>
<pre><code class="Python"># mmseg/datasets/uavdataset.py

...
CLASSES = (&#39;block&#39;, &#39;field&#39;, &#39;notused&#39;)

PALETTE = [[120, 120, 120], [180, 120, 120], [120, 180, 120]]
...
</code></pre>
<ul>
<li>然后参考 <code>configs/_base_/datasets</code> 的其他配置文件编写 <code>uavdataset.py</code> 作为 <code>UAVDataset</code> 的配置文件。最后，在选用的模型配置文件中更换数据集加载方式为 <code>UAVDataset</code>。</li>
</ul>
<h3 id="对源码的增改没有效果？"><a href="#对源码的增改没有效果？" class="headerlink" title="对源码的增改没有效果？"></a>对源码的增改没有效果？</h3><ol>
<li>OpenMMLab 各种工具箱的官方文档中，都会教你用 <code>pip install -v -e .</code> 安装项目，但很多新手对 <code>pip</code> 的这种命令并不了解。其实这个命令是用来开启 <code>mmseg</code> 库编辑模式的，这样修改 <code>mmseg</code> 库内的代码片段可以自动被应用上，无需重新安装（如图截取自 MMSegmentation 官方文档的安装教程，其中注释已经解释了这种 pip 命令的含义）。</li>
</ol>
<p>
        <span class="lazyload-img-span">
        <img data-src="https://pic4.zhimg.com/80/v2-3c235f7971fed8a696eddc255d756bf7_720w.webp">
        
      </span></p>
<ul>
<li>切记！使用 <code>python ``setup.py`` install</code> 安装的 <code>mmseg</code> 每次编辑源码都需要重新安装，这也是为什么大部分新手更改 <code>CLASSES</code> 却不生效的原因，建议换用以下安装方法 ：</li>
</ul>
<pre><code class="Shell">cd ~/mmsegmentation-master/ # 进入你的mmseg项目路径下
pip install -v -e . # 重新安装 英文句点表示安装当前路径下的项目
</code></pre>
<ul>
<li>也有可能会有小伙伴问可不可以用 <code>python setup.py develop</code>，我没做过实验。但是 <code>setup.py</code> 也是门学问，既然官方文档教新手们用 <code>pip</code> 的方式就能成，也就没必要找太多替换方案了，新手上来没必要钻研在这上面。</li>
</ul>
<h2 id="独特的数据集参数：reduce-zero-label"><a href="#独特的数据集参数：reduce-zero-label" class="headerlink" title="独特的数据集参数：reduce_zero_label"></a>独特的数据集参数：reduce_zero_label</h2><h3 id="借助-reduce-zero-label-管理-0-值背景"><a href="#借助-reduce-zero-label-管理-0-值背景" class="headerlink" title="借助 reduce_zero_label 管理 0 值背景"></a>借助 reduce_zero_label 管理 0 值背景</h3><p><code>mmseg</code> 中已经为各种公共分割数据集编写了描述文件和加载代码，对于有用过 PyTorch 的小伙伴而言，学习各种数据集的描述文件还是很自如的，只有 <code>reduce_zero_label</code> 对于 <code>mmseg</code> 的新手比较陌生，所以，在搭建自己的 <code>mmseg</code> 数据集时，新手最疑惑的大概就是 <code>reduce_zero_label</code> 到底应该是 <code>True</code> 还是 <code>False</code>。</p>
<p>它有什么用呢？从名字直译过来就是“减少 0 值标签”。在多类分割任务中，如果你的数据集中 <code>0</code> 值作为 label 文件中的背景类别，是建议忽略的。</p>
<p>打开加载数据的源码片段可以看到一段处理 <code>reduce_zero_label</code> 的代码，意思是：若开启了 <code>reduce_zero_label</code>，原本为 <code>0</code> 的所有标注设置为 <code>255</code>，也就是损失函数中 <code>ignore_index</code> 参数的默认值，该参数默认避免值为 <code>255</code> 的标注参与损失计算。前文按下不表的 <code>150</code> 类的 ADE 数据集，它不包含背景的原因就是开了 <code>reduce zero label</code>，原本为 <code>0</code> 值的背景设置为了 <code>ignore_index</code>。</p>
<pre><code class="Python"># mmseg/datasets/pipelines/loading.py

...
# reduce zero_label
if self.reduce_zero_label:
    # avoid using underflow conversion
    gt_semantic_seg[gt_semantic_seg == 0] = 255
    gt_semantic_seg = gt_semantic_seg - 1
    gt_semantic_seg[gt_semantic_seg == 254] = 255
...
</code></pre>
<h3 id="reduce-zero-label-导致的常见问题描述"><a href="#reduce-zero-label-导致的常见问题描述" class="headerlink" title="reduce_zero_label 导致的常见问题描述"></a>reduce_zero_label 导致的常见问题描述</h3><p>我们这里以 <code>ADE</code> 数据集源码为例，<code>reduce_zero_label</code> 默认设置为 <code>True</code>，然而，就算新手掌握了上一节的 <code>reduce_zero_label</code>，也可能对 <code>ADE</code> 了解比较肤浅，会怀疑配置文件中开启的 <code>reduce_zero_label</code> 是不是把 150 个实例类中的第一个给忽略掉了，毕竟 <code>num_classes</code> 不就是 <code>150</code> 吗，然后想当然把 <code>reduce_zero_label</code> 关掉。</p>
<h3 id="错误原因分析"><a href="#错误原因分析" class="headerlink" title="错误原因分析"></a>错误原因分析</h3><pre><code class="R"># configs/_base_/datasets/ade20k.py

train_pipeline = [
    dict(type=&#39;LoadImageFromFile&#39;),
    dict(type=&#39;LoadAnnotations&#39;, reduce_zero_label=True), # ADE中reduce_zero_label默认设置为True
    dict(...),
    ...
]
</code></pre>
<p>label 中实际参加训练的确实只有 <code>150</code> 类，定义在 <code>CLASSES</code> 中，但 label 文件中实际包含了 <code>151</code> 类，而背景类（剩下仍没有标记的，或者被意外忽略的区域都归为背景，在 label 中值为 <code>0</code>）不包含在 <code>150</code> 个 <code>CLASSES</code> 中，需要在训练的时候设置成 <code>ignore_index</code>，所以我们借助上一小节的 <code>reduce_zero_label</code> 将背景从 151 个类中提出来单独设置为了 <code>ignore_index</code>，我们倘若错误地将 <code>reduce_zero_label</code> 关掉了，那 <code>num_classes</code> 就是 <code>151</code> 了。</p>
<h3 id="如何增强对数据集更多参数的理解？"><a href="#如何增强对数据集更多参数的理解？" class="headerlink" title="如何增强对数据集更多参数的理解？"></a>如何增强对数据集更多参数的理解？</h3><p>实际工程中的数据集往往是我们自己设计预测类别和标注规则的，如果背景真的很重要，那无论是修改 ADE 的配置文件，还是硬搬 ADE 格式数据集的使用方式，都不如尊重开发者写好的数据集加载代码，改用自己编写的数据集加载方式（只需继承自 <code>CustomDataset</code> 即可）。</p>
<p>在一行行编写的过程中，新手炼丹师可以不断参考研究现存的其他数据集的解决方案，如果遇到不懂的地方也能有查漏补缺的方向，尤其是 <code>reduce_zero_label</code> 这种参数，需要充分理解消化才能运用自如。不断尝试尝试尝试的过程中，新手炼丹师也会对各式各样的数据集加载方式产生自己的理解和看法，在迎接特殊任务的时候能够分析自己的数据集，创新设计出自己独特的数据集加载方式。</p>
<h2 id="数据集文件后缀的坑：大小写"><a href="#数据集文件后缀的坑：大小写" class="headerlink" title="数据集文件后缀的坑：大小写"></a>数据集文件后缀的坑：大小写</h2><p>接着看 <code>mmseg``/``datasets</code> 的 <code>ade.py</code>，这里 <code>ADE20KDataset</code> 类有两个 suffix（文件后缀）相关的参数配置，<code>img_suffix</code> 负责定义图像文件的后缀名，<code>seg_map_suffix</code> 定义标签文件的后缀名。默认配置：</p>
<pre><code class="Python"># mmseg/datasets/ade.py

...
def __init__(self, **kwargs):
        super(ADE20KDataset, self).__init__(
            img_suffix=&#39;.jpg&#39;, # 图像的后缀名
            seg_map_suffix=&#39;.png&#39;, # 标签的后缀名
            reduce_zero_label=True,
            **kwargs)
        ...
</code></pre>
<p>但是有些炼丹师拿到的图像后缀是 <code>.JPG</code>，它和 <code>.jpg</code> 的区别仅仅是大小写不同，但是数据集加载会不断报 <code>FileNotFound</code> 的错误。所以新手遇到此类报错一定要注意大小写差异，直接修改配置文件中的 suffix 相关参数即可。</p>
<h2 id="日志可视化"><a href="#日志可视化" class="headerlink" title="日志可视化"></a>日志可视化</h2><p>经常可以看到社区的小伙伴在问训练遇到的问题，而且喜欢直接对终端的日志截图，就算是巨佬也不一定对一长串数字敏感。当我遇到这类情况一般会教他们去官方文档找可视化的章节，学习官方提供的绘制日志曲线图的脚本。但是运行脚本可视化是很麻烦的，使用的教程很少还很容易报错，而 <code>tensorboard</code> 可视化库是各工具箱都通用的，可以一句命令可视化训练过程的各种指标，并展示在统一的本地网页上，也给新手提供了更好展现自己训练问题的手段，在 <code>mmseg</code> 使用 <code>tensorboard</code> 的方法也很简单：</p>
<ul>
<li>在 <code>config/_base_</code> 中找到 <code>default_runtime.py</code>，第 6 行一般默认是注释起来的，将这行取消注释也就开启了 tensorboard 记录，以后启动的训练都会在 <code>work_dirs</code> 的对应文件夹中生成 <code>tf_log</code> 文件夹。</li>
</ul>
<pre><code class="Python"># config/_base_/default_runtime.py

# yapf:disable
log_config = dict(
    interval=50,
    hooks=[
        dict(type=&#39;TextLoggerHook&#39;, by_epoch=False),
        dict(type=&#39;TensorboardLoggerHook&#39;) # 启动tensorboard记录（该行一般默认被注释起来）
    ])
# yapf:enable
dist_params = dict(backend=&#39;nccl&#39;)
log_level = &#39;INFO&#39;
load_from = None
resume_from = None
workflow = [(&#39;train&#39;, 1)]
cudnn_benchmark = True
</code></pre>
<ul>
<li>那么这个 <code>tf_log</code> 文件夹怎么使用呢？我们只需要复制绝对路径，打开终端，切换到 OpenMMLab 所依赖的环境，并安装 <code>tensorboard</code> 的 python 库。</li>
</ul>
<pre><code class="Shell">pip install tensorboard
</code></pre>
<ul>
<li>然后将 tensorboard 日志部署到本地 IP 和端口。</li>
</ul>
<pre><code class="Shell">tensorboard --logdir &#123;TF_LOG_PATH&#125; # TF_LOG_PATH替换为自己的tf_log文件夹绝对路径即可
</code></pre>
<ul>
<li>执行成功之后可以看到终端打印了一个本地 IP 和端口，默认是 <code>http://localhost:6006/</code>，按住 <code>ctrl</code> 键鼠标点击即可进入浏览器打开可视化页面，终端连续多次按下 <code>ctrl + c</code> 组合键可以停止 <code>tensorboard</code> 服务。</li>
</ul>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>我使用 OpenMMLab 各种工具箱的时候编写的几个辅助脚本在<a href="https://github.com/TianWen580/myscripts-openmmlab">我的GitHub</a>，涵盖了数据集预处理、维护和质检等功能，大家可以去看看有没有能帮上自己的。 MMSegmentation 的大小坑真的让我哭笑不得哈哈哈，也辛苦 MMSegmentation 开源开发者的付出，祝自己有一天能加入 OpenMMLab 的大家庭一起维护这个开源之星，也祝各位炼丹师实验顺利。</p>
]]></content>
      <categories>
        <category>For OpenMMLab</category>
      </categories>
  </entry>
  <entry>
    <title>地图制图「上」标准制图流程展示</title>
    <url>/2021/0946040.html</url>
    <content><![CDATA[<h1 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h1><h2 id="ArcMap中的添加数据"><a href="#ArcMap中的添加数据" class="headerlink" title="ArcMap中的添加数据"></a>ArcMap中的添加数据</h2><p>在ArcMap默认的软件页面中可以很容易找到「添加文件」按钮，这个是ArcGIS操作逻辑中用来添加某一个已有数据的按钮。单击，打开添加数据窗口。我是用顶部下拉菜单可以选择“catelog”中提前链接好的文件目录，这里我第一次使用，因此要单击顶部「链接文件夹」按钮以添加实验数据所在文件夹。添加完成后，将新荣县1：10000地形图矢量数据，随后可以看到图层管理器已经有了我需要的文件(在ArcMap中图层管理器称为“内容列表”)。<br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8A%E3%80%8D%E6%A0%87%E5%87%86%E5%88%B6%E5%9B%BE%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE1.png">
        
      <br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8A%E3%80%8D%E6%A0%87%E5%87%86%E5%88%B6%E5%9B%BE%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE2.png">
        
      </span></span></p>
<h1 id="编辑与设计"><a href="#编辑与设计" class="headerlink" title="编辑与设计"></a>编辑与设计</h1><h2 id="挑选符号"><a href="#挑选符号" class="headerlink" title="挑选符号"></a>挑选符号</h2><p>内容列表有非常多的点类，现在将他们直接导入进来是初始化符号的，千篇一律。双击其中的地名符号，我发现可以打开符号系统来挑选一些默认的ESRI符号，右边可以更改必要的参数值来调整颜色、大小、形状等等。<br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8A%E3%80%8D%E6%A0%87%E5%87%86%E5%88%B6%E5%9B%BE%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE3.png">
        
      </span></p>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>首先地图的符号之间是分好类别的，必要的区分度需要在符号的选择上体现出来。诚然，默认符号库中的符号可能不足以完成全部的符号更换，但是已经能体现出符号之间的差别了。</p>
<h1 id="布局视图"><a href="#布局视图" class="headerlink" title="布局视图"></a>布局视图</h1><h2 id="打开布局视图"><a href="#打开布局视图" class="headerlink" title="打开布局视图"></a>打开布局视图</h2><p>为了输出我的地图，我需要打开布局视图窗口来编辑版式和其他显示要素。我可以在顶部视图菜单中打开，也可以单击底下非常小的「布局视图」按钮来切换。<br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8A%E3%80%8D%E6%A0%87%E5%87%86%E5%88%B6%E5%9B%BE%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE4.png">
        
      </span></p>
<h2 id="调整布局"><a href="#调整布局" class="headerlink" title="调整布局"></a>调整布局</h2><p>实验指导书说拖动数据框角点以填充工作空间，但是其实可以右键布局区域，选择Distribute-&gt;Fit to Margins来自动完成。完成后更换比例尺为1:10000<br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8A%E3%80%8D%E6%A0%87%E5%87%86%E5%88%B6%E5%9B%BE%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE5.png">
        
      </span></p>
<p>但是考虑到最终出图是一个类似正方形的工作空间，所以我还需要调整页面，如页面大小、页面方向等等，这个可以在顶部文件菜单中找到Page and Print setup窗口来设置。这里我设置为横向视图、宽25cm、高23cm。<br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8A%E3%80%8D%E6%A0%87%E5%87%86%E5%88%B6%E5%9B%BE%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE6.png">
        
      </span></p>
<p>为了给地图添加一个标题，在顶部插入菜单中选择title，在布局视图的顶部合适位置插入，并输入“新荣县”。双击该文本可以进入属性设置，以调整字体、大小等参数。<br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8A%E3%80%8D%E6%A0%87%E5%87%86%E5%88%B6%E5%9B%BE%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE7.png">
        
      </span></p>
<p>顶部插入菜单中还可以插入图例，选择legend设置图例各个位置文本的参数，并调整图例内容。<br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8A%E3%80%8D%E6%A0%87%E5%87%86%E5%88%B6%E5%9B%BE%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE8.png">
        
      </span></p>
<p>这次插入比例尺(scale bar)，在比例尺的候选框中选择合适的比例尺符号，拖动比例尺到地图合适位置。随后在顶部文件菜单点击保存，将本次实验的工作空间保存在实验文件夹下。<br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8A%E3%80%8D%E6%A0%87%E5%87%86%E5%88%B6%E5%9B%BE%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE9.png">
        
      </span></p>
<h1 id="成果"><a href="#成果" class="headerlink" title="成果"></a>成果</h1><p>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8A%E3%80%8D%E6%A0%87%E5%87%86%E5%88%B6%E5%9B%BE%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE10.png">
        
      </span></p>
]]></content>
      <categories>
        <category>GIS Experiments</category>
      </categories>
  </entry>
  <entry>
    <title>地图制图「下」符号设计流程展示</title>
    <url>/2021/0914607.html</url>
    <content><![CDATA[<h1 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h1><h2 id="关于ESRI符号库"><a href="#关于ESRI符号库" class="headerlink" title="关于ESRI符号库"></a>关于ESRI符号库</h2><p>在ArcMap中打开顶部菜单中Customize-&gt;Style Manager工具,在这里面可以看见有黄色和黑色文件夹，都是ArcGIS默认的地理符号库，黄色的是空的，而黑色的包含了ESRI默认自带有的所有符号。如果需要自定义一些符号，我们可以在黄色的文件夹中添加，当然也可以使用新建文件夹的方式在新文件路径下添加自定义符号。<br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8B%E3%80%8D%E7%AC%A6%E5%8F%B7%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE1.png">
        
      </span></p>
<p>点击“Style”按钮可以看见默认的自带符号库还有非常多，我们可以勾选以添加进Style Manager进行显示和查看。点击Create New Style新建一个新的目录用来创建我的自定义符号。<br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8B%E3%80%8D%E7%AC%A6%E5%8F%B7%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE2.png">
        
      </span></p>
<h1 id="创建自定义符号"><a href="#创建自定义符号" class="headerlink" title="创建自定义符号"></a>创建自定义符号</h1><h2 id="建立目录"><a href="#建立目录" class="headerlink" title="建立目录"></a>建立目录</h2><p>点击Style窗口中的Create New Style，选择实验文件夹作为符号库路径，命名为“sy2”。右边双击路径Marker Symbols进入点符号路径。右键，选择new-&gt;Marker Sysbol，接下来进入点符号的自定义界面。</p>
<h2 id="新建Simple-Marker-Symbol符号"><a href="#新建Simple-Marker-Symbol符号" class="headerlink" title="新建Simple Marker Symbol符号"></a>新建Simple Marker Symbol符号</h2><p>顶部的Type中可以选择点符号的定义方法，我们第一个实验任务是创建高程点，形状简单，参数较少，所以此处选择Type：Simple Marker Symbol。在该Type中设置点的大小单位是毫米(Millmeter)，随后设置size:0.5，style：circle保持默认，color：全黑保持默认。单击OK完成编辑，回到Style Manager后将新的符号命名为“高程点”。<br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8B%E3%80%8D%E7%AC%A6%E5%8F%B7%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE3.png">
        
      </span></p>
<h2 id="以新字体的形式新建符号"><a href="#以新字体的形式新建符号" class="headerlink" title="以新字体的形式新建符号"></a>以新字体的形式新建符号</h2><p>ArcMap为了与其他软件共享符号系统，会将符号以字体的形式保存在计算机的内部，用户也可以通过绘制字体来完成复杂符号的自定义。</p>
<h3 id="设置格式"><a href="#设置格式" class="headerlink" title="设置格式"></a>设置格式</h3><p>打开FontCreator应用，顶部文件中选择新建，命名该字体为“地形图”，字符类选择为符号。单击确定。因为字体制作软件和我们的ArcGIS并没有一个等价的单位相互联系，所以第一步我需要设置软件编辑画布的尺寸，让ArcGIS上可以间接转化尺寸。进入顶部的格式-&gt;设置，我们首先可以调整布局的单位。因为10可以与许多数相除而避免产生无穷小数的情况，所以10的倍数都可以作为沟通该软件与ArcGIS软件单位的桥梁，这里10太小，会产生小数点，故考虑使用1000作为布局单位。<br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8B%E3%80%8D%E7%AC%A6%E5%8F%B7%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE4.png">
        
      </span></p>
<p>接下来设置一下度量的参数。字型上行字母、上行字母、Win上升设置为1000，右键的属性中选择预置宽度为1000。这样可以使符号的尺寸在1000且居中的时候可以局限在一个方形区域内。<br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8B%E3%80%8D%E7%AC%A6%E5%8F%B7%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE5.png">
        
      <br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8B%E3%80%8D%E7%AC%A6%E5%8F%B7%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE6.png">
        
      </span></span></p>
<h3 id="绘制符号"><a href="#绘制符号" class="headerlink" title="绘制符号"></a>绘制符号</h3><p>根据实验任务的要求，首先要绘制一个圆形轮廓(直径2mm线宽为默认0.15mm，对应了该软件单位的1000、75)，再绘制一个默认尺寸点(0.3mm对应软件单位的150)。因为该软件只有实心圆，所以需要借助软件的方向功能绘制圆形边界，先绘制(500，500)中心上的尺寸1000<em>1000的大圆，再绘制(500，500)中心上的尺寸850</em>850的小圆，将小圆方向，就形成了尺寸正确的圆形边界，接下来再拖入一个实心圆，绘制于(500，500)中心上，尺寸为150*150。<br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8B%E3%80%8D%E7%AC%A6%E5%8F%B7%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE7.png">
        
      </span></p>
<h3 id="绘制完成"><a href="#绘制完成" class="headerlink" title="绘制完成"></a>绘制完成</h3><p>2.3.3保存字体文件并安装入机<br>顶部文件中点击另存为，选择好保存路径方便查找即可，命名为“地形图”。这样子我可以很方便地完成符号的安装。回到ArcMap，我已经可以右键new-&gt;Marker Symbol来导入字体了。这一次选择type：Character Marker Symbol，加载一段时间后就可以选择上我刚安装的字体文件，里面有我绘制的点符号，选择上，在右上角选择尺寸单位为毫米Millmeter。设置size：2。这样点OK就完成了我电脑上的字符点符号创建。回到Style Manager后将符号命名为“导线控制点”。<br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8B%E3%80%8D%E7%AC%A6%E5%8F%B7%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE8.png">
        
      <br>
        <span class="lazyload-img-span">
        <img data-src="%E5%9C%B0%E5%9B%BE%E5%88%B6%E5%9B%BE%E3%80%8C%E4%B8%8B%E3%80%8D%E7%AC%A6%E5%8F%B7%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B%E5%B1%95%E7%A4%BA/%E5%9B%BE9.png">
        
      </span></span></p>
]]></content>
      <categories>
        <category>GIS Experiments</category>
      </categories>
  </entry>
  <entry>
    <title>实验命令汇总</title>
    <url>/2023/033.html</url>
    <content><![CDATA[<blockquote>
<p>学习心得：MMLab 实战营全程</p>
<p>注：本文总结了实验过程中跨平台通用的技术方案，主要结合了自己的笔记</p>
</blockquote>
<h2 id="ANACONDA-常用命令与一些解决方案"><a href="#ANACONDA-常用命令与一些解决方案" class="headerlink" title="ANACONDA 常用命令与一些解决方案"></a>ANACONDA 常用命令与一些解决方案</h2><h3 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h3><pre><code class="shell">conda create -n your_env_name python=X.X
</code></pre>
<h3 id="更新-conda（慎用！！！，新-conda-可能用不了）"><a href="#更新-conda（慎用！！！，新-conda-可能用不了）" class="headerlink" title="更新 conda（慎用！！！，新 conda 可能用不了）"></a>更新 conda（慎用！！！，新 conda 可能用不了）</h3><pre><code class="shell">conda updata conda
</code></pre>
<h3 id="查看虚拟环境菜单和环境内已载入库"><a href="#查看虚拟环境菜单和环境内已载入库" class="headerlink" title="查看虚拟环境菜单和环境内已载入库"></a>查看虚拟环境菜单和环境内已载入库</h3><pre><code class="shell">conda env list
conda list
</code></pre>
<h3 id="激活虚拟环境"><a href="#激活虚拟环境" class="headerlink" title="激活虚拟环境"></a>激活虚拟环境</h3><pre><code class="shell">Conda activate your_env_name
</code></pre>
<h3 id="如果遇到-conda-安装频繁报错，使用如下语句："><a href="#如果遇到-conda-安装频繁报错，使用如下语句：" class="headerlink" title="如果遇到 conda 安装频繁报错，使用如下语句："></a>如果遇到 conda 安装频繁报错，使用如下语句：</h3><pre><code class="shell">conda clean -i
</code></pre>
<h3 id="如果不幸要删除虚拟环境"><a href="#如果不幸要删除虚拟环境" class="headerlink" title="如果不幸要删除虚拟环境"></a>如果不幸要删除虚拟环境</h3><pre><code class="shell">conda remove -n your_env_name --all
</code></pre>
<h3 id="PyTorch-推荐安装命令"><a href="#PyTorch-推荐安装命令" class="headerlink" title="PyTorch 推荐安装命令"></a>PyTorch 推荐安装命令</h3><pre><code class="Shell">pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 -f https://download.pytorch.org/whl/cu116/torch_stable.html
</code></pre>
<h3 id="我常用的-pip-镜像"><a href="#我常用的-pip-镜像" class="headerlink" title="我常用的 pip 镜像"></a>我常用的 pip 镜像</h3><pre><code class="shell">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple # 清华  
pip install -i https://pypi.douban.com/simple # 豆瓣（推荐）
</code></pre>
<h3 id="安装其他项目的-requirements-txt"><a href="#安装其他项目的-requirements-txt" class="headerlink" title="安装其他项目的 requirements.txt"></a>安装其他项目的 requirements.txt</h3><pre><code class="shell">pip install -r requirements.txt
</code></pre>
<h2 id="TENSORBOARD-可视化"><a href="#TENSORBOARD-可视化" class="headerlink" title="TENSORBOARD 可视化"></a>TENSORBOARD 可视化</h2><ul>
<li>首先学习以下 tensorboardX 怎么用。在 OpenMMLab 中，只需找到 <em>configs/<em>base</em>/default_runtime.py</em> 中的如下代码，解除 <code>dict(type=&#39;TensorboardLoggerHook&#39;)</code> 注释部分即可开启 tensorboard 记录器</li>
</ul>
<pre><code class="python">log_config = dict(  
    interval=50,  
    hooks=[  
        dict(type=&#39;TextLoggerHook&#39;, by_epoch=False),  
        # dict(type=&#39;TensorboardLoggerHook&#39;)  
        # dict(type=&#39;PaviLoggerHook&#39;) # for internal services  
    ])
</code></pre>
<ul>
<li><p>如果遇到环境问题，则按照提示配置 tensorboard 环境即可</p>
</li>
<li><p>一般训练代码运行之后会同时生成 tensorboardX 的日志文件。这时复制日志文件所在文件夹路径，打开 Anaconda 命令行，切换环境至 torch，输入图中语句为日志文件夹创建 tensorboardX 默认的本地端口</p>
<pre><code class="shell">tensorboard --logdir PATH &#123;log_file_abs_path&#125;
</code></pre>
</li>
<li><p>执行得到端口地址，复制到浏览器打开即可查看训练可视化内容<br>
        <span class="lazyload-img-span">
        <img data-src="https://i.imgur.com/fY83Oja.jpg">
        
       
        <span class="lazyload-img-span">
        <img data-src="https://i.imgur.com/omCSC0U.jpg">
        
      </span></span></p>
</li>
<li><p>关闭端口占用，只需短/长按 <em>CTRL + C</em></p>
</li>
</ul>
<h2 id="利用预训练模型"><a href="#利用预训练模型" class="headerlink" title="利用预训练模型"></a>利用预训练模型</h2><ul>
<li>OpenMMLab 几乎为 <em>configs/</em> 中的所有模型提供了预训练模型，链接存放在了各个算法文件夹下的 yaml 文件中，将链接以字符串的形式传给 <em>_configs/<em>base</em>/default_runtime.py</em> 中的 load_from 参数即可</li>
<li>预训练策略：<table>
<thead>
<tr>
<th>待训练数据集</th>
<th>与预训练模型数据集相似度</th>
<th>处理方式</th>
</tr>
</thead>
<tbody><tr>
<td>较小</td>
<td>较高</td>
<td>例如待训练数据集中数据存在于预训练模型中时，不需要重新训练模型，只需要修改最后一层输出层即可</td>
</tr>
<tr>
<td>较小</td>
<td>较小</td>
<td>可以冻结模型的前k层，重新模型的后n-k层。冻结模型的前k层，用于弥补数据集较小的问题</td>
</tr>
<tr>
<td>较大</td>
<td>较高</td>
<td>采用预训练模型会非常有效，保持模型结构不变和初始权重不变，对模型重新训练</td>
</tr>
<tr>
<td>较大</td>
<td>较小</td>
<td>采用预训练模型不会有太大的效果，可以使用预训练模型或者不使用预训练模型，然后进行重新训练</td>
</tr>
</tbody></table>
</li>
</ul>
]]></content>
      <categories>
        <category>Computer Vision 计算机视觉</category>
      </categories>
  </entry>
  <entry>
    <title>美食之美——《雅舍谈吃》</title>
    <url>/2021/0731855.html</url>
    <content><![CDATA[<blockquote>
<p>美，不尽收；食，不尽全。 ——题记</p>
</blockquote>
<ul>
<li>谈到美食，正如饥肠辘辘的人，心里面急迫等待着吃的味觉。但不是所谓，仅仅为着生存。追寻美食的目标，不像一种世俗的冲动，正如雅舍先生题序「我以为要求美味固是人欲，然而何曾有背于天理？如果天理不包括美味的要求在内，上天之人，在舌头上为什么要生那么多的味蕾」。</li>
</ul>
<hr>
<ul>
<li>吃得好，是让人幸福，是对舌尖感受的升华，一旦提到耳熟能详的菜名，人的愉悦便像剑拔弩张，舌尖一触，百般馋舌。左右结构的「馋」字，右边两点比喻两腿迅猛，迅猛狡兔之肉，并非丰足，但人为了啖其美味，愿意逐此狡兔，故写作馋。西施舌、火腿、醋溜鱼、烤羊肉、烧鸭……谁若吃一口，也不至于在脑袋里臆想，两腿一跺，手掌一直，啪的一下，就算有事缠身也无暇顾及。然而，追求高级的味觉，就必然失去点东西，尤其时间宝贵，未必能每次都放纵自己。</li>
<li>今年除夕，我从繁忙学业中抽身，冒着病毒的风险，踉跄到广东家中度过佳节。目的尚且鲜明，写在文章也就印证我所说为了吃美食。老母亲的美味已经不是天天所得，不回一趟家，不吃慈母饭。而明年除夕，已经计划好的美食清单，都在湖南临武，是为了外婆欢庆大寿，为了庆幸生能继续享用湘菜美味。诚然，计划是幸福的计划，我学业若是耽误，便影响我母亲回乡的心情。</li>
<li>学业之中，不代表就缺乏美味。学校前门小吃街，第一条正中间的店面以「炒」为技艺，炒粉丝、炒饭、炒土豆粉、炒米线……每份炒制不少料、不过熟，香味浓郁滑入咽喉。旁边五碗小菜随取，勺筷俱全，好不心爽幸福。校外也有全州拌饭，馋嘴烤鱼、淮南牛肉汤、鸡蛋灌饼、特制酸奶等等玲琅满，如饕餮大餐，本人如数家珍，乐子甚广。</li>
</ul>
<hr>
<ul>
<li>但不是美食就令人幸福的，美食反被美食误，吃得不好，有可能还要怪罪一下美食。</li>
<li>美食为了尝而点，而不是为了点而尝。打开一个外卖软件，铺天盖地的优惠券、广告条、满减促销，全然改变了美食服务的本质。美食不是为了优惠而美，但你看，我想起26元的双层牛肉芝士汉堡，打开外卖软件，又开始送我优惠券，一张是满27减5，一张是满80减9。我心念的汉堡计划被搁置一旁。现在，我购物车换了又换，想法改了又改，点了一家鸡架，满减很高，大份鸡架配油饼、薯条、龙串、千叶只要81，满减后43，优惠券折至34，天啊，真捡大便宜了，一个人，母亲常骂我浪费，也怕生冷不好吃，终于胃饱难入。于是，半个月后的一天，我说：「我心心念念的大汉堡还没吃到呢」，于是又习惯地，打开了外卖软件……</li>
<li>啖美味的人是得爽口，而不是失口德。点到为止，也是嘴的道规。母亲常嘱咐我「什么东西都不能吃多」，但你看，考试结束，我路过一家烤番薯，摊主一旁慢悠悠削菠萝，菠萝酸甜爽口，头脑一昏，要来四根。老板连忙感谢，我心生奇怪。一路上我大快朵颐，直到第三根已然不对劲，牙齿酸疼，满口酥麻。是啊，我知道我吃多了，但是当时人已然傻了，没办法，又已然泛起恶心。</li>
</ul>
<hr>
<ul>
<li>美食之美，若天仙之佳，似陋室之雅。雅舍谈吃，谈世俗之赏，谈高雅之堂。南京翠香阁的早茶、北京全聚德的烧鸭，绝不同于广东潮州的街边烂摊子，不带有地域的歧视，不带有阶层的歧视，雅舍或许能尽情在破烂中寻味潮州，若是「破烂」登上大雅之堂的高级餐席，也不感觉一丝违和。但是，真世上能为高价路边摊买单的人已然不多见，多人不敢斗胆，为了金钱的缺憾。别人跟我谈到吃，我更愿意关注在吃本身上，谈钱，色变。但美食不是阶级之物。凡人也能有凡人的美味，大雅之堂也未必不入凡人；大雅食材未必得凡人认可，没有凡人认可也不称美食。</li>
<li>美，有庐山仙境，有墙头杏花，观不可尽收；食，有满汉全席，有菜汤小食，尝不可尽全。却有人生百态，五味杂陈。文章，有百科全书，有一本便笺，若不是看不完，怎么会藏书如山，孜孜不倦。吃饭，若不是吃不完，怎么会馋如饕餮，感想良多。我写的字，就是把味蕾的感想说出来。可，不是我的味蕾，是我熙熙攘攘间，听闻的许多味蕾……</li>
</ul>
]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
  </entry>
  <entry>
    <title>遥感深度学习学界综合调查</title>
    <url>/2023/0333002.html</url>
    <content><![CDATA[<h2 id="OVERVIEW"><a href="#OVERVIEW" class="headerlink" title="OVERVIEW"></a>OVERVIEW</h2><h3 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h3><ul>
<li>遥感检索</li>
<li>目标检测</li>
<li>地物分类(场景分类 &amp; 语义分割)</li>
<li>变化检测</li>
<li>三维重建</li>
<li>图像描述</li>
<li>图像融合</li>
<li>图像配准</li>
</ul>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><ul>
<li>基于<strong>深度学习</strong>的遥感影像<strong>分类</strong>、<strong>分割</strong>、<strong>检测</strong>、<strong>生成</strong>等</li>
<li>基于<strong>知识图谱</strong>的遥感影像<strong>语义理解</strong>和<strong>推理</strong>等</li>
<li>基于<strong>深度强化学习</strong>的遥感影像<strong>目标跟踪和决策支持</strong>等</li>
<li>基于<strong>生成对抗网络</strong>以及<strong>多模态</strong>的遥感影像<strong>生成</strong>等</li>
<li>基于<strong>亚像素-像素-超像素</strong>的遥感影像<strong>特征提取</strong>、<strong>表征</strong>及<strong>其他常规任务</strong>中的应用</li>
</ul>
<h3 id="价值较高的应用场景"><a href="#价值较高的应用场景" class="headerlink" title="价值较高的应用场景"></a>价值较高的应用场景</h3><ul>
<li>土地利用</li>
<li>城市规划</li>
<li>环境监测 </li>
<li>农业估产</li>
<li>灾害评估和预警</li>
</ul>
<h3 id="面临的挑战"><a href="#面临的挑战" class="headerlink" title="面临的挑战"></a>面临的挑战</h3><ul>
<li>遥感影像数据的<strong>多源性</strong>、<strong>多尺度性</strong>、<strong>多模态性</strong>和<strong>动态性</strong></li>
<li>遥感影像标注数据的<strong>稀缺性</strong>、<strong>不一致性</strong>和<strong>不可靠性</strong></li>
<li>遥感影像语义理解和推理的<strong>复杂性</strong>、<strong>不确定性</strong>和<strong>多样性</strong></li>
<li>遥感影像目标跟踪和决策支持的<strong>实时性</strong>、<strong>可解释性</strong>和<strong>可信赖性</strong>等</li>
</ul>
<h3 id="当前研究热点"><a href="#当前研究热点" class="headerlink" title="当前研究热点"></a>当前研究热点</h3><p>当前研究者投入最多的遥感深度学习任务仍然是<strong>目标检测</strong>和<strong>语义分割</strong>，两个任务占据了近60%的文献数量。主要原因分析有：</p>
<ul>
<li>这两个任务作为 AI 的基础任务，是其他遥感子任务的基础，大多数学者寻求从基础领域突破</li>
<li>这两个任务具有较高的实用价值，并且发展历史最长</li>
<li>这两个任务内容最抽象，看待问题的角度也最复杂，虽然不够新，但是可以做的工作还很多</li>
</ul>
<h2 id="遥感检索"><a href="#遥感检索" class="headerlink" title="遥感检索"></a>遥感检索</h2><h3 id="任务总览"><a href="#任务总览" class="headerlink" title="任务总览"></a>任务总览</h3><table>
<thead>
<tr>
<th>信息</th>
<th>属性</th>
</tr>
</thead>
<tbody><tr>
<td>任务描述</td>
<td>(高级任务)根据自然语言或用户输入的示例图像，检索出所有符合描述的遥感影像或内含主体</td>
</tr>
<tr>
<td>难度</td>
<td>4/5</td>
</tr>
<tr>
<td>研究价值</td>
<td>5/5</td>
</tr>
</tbody></table>
<h3 id="难度分析"><a href="#难度分析" class="headerlink" title="难度分析"></a>难度分析</h3><ul>
<li>遥感检索的过程受到非常多因素共同影响，比如目标的数量关系、类别关系、尺度关系、大小关系、形状属性、方向、姿态、遮挡关系、光照环境、背景等</li>
<li>基于深度学习端到端模型的检索方式能够完全自适应提取数据各种方面的高级抽象特征，所有影像生成各自的表征，然后基于表征由神经网络输出相似度，这种方法在复杂场景下是目前性能最先进的，但同时也是进步空间最大的。<a href="https://kns.cnki.net/kcms2/article/abstract?v=3uoqIhG8C44YLTlOAiTRKu87-SJxoEJu6LL9TJzd50lpJ34nePrutGLwni6THv6wBGPwGNsOMPESgJjxKTV-HsN7aZAnj5yo&uniplatform=NZKPT">传统人工设计特征的检索模式或非端到端的深度学习模式已经失去了优势地位</a></li>
</ul>
<h3 id="研究角度"><a href="#研究角度" class="headerlink" title="研究角度"></a>研究角度</h3><table>
<thead>
<tr>
<th>角度</th>
<th>方法</th>
</tr>
</thead>
<tbody><tr>
<td>更有效的特征表征和深度学习技术</td>
<td>基于注意力机制、对比学习、多模态融合等</td>
</tr>
<tr>
<td>更智能灵活的检索</td>
<td>引入自然语言、各种交互或反馈等</td>
</tr>
<tr>
<td>更适应遥感数据复杂性</td>
<td>基于多源多模态数据、多任务学习、多尺度分析等</td>
</tr>
<tr>
<td>更具挑战性和实用价值的检索任务</td>
<td>基于视频数据、时相数据、地理位置等</td>
</tr>
</tbody></table>
<h3 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a>应用实例</h3><ul>
<li>检索遥感影像中的飞机</li>
<li>在遥感视频中检索发生特定事件的片段或对应地理范围</li>
<li>检索与用户自然语言描述相符的图像</li>
</ul>
<h3 id="研究价值较高的数据集"><a href="#研究价值较高的数据集" class="headerlink" title="研究价值较高的数据集"></a>研究价值较高的数据集</h3><ul>
<li>场景分类的数据集如 UC Merced Land Use Dataset、WHU-RS19、RSSCN7、PatternNet 都可以用在检索任务。作为分类任务，这些数据集精度已经够高了，因此不再介绍</li>
</ul>
<h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><table>
<thead>
<tr>
<th>信息</th>
<th>属性</th>
</tr>
</thead>
<tbody><tr>
<td>任务描述</td>
<td>(基础任务)从遥感影像中定位和识别不同类别的目标，如飞机、桥梁、农作物。为场景理解、变化检测等下游任务提供较精确的特征</td>
</tr>
<tr>
<td>难度</td>
<td>4/5</td>
</tr>
<tr>
<td>研究价值</td>
<td>5/5</td>
</tr>
</tbody></table>
<h3 id="难度分析-1"><a href="#难度分析-1" class="headerlink" title="难度分析"></a>难度分析</h3><ul>
<li>跟遥感检索类似，目标检测所面对的遥感数据也存在非常多的因素</li>
<li>此外，目标检测任务还受到目标过小、类别不平衡等因素影响。目前，深度学习算法在该领域已经取得了非常多成绩，但仍然有各种各样的问题要解决</li>
</ul>
<h3 id="研究角度-1"><a href="#研究角度-1" class="headerlink" title="研究角度"></a>研究角度</h3><table>
<thead>
<tr>
<th>角度</th>
<th>方法</th>
</tr>
</thead>
<tbody><tr>
<td>继续改进端到端的神经网络</td>
<td>基于 Transformer、Residual Block等架构或模块</td>
</tr>
<tr>
<td>降低模型对旋转角度的敏感</td>
<td>利用对旋转敏感的特征学习到任意旋转角度的映射</td>
</tr>
<tr>
<td>降低模型对尺度的敏感</td>
<td>基于多尺度模块或注意力机制来学习多尺度信息或抑制杂乱</td>
</tr>
<tr>
<td>更强大的多分类能力</td>
<td>基于增强的多尺度能力充分利用不同类别的上下文信息</td>
</tr>
</tbody></table>
<h3 id="应用实例-1"><a href="#应用实例-1" class="headerlink" title="应用实例"></a>应用实例</h3><ul>
<li>移动目标探测</li>
<li>受灾检测</li>
<li>海上监测</li>
<li>军事打击</li>
<li>环境监测</li>
</ul>
<h3 id="研究价值较高的数据集-1"><a href="#研究价值较高的数据集-1" class="headerlink" title="研究价值较高的数据集"></a>研究价值较高的数据集</h3><ul>
<li>DOTA<br>这是一个大规模的遥感图像目标检测数据集，包含<strong>2806张</strong>图像，总共<strong>15个类别</strong>，共计188282个目标。图像来源于不同的传感器和平台，<strong>具有不同的分辨率、尺度、视角、光照和密度</strong>。目标以四边形的形式标注，可以处理任意方向的目标。该数据集还提供了一个评估协议和一个基准测试</li>
</ul>
<table>
<thead>
<tr>
<th>Attr</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>类型</td>
<td>卫星影像</td>
</tr>
<tr>
<td>数据量</td>
<td>2806(15 类)</td>
</tr>
<tr>
<td>分辨率</td>
<td>多样</td>
</tr>
<tr>
<td>论文数</td>
<td>117</td>
</tr>
<tr>
<td>best perform(mAP)</td>
<td>81.85%(2023 <a href="https://paperswithcode.com/paper/large-selective-kernel-network-for-remote">LSKNet-S</a>)</td>
</tr>
</tbody></table>
<p>![[Pasted image 20230325225430.png]]</p>
<ul>
<li>DIOR<br>来自西北工业大学。含<strong>23463张</strong>图片和190288实例，覆盖<strong>20种目标</strong>，比DOTA数据集更大！2019年9月开始挂在arXiv上面</li>
</ul>
<table>
<thead>
<tr>
<th>Attr</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>类型</td>
<td>卫星影像</td>
</tr>
<tr>
<td>数据量</td>
<td>23463(20 类)</td>
</tr>
<tr>
<td>分辨率</td>
<td>多样</td>
</tr>
<tr>
<td>论文数</td>
<td>未知</td>
</tr>
<tr>
<td>best perform(mAP)</td>
<td>64.41%(2022 <a href="https://ieeexplore.ieee.org/document/9795321">AOPG</a>)</td>
</tr>
</tbody></table>
<h2 id="地物分类-语义分割"><a href="#地物分类-语义分割" class="headerlink" title="地物分类(语义分割)"></a>地物分类(语义分割)</h2><h3 id="任务总览-1"><a href="#任务总览-1" class="headerlink" title="任务总览"></a>任务总览</h3><table>
<thead>
<tr>
<th>信息</th>
<th>属性</th>
</tr>
</thead>
<tbody><tr>
<td>任务描述</td>
<td>(基础任务)对地表覆盖类型进行分类，可以是场景尺度也可以是像素级尺度</td>
</tr>
<tr>
<td>难度</td>
<td>5/5</td>
</tr>
<tr>
<td>研究价值</td>
<td>5/5</td>
</tr>
</tbody></table>
<h3 id="难度分析-2"><a href="#难度分析-2" class="headerlink" title="难度分析"></a>难度分析</h3><ul>
<li>高分辨率遥感影像中，物体的尺度、角度、光照条件总是有较大的差异</li>
<li>高分辨率遥感影像的背景更加复杂，而且归类为背景的区域特别复杂，造成类别之间的不确定性较高</li>
<li>前景的比例远小于背景，造成二分类的不平衡问题</li>
</ul>
<h3 id="研究角度-2"><a href="#研究角度-2" class="headerlink" title="研究角度"></a>研究角度</h3><table>
<thead>
<tr>
<th>角度</th>
<th>方法</th>
</tr>
</thead>
<tbody><tr>
<td>继续改进端到端的神经网络</td>
<td>基于 Transformer、Residual Block等架构或模块</td>
</tr>
<tr>
<td>降低模型对旋转角度的敏感</td>
<td>利用对旋转敏感的特征学习到任意旋转角度的映射</td>
</tr>
<tr>
<td>降低模型对尺度的敏感</td>
<td>基于多尺度模块或注意力机制来学习多尺度信息或抑制杂乱</td>
</tr>
<tr>
<td>降低数据量的依赖</td>
<td>利用迁移学习、伪标签或领域自适应来提高泛化能力</td>
</tr>
<tr>
<td>生成样本</td>
<td>利用生成模型扩充数据量较少的类别</td>
</tr>
</tbody></table>
<h3 id="应用实例-2"><a href="#应用实例-2" class="headerlink" title="应用实例"></a>应用实例</h3><ul>
<li>土地利用</li>
<li>土地覆盖</li>
<li>自动制图解译</li>
<li>地球监测</li>
</ul>
<h3 id="研究价值较高的数据集-2"><a href="#研究价值较高的数据集-2" class="headerlink" title="研究价值较高的数据集"></a>研究价值较高的数据集</h3><blockquote>
<p>只介绍语义分割数据集</p>
</blockquote>
<ul>
<li>BigEarthNet<br>BigEarthNet是一个大规模的多标签遥感图像数据集，用于地球观测和环境监测任务。该数据集包含<strong>125,000张</strong>Sentinel-2<strong>卫星图像</strong>，覆盖了整个世界上12个不同地区的陆地表面，并使用<strong>43个类别</strong>的标签进行注释，包括森林、道路、河流等。每个图像都有多个标签，因此可以用于多标签分类任务。这个数据集是公开可用的，可以用于训练和评估遥感图像分析算法的性能。</li>
</ul>
<table>
<thead>
<tr>
<th>Attr</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>类型</td>
<td>卫星影像</td>
</tr>
<tr>
<td>数据量</td>
<td>10(43 类)</td>
</tr>
<tr>
<td>分辨率</td>
<td>10 m 到 60 m 不等</td>
</tr>
<tr>
<td>论文数</td>
<td>43</td>
</tr>
<tr>
<td>best perform</td>
<td>89.3%(2022 MoCo-v2 微调，来自一篇 Review)</td>
</tr>
</tbody></table>
<p>![[Pasted image 20230320221929.png]]</p>
<ul>
<li>ISPRS Potsdam<br>ISPRS Potsdam是一个用于2D语义标记竞赛的数据集。该数据集包含<strong>38个</strong>大小相同的补丁，每个补丁都由从更大的TOP马赛克中提取的真正正射影像（TOP）组成。TOP和DSM的地面采样距离均为5厘米</li>
</ul>
<table>
<thead>
<tr>
<th>Attr</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>类型</td>
<td>航飞影像</td>
</tr>
<tr>
<td>数据量</td>
<td>38(6 类)</td>
</tr>
<tr>
<td>分辨率</td>
<td>5 m</td>
</tr>
<tr>
<td>论文数</td>
<td>11</td>
</tr>
<tr>
<td>best perform</td>
<td>92%(2021 DC-Swin &amp; FT-UnetFormer)</td>
</tr>
</tbody></table>
<p>![[Pasted image 20230322165622.png]]</p>
<ul>
<li>ISPRS Vaihingen<br>ISPRS Vaihingen数据集是一个用于评估遥感图像分割算法性能的公共数据集。该数据集基于德国城市Vaihingen的<strong>航空摄影图像</strong>，包含<strong>16</strong>个不同区域的高分辨率<strong>RGB图像</strong>和相应的地面真实值图。这些图像涵盖了各种场景，包括建筑物、道路、树木等，可以用于测试和比较不同的遥感图像分割算法的性能。该数据集已成为评估遥感图像分割算法的标准基准数据集之一，广泛应用于学术界和工业界。</li>
</ul>
<table>
<thead>
<tr>
<th>Attr</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>类型</td>
<td>航飞影像</td>
</tr>
<tr>
<td>数据量</td>
<td>33(6 类)</td>
</tr>
<tr>
<td>分辨率</td>
<td>9 cm</td>
</tr>
<tr>
<td>论文数</td>
<td>11</td>
</tr>
<tr>
<td>best perform</td>
<td>91.6%(2021 DC-Swin &amp; FT-UnetFormer)</td>
</tr>
</tbody></table>
<p>![[Pasted image 20230320214402.png]]</p>
<ul>
<li>xBD<br>它包含超过45,000KM^2的<strong>多边形标记的灾前和灾后图像</strong>。该数据集提供了灾后图像，其中从灾前转换的多边形覆盖在建筑物上，并带有损坏分类标签。在xBD上有一个2D语义分割基准测试，其中模型根据其<strong>加权平均F1得分</strong>进行排名。目前在这个基准测试中<strong>最先进的模型是BDANet</strong></li>
</ul>
<table>
<thead>
<tr>
<th>Attr</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>类型</td>
<td>卫星影像</td>
</tr>
<tr>
<td>数据量</td>
<td>未知(2 类)</td>
</tr>
<tr>
<td>分辨率</td>
<td>未知</td>
</tr>
<tr>
<td>论文数</td>
<td>29</td>
</tr>
<tr>
<td>best perform (F1-score)</td>
<td>80.6%(2021 BDANet)</td>
</tr>
</tbody></table>
<p>![[Pasted image 20230322161739.png]]</p>
<h2 id="变化检测"><a href="#变化检测" class="headerlink" title="变化检测"></a>变化检测</h2><h3 id="任务总览-2"><a href="#任务总览-2" class="headerlink" title="任务总览"></a>任务总览</h3><table>
<thead>
<tr>
<th>信息</th>
<th>属性</th>
</tr>
</thead>
<tbody><tr>
<td>任务描述</td>
<td>(高级任务)对不同时期的遥感影像进行对比分析(像素级)，发现特定对象地表覆盖的变化面，如灾害、建筑开发、景观演化</td>
</tr>
<tr>
<td>难度</td>
<td>5/5</td>
</tr>
<tr>
<td>研究价值</td>
<td>5/5</td>
</tr>
</tbody></table>
<h3 id="难度分析-3"><a href="#难度分析-3" class="headerlink" title="难度分析"></a>难度分析</h3><ul>
<li>做对比的两张时相数据，往往是多源、多尺度、时相差异较大的，我们要同时考虑影像的配准、辐射校正、去噪等预处理问题。</li>
<li>强大的深度学习能够更好地学习复杂时相数据地关系，但目前进步空间还很大</li>
</ul>
<h3 id="研究角度-3"><a href="#研究角度-3" class="headerlink" title="研究角度"></a>研究角度</h3><table>
<thead>
<tr>
<th>角度</th>
<th>方法</th>
</tr>
</thead>
<tbody><tr>
<td>降低时相差异的干扰</td>
<td>学习受噪声干扰较小的表征来做深度学习的对比</td>
</tr>
<tr>
<td>学习更丰富的时序差异</td>
<td>提取两幅以上的时相数据与目标时相的差异</td>
</tr>
</tbody></table>
<h3 id="应用实例-3"><a href="#应用实例-3" class="headerlink" title="应用实例"></a>应用实例</h3><ul>
<li>城市生长监测</li>
<li>自然演变检测</li>
<li>灾害评估与预防</li>
</ul>
<h3 id="研究价值较高的数据集-3"><a href="#研究价值较高的数据集-3" class="headerlink" title="研究价值较高的数据集"></a>研究价值较高的数据集</h3><ul>
<li>LEVIR-CD<br>LEVIR-CD数据集由<strong>637对</strong>非常高分辨率（VHR，<strong>像素0.5米</strong>）的Google Earth（GE）图像补丁组成，每个<strong>补丁大小为1024×1024像素</strong>。这些相隔5至14年的双时相图像有显著的土地利用变化，特别是建筑物增长。LEVIR-CD覆盖了各种类型的建筑物，例如别墅住宅、高层公寓、小车库和大型仓库。我们<strong>主要关注与建筑物相关的变化</strong>，包括建筑物的增长（从土地/草地/硬化地面或正在建造中的建筑物到新的建筑区域的变化）和建筑物的减少。这些双时相图像由遥感影像解译专家使用二进制标签（1表示变化，0表示未变化）进行注释。我们的每个样本都由一个注释者进行注释，然后由另一个注释者进行双重检查以产生高质量的注释。完全注释的LEVIR-CD包含一共31,333个单独的变化建筑实例</li>
</ul>
<table>
<thead>
<tr>
<th>Attr</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>类型</td>
<td>卫星影像</td>
</tr>
<tr>
<td>数据量</td>
<td>637对(1 类)</td>
</tr>
<tr>
<td>分辨率</td>
<td>0.5 m</td>
</tr>
<tr>
<td>论文数</td>
<td>36</td>
</tr>
<tr>
<td>best perform (F1-score)</td>
<td>92.33%(2022 Changer-R101)</td>
</tr>
</tbody></table>
<p>![[Pasted image 20230326163447.png]]</p>
<h2 id="三维重建"><a href="#三维重建" class="headerlink" title="三维重建"></a>三维重建</h2><h3 id="任务总览-3"><a href="#任务总览-3" class="headerlink" title="任务总览"></a>任务总览</h3><table>
<thead>
<tr>
<th>信息</th>
<th>属性</th>
</tr>
</thead>
<tbody><tr>
<td>任务描述</td>
<td>(高级任务)从多角度遥感影像中提取高精度的三维信息，如高程、形态</td>
</tr>
<tr>
<td>难度</td>
<td>5/5</td>
</tr>
<tr>
<td>研究价值</td>
<td>5/5</td>
</tr>
</tbody></table>
<h3 id="难度分析-4"><a href="#难度分析-4" class="headerlink" title="难度分析"></a>难度分析</h3><ul>
<li>需要解决图像配准、深度估计、点云生成等子问题</li>
<li>要克服的挑战有图像的低质量、噪声、遮挡、光照差异、尺度变化等</li>
</ul>
<h3 id="研究角度-4"><a href="#研究角度-4" class="headerlink" title="研究角度"></a>研究角度</h3><table>
<thead>
<tr>
<th>角度</th>
<th>方法</th>
</tr>
</thead>
<tbody><tr>
<td>更高精度的深度图或点云</td>
<td>利用生成模型提取</td>
</tr>
<tr>
<td>降低标注依赖</td>
<td>利用注意力机制增强表征能力，并利用自监督或弱监督增强泛化能力</td>
</tr>
<tr>
<td>更强的先验约束</td>
<td>利用拓扑约束来优化点云结构，使用物理规则增强场景理解</td>
</tr>
<tr>
<td>更丰富的数据特征</td>
<td>利用多源、时序或多模态数据学习到丰富的特征</td>
</tr>
</tbody></table>
<h3 id="应用实例-4"><a href="#应用实例-4" class="headerlink" title="应用实例"></a>应用实例</h3><ul>
<li>三维可视化</li>
<li>三维关系表达</li>
</ul>
<h2 id="图像描述"><a href="#图像描述" class="headerlink" title="图像描述"></a>图像描述</h2><h3 id="任务总览-4"><a href="#任务总览-4" class="headerlink" title="任务总览"></a>任务总览</h3><table>
<thead>
<tr>
<th>信息</th>
<th>属性</th>
</tr>
</thead>
<tbody><tr>
<td>任务描述</td>
<td>用自然语言描述遥感影像的内容，对数量关系和空间位置关系有一定要求</td>
</tr>
<tr>
<td>难度</td>
<td>5/5</td>
</tr>
<tr>
<td>研究价值</td>
<td></td>
</tr>
</tbody></table>
<h2 id="图像融合"><a href="#图像融合" class="headerlink" title="图像融合"></a>图像融合</h2><h3 id="任务总览-5"><a href="#任务总览-5" class="headerlink" title="任务总览"></a>任务总览</h3><table>
<thead>
<tr>
<th>信息</th>
<th>属性</th>
</tr>
</thead>
<tbody><tr>
<td>任务描述</td>
<td>多源遥感影像进行对齐和融合，提高空间分辨率和光谱分辨率</td>
</tr>
<tr>
<td>难度</td>
<td>3/5</td>
</tr>
<tr>
<td>研究价值</td>
<td></td>
</tr>
</tbody></table>
<h2 id="图像配准"><a href="#图像配准" class="headerlink" title="图像配准"></a>图像配准</h2><h3 id="任务总览-6"><a href="#任务总览-6" class="headerlink" title="任务总览"></a>任务总览</h3><table>
<thead>
<tr>
<th>信息</th>
<th>属性</th>
</tr>
</thead>
<tbody><tr>
<td>任务描述</td>
<td>多源遥感影像进行几何变换和灰度匹配，以消除或减少多源图像之间的空间差异</td>
</tr>
<tr>
<td>难度</td>
<td>4/5</td>
</tr>
<tr>
<td>研究价值</td>
<td></td>
</tr>
</tbody></table>
]]></content>
      <tags>
        <tag>Computer Vision 计算机视觉</tag>
      </tags>
  </entry>
</search>
