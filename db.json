{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"source/about/index/weixin.jpg","path":"about/index/weixin.jpg","modified":0,"renderable":0},{"_id":"source/about/index/zhifubao.jpg","path":"about/index/zhifubao.jpg","modified":0,"renderable":0},{"_id":"themes/butterfly/source/css/index.styl","path":"css/index.styl","modified":0,"renderable":1},{"_id":"themes/butterfly/source/css/var.styl","path":"css/var.styl","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/tw_cn.js","path":"js/tw_cn.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/404.jpg","path":"img/404.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/algolia.svg","path":"img/algolia.svg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/algolia_logo.svg","path":"img/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/apple-touch-icon-next.png","path":"img/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/avatar.gif","path":"img/avatar.gif","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/favicon.png","path":"img/favicon.png","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/friend_404.gif","path":"img/friend_404.gif","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/loading.gif","path":"img/loading.gif","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/logo.svg","path":"img/logo.svg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/weixin.png","path":"img/weixin.png","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/zhifubao.jpg","path":"img/zhifubao.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/封面原稿.png","path":"img/封面原稿.png","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/类.png","path":"img/类.png","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/search/algolia.js","path":"js/search/algolia.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/search/local-search.js","path":"js/search/local-search.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/Anaconda环境日志.md","hash":"cad70abf726e531dd0fc0aa2b492d0710b799fe3","modified":1628346020329},{"_id":"source/_posts/.DS_Store","hash":"6e516069ee4035d19c23c3e33405e8c847e1feab","modified":1632702858924},{"_id":"source/CNAME","hash":"50464c10e057410560109446a31a76a6332b7ed8","modified":1625475571718},{"_id":"source/_posts/GISer的思想.md","hash":"9404c487714623df7a1a65d315b4e6a0f03b2e4f","modified":1626187926596},{"_id":"source/.DS_Store","hash":"9fa8fb4036090098dc43ae87cf69d4983c66f297","modified":1632726280478},{"_id":"source/_posts/GIS的前沿现状.md","hash":"985d415129bb6a18f066e0bf44f15a423ab2de02","modified":1626188268923},{"_id":"source/_posts/GIS的应用.md","hash":"ac904026d34dfc630e7ce9736309889a50acd2ec","modified":1628344976488},{"_id":"source/_posts/Keras·GPU训练（单-多）.md","hash":"4e9232812ec3284d32ea37dbf4b7bf8f3efd7276","modified":1628351679097},{"_id":"source/_posts/GIS的简要定义.md","hash":"5b0b717457f68436b991b602700f2033e60c9f3e","modified":1626188337577},{"_id":"source/_posts/Pytorch·API部署WEB·ONNX.md","hash":"476c571d1bdb29ff1513ce1d76a1477eefe6a08c","modified":1628350754664},{"_id":"source/_posts/Pytorch·GPU训练-单-多.md","hash":"4b1b133ac6f76d492c48769489e129b5a25f9457","modified":1628349856776},{"_id":"source/_posts/美食之美.md","hash":"2761852b9075e467c975e84f6e6466c6b9bddddc","modified":1632710136445},{"_id":"source/_posts/TensorboardX训练可视化.md","hash":"00f72e075bb7638bb3daa70532fd503930b8362b","modified":1628349403553},{"_id":"source/_posts/损失函数与语义分割任务.md","hash":"d8ae29f792e107b065a796cf4e6ee8845a1ead5d","modified":1628347064497},{"_id":"source/_posts/分割任务下数据集增广.md","hash":"7475d6c8f8bcf975b356e92b923b8e6505ccdcfc","modified":1629537478141},{"_id":"source/_posts/语义分割网络模型笔记.md","hash":"bc32aed0eb99d3b2a70e56307975705a94bc521c","modified":1628346505967},{"_id":"source/categories/index.md","hash":"792b6c7fd4ca07f74062798eddd553e3f129b2b1","modified":1625576600275},{"_id":"source/tags/index.md","hash":"ff10466f775756d0695536210393c43b8697cf44","modified":1625575419309},{"_id":"source/about/index.md","hash":"a70c44bb4f26b0b98d83ec70bbbebce1ce1e2f4f","modified":1626319439287},{"_id":"source/photos/index.md","hash":"a66e6821216431b83052ccfc2e9231443387cffc","modified":1629534031347},{"_id":"source/_posts/GIS的应用/.DS_Store","hash":"1d375697da1d22c87c0b43707b6dd57dbf625635","modified":1626188039436},{"_id":"source/_posts/GIS的前沿现状/.DS_Store","hash":"ab04de3da934f6bf293d3645d8ad59951154d630","modified":1626188303052},{"_id":"source/_posts/Keras·GPU训练（单-多）/图2.png","hash":"c29cd7f6886ca460e0d11bc9ea059323ac847204","modified":1628351242161},{"_id":"source/_posts/Keras·GPU训练（单-多）/图3.png","hash":"1c422fafc65871aba643217bd981f82fdfabd785","modified":1628351380834},{"_id":"source/_posts/Keras·GPU训练（单-多）/图6.png","hash":"fa46d05d9e4363beba987a8d72585e88b57ecdfc","modified":1628351272179},{"_id":"source/_posts/Keras·GPU训练（单-多）/图5.png","hash":"40d29cf2f8a172c8140a7fcfa7714bfa7dfd21fd","modified":1628351264026},{"_id":"source/_posts/Keras·GPU训练（单-多）/图7.png","hash":"51ca9fdfa66aac074da84e7ab5e98ea55dcd41f7","modified":1628351277991},{"_id":"source/_posts/Keras·GPU训练（单-多）/图8.png","hash":"bc157f73febe73ca638dba7dfbfe387786974ffe","modified":1628351282945},{"_id":"source/_posts/Keras·GPU训练（单-多）/图9.png","hash":"104d91db94ece58ccf45a85204c019b95c5324d3","modified":1628351287995},{"_id":"source/_posts/Pytorch·API部署WEB·ONNX/.DS_Store","hash":"ebf6aafb548470a2622beb2116b9f8b716890aff","modified":1628350786355},{"_id":"source/_posts/Pytorch·API部署WEB·ONNX/图1.png","hash":"f2eb62463dd9ef45312bf96ad0cf2cd79489bb80","modified":1628350070462},{"_id":"source/_posts/Pytorch·API部署WEB·ONNX/图2.png","hash":"b8c63e9788e937c7e075ad509998e146924b7807","modified":1628350115081},{"_id":"source/_posts/Pytorch·API部署WEB·ONNX/图10.png","hash":"ce2f03a12fa9f199ff47f2872a65abf8cd06c376","modified":1628350177252},{"_id":"source/_posts/Pytorch·API部署WEB·ONNX/图3.png","hash":"48a5bc13c8f952ced8f02ef412c11a098f979b6a","modified":1628350119966},{"_id":"source/_posts/Pytorch·API部署WEB·ONNX/图4.png","hash":"eb9a8257500202daf635f6d66468864a6381ca66","modified":1628350126629},{"_id":"source/_posts/Pytorch·API部署WEB·ONNX/图5.png","hash":"ff7ed6c6a7a7136e15f08a2e7f7011efb341907f","modified":1628350150162},{"_id":"source/_posts/Pytorch·API部署WEB·ONNX/图7.png","hash":"934c34b77d6bd2bd762a14fda27eb03dd9049cd2","modified":1628350160602},{"_id":"source/_posts/Pytorch·API部署WEB·ONNX/图6.png","hash":"e86993a34016ac9dad9874b9b900b1f6b1e637fc","modified":1628350155055},{"_id":"source/_posts/Pytorch·API部署WEB·ONNX/图8.png","hash":"f4abad45c4ee0c78c1aca02f2dba042bc8b3c792","modified":1628350165845},{"_id":"source/_posts/Pytorch·API部署WEB·ONNX/图9.png","hash":"e014f6ede3b440aaa880c97d916760ccd909e7e4","modified":1628350171134},{"_id":"source/_posts/Pytorch·GPU训练-单-多/图1.png","hash":"26bd8275e2e18a9a5ab1b68e85c55f4dd5aed832","modified":1628349734900},{"_id":"source/_posts/TensorboardX训练可视化/图1.png","hash":"dfac55a597500d3a296b3ddb5ebcf8dc9f395f91","modified":1628347470063},{"_id":"source/_posts/TensorboardX训练可视化/图2.png","hash":"186fafb5c7f43b516231e12c6cdff5d438eb21e4","modified":1628347475826},{"_id":"source/_posts/TensorboardX训练可视化/图6.png","hash":"086ebb091448f227d7264ba409ae0cc0980d9389","modified":1628347502350},{"_id":"source/_posts/TensorboardX训练可视化/图5.png","hash":"f640efd68535922f4435116babd6184086264f0c","modified":1628347494753},{"_id":"source/_posts/TensorboardX训练可视化/图4.png","hash":"02bfbdff299f4e8da08b5e19fb1e66e9343a962c","modified":1628347489642},{"_id":"source/_posts/TensorboardX训练可视化/图7.png","hash":"7ff836bc832d52b5b44cec499f62118fd3702435","modified":1628347507661},{"_id":"source/_posts/TensorboardX训练可视化/图8.png","hash":"4669aa6c432d9a709f3e94b81f98541a92a1b3a3","modified":1628347512618},{"_id":"source/_posts/损失函数与语义分割任务/图2.png","hash":"c7cf04aa61eeb3e200677eb937d922bc06f2286a","modified":1628346823300},{"_id":"source/_posts/损失函数与语义分割任务/图3.png","hash":"c3db3bc33b9baa19b8d7018de9b3f3de1c3f11c0","modified":1628346827957},{"_id":"source/_posts/TensorboardX训练可视化/图9.gif","hash":"2cb5fe15b54329d2481c97cb092fe4b1e4d502b9","modified":1628347518909},{"_id":"source/_posts/损失函数与语义分割任务/图4.png","hash":"74845a11a80d351223766426f11eea7a42c4668a","modified":1628346834309},{"_id":"source/_posts/损失函数与语义分割任务/图6.png","hash":"fd8d371b7e72fdd99d54ca55201fca1c62f0a837","modified":1628346845394},{"_id":"source/_posts/损失函数与语义分割任务/图7.png","hash":"b18d25481683aeca98a837e11c1dd5f61a2a7572","modified":1628346850433},{"_id":"source/about/index/.DS_Store","hash":"87b74537bbd3b604007e619169a29fd3c3e0aa2b","modified":1625572634919},{"_id":"source/_posts/GIS的应用/图1.png","hash":"442326c73998a3928bfae624f1e9cfc6a79c90cc","modified":1626188017715},{"_id":"source/_posts/Anaconda环境日志/图1.png","hash":"c3ce12af475627eb519198b709538e900d0df8d7","modified":1628345446201},{"_id":"source/_posts/Anaconda环境日志/图3.png","hash":"2add624209936c9c0ace5288888752a59fc2914b","modified":1628345467472},{"_id":"source/_posts/Keras·GPU训练（单-多）/图4.png","hash":"bdd2e24ce284e651b2be49aafe82633952473c2d","modified":1628351256628},{"_id":"source/_posts/语义分割网络模型笔记/图1.jpg","hash":"d78a5357ee602caa99e9a515380b1a54ebdd8825","modified":1628346205145},{"_id":"source/_posts/语义分割网络模型笔记/图3.jpg","hash":"5a17e9d398a94f466879c0d106c42d30314b17a1","modified":1628346221111},{"_id":"source/_posts/语义分割网络模型笔记/图5.jpg","hash":"1d32fd03cc29ea18e146c446bce48a03c47a2091","modified":1628346233798},{"_id":"source/_posts/损失函数与语义分割任务/图5.png","hash":"b4875c91ad35be184411874c858e4913314bf5b5","modified":1628346839990},{"_id":"source/_posts/Keras·GPU训练（单-多）/图1.png","hash":"de643993afcf14f16330f442444ba14a8237653e","modified":1628351236803},{"_id":"source/_posts/语义分割网络模型笔记/图2.jpg","hash":"424040a9ae9ebcc2ea6bca47cb7c9ca30767b4d5","modified":1628346214171},{"_id":"source/_posts/语义分割网络模型笔记/图4.jpg","hash":"9d1d240f5ea3c8af9bcadaebc980ae4f1d5792f6","modified":1628346227640},{"_id":"source/_posts/语义分割网络模型笔记/图6.jpg","hash":"b72c093509600dd35591e38ca27d62a0004474b2","modified":1628346367698},{"_id":"source/_posts/分割任务下数据集增广/图1.png","hash":"b4f53494c472f15100bc82fb61c7cd3db340dea6","modified":1628347262123},{"_id":"source/_posts/TensorboardX训练可视化/图3.png","hash":"63b21576609627b3d9d4e4ad059e39a267b86d6e","modified":1628347482895},{"_id":"source/about/index/weixin.jpg","hash":"bef1f685d45963a0308deac4b2395ae5ecd8cee2","modified":1625571482247},{"_id":"source/_posts/GISer的思想/1.png","hash":"efda4cb11c6dc8b706648de170fed29cebe03d0b","modified":1626098400679},{"_id":"source/_posts/GIS的前沿现状/图1.png","hash":"c64839c1e4426e00fa0df83314f8c5fd9c3b1e7b","modified":1626188186762},{"_id":"source/about/index/zhifubao.jpg","hash":"7b71e940d3082e499ac071f95029e7fa8530f746","modified":1625571482841},{"_id":"source/_posts/GIS的前沿现状/图2.png","hash":"103da15478f7457fbc3d5aad459edbdcf8fc7013","modified":1626188180572},{"_id":"source/_posts/损失函数与语义分割任务/图1.png","hash":"f75108240d13f4bb3e2ec3a4932093ec5e2ec0b9","modified":1628346815002},{"_id":"themes/butterfly/README.md","hash":"cedd13fcd8c75a68742265dd8eced4087e940ffd","modified":1632560182631},{"_id":"themes/butterfly/package.json","hash":"333603963b3d2cedb643def8a9955f6d540f5d95","modified":1632560182639},{"_id":"themes/butterfly/LICENSE","hash":"1128f8f91104ba9ef98d37eea6523a888dcfa5de","modified":1632560182631},{"_id":"themes/butterfly/languages/default.yml","hash":"7ca673fb629ea74f5ba5e75b4f0f95248cfb5090","modified":1632560182632},{"_id":"themes/butterfly/README_CN.md","hash":"459d6f3200863021bee1fe72a719aef236fb4090","modified":1632560182632},{"_id":"themes/butterfly/languages/en.yml","hash":"cd333235ff1648a6bf58dfafc81f2c57672a15a5","modified":1632560182632},{"_id":"themes/butterfly/languages/zh-TW.yml","hash":"79a50c40d9f5463f1fa42aa870ac6b8b84540412","modified":1632560182632},{"_id":"themes/butterfly/.github/stale.yml","hash":"05a55a87fa7f122c59683e41c8b2e37e79f688f0","modified":1632560182631},{"_id":"themes/butterfly/_config.yml","hash":"1d9a732c8f654874da4db34ce9185158df8c9ba2","modified":1632702638810},{"_id":"themes/butterfly/layout/archive.pug","hash":"bd62286afb64a51c97e800c5945620d51605d5fa","modified":1632560182632},{"_id":"themes/butterfly/layout/category.pug","hash":"60c1b795b6e227b5dd81963b51d29d1b81d0bf49","modified":1632560182632},{"_id":"themes/butterfly/layout/index.pug","hash":"e1c3146834c16e6077406180858add0a8183875a","modified":1632560182638},{"_id":"themes/butterfly/layout/page.pug","hash":"82aa988527a11835e7ac86ce4f23b8cd20014dfa","modified":1632560182639},{"_id":"themes/butterfly/layout/post.pug","hash":"8d398c8925182699d9f2b9f1b727f06228488312","modified":1632560182639},{"_id":"themes/butterfly/layout/tag.pug","hash":"0440f42569df2676273c026a92384fa7729bc4e9","modified":1632560182639},{"_id":"themes/butterfly/languages/zh-CN.yml","hash":"1e6472149c3543345c5ab130411c0066e15391c4","modified":1632634265568},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/bug_report.md","hash":"476802922b774b679225102ac30a9d9183394701","modified":1632560182631},{"_id":"themes/butterfly/scripts/events/404.js","hash":"83cd7f73225ccad123afbd526ce1834eb1eb6a6d","modified":1632560182639},{"_id":"themes/butterfly/scripts/events/init.js","hash":"018aa446265fe627301b1d53d7cba4f4ff1960ac","modified":1632560182639},{"_id":"themes/butterfly/scripts/events/welcome.js","hash":"d575137c8779e50422c2416f4d0832fdea346ee6","modified":1632560182639},{"_id":"themes/butterfly/scripts/filters/post_lazyload.js","hash":"4cc2d517195c8779471d326ada09f9371cbad4dd","modified":1632560182639},{"_id":"themes/butterfly/scripts/filters/random_cover.js","hash":"9821872007cf57efae4b728dc575ef9d004547bb","modified":1632560182639},{"_id":"themes/butterfly/scripts/helpers/aside_archives.js","hash":"2ec66513d5322f185d2071acc052978ba9415a8e","modified":1632560182639},{"_id":"themes/butterfly/scripts/helpers/aside_categories.js","hash":"e00efdb5d02bc5c6eb4159e498af69fa61a7dbb9","modified":1632560182639},{"_id":"themes/butterfly/scripts/helpers/inject_head_js.js","hash":"65f2442e04c4defd16e7c1e67701d3bb41d9577a","modified":1632560182639},{"_id":"themes/butterfly/scripts/helpers/page.js","hash":"c6611d97087c51845cb1ab4821696a62fa33daeb","modified":1632560182639},{"_id":"themes/butterfly/scripts/helpers/related_post.js","hash":"21556f9cb412ddc500ad12ecfd419f3ea6c9f663","modified":1632560182639},{"_id":"themes/butterfly/scripts/tag/button.js","hash":"b816ded1451f28c7c54151ffe6c259b110253ae3","modified":1632560182640},{"_id":"themes/butterfly/scripts/tag/gallery.js","hash":"94826ea6bcc4d2304199adae12c4e2b272caf529","modified":1632560182640},{"_id":"themes/butterfly/scripts/tag/hide.js","hash":"f33858ffb9e88191e644796e11d2f901eb332308","modified":1632560182640},{"_id":"themes/butterfly/scripts/tag/inlineImg.js","hash":"a43ee2c7871bdd93cb6beb804429e404570f7929","modified":1632560182640},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/custom.md","hash":"eff495eb1584cf4586e33c76e8b2fa6a469a179b","modified":1632560182631},{"_id":"themes/butterfly/scripts/tag/label.js","hash":"03b2afef41d02bd1045c89578a02402c28356006","modified":1632560182640},{"_id":"themes/butterfly/scripts/tag/mermaid.js","hash":"35f073021db93699fcac9ef351e26c59c31aadf7","modified":1632560182640},{"_id":"themes/butterfly/scripts/tag/note.js","hash":"c16c6eb058af2b36bcd583b2591076c7ebdd51ad","modified":1632560182640},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/feature_request.md","hash":"f6867a2f0417fe89a0f2008730ee19dd38422021","modified":1632560182631},{"_id":"themes/butterfly/.github/workflows/publish.yml","hash":"05857c2f265246d8de00e31037f2720709540c09","modified":1632560182631},{"_id":"themes/butterfly/scripts/tag/tabs.js","hash":"6c6e415623d0fd39da016d9e353bb4f5cca444f5","modified":1632560182640},{"_id":"themes/butterfly/layout/includes/404.pug","hash":"7d378e328a53cc99d5acc9682dce53f5eb61537d","modified":1632560182632},{"_id":"themes/butterfly/layout/includes/additional-js.pug","hash":"4156224c47bfc2482281ac4e4df701c30476ff00","modified":1632560182632},{"_id":"themes/butterfly/layout/includes/footer.pug","hash":"02390a5b6ae1f57497b22ba2e6be9f13cfb7acac","modified":1632560182632},{"_id":"themes/butterfly/layout/includes/head.pug","hash":"1377952022ee0a9eaa7a2fd1098f1571efc468d9","modified":1632560182632},{"_id":"themes/butterfly/layout/includes/layout.pug","hash":"6f2608c4d93d3d10ae6b2cd7f8918f303f024321","modified":1632560182633},{"_id":"themes/butterfly/layout/includes/pagination.pug","hash":"0b80f04950bd0fe5e6c4e7b7559adf4d0ce28436","modified":1632560182634},{"_id":"themes/butterfly/layout/includes/rightside.pug","hash":"2d0453adf92a3fd3466cf0793f14685d17b8b51d","modified":1632560182634},{"_id":"themes/butterfly/layout/includes/sidebar.pug","hash":"8dafc2dcd8c33f70a546fee443f0b6d80b3cd243","modified":1632560182634},{"_id":"themes/butterfly/source/css/index.styl","hash":"861998e4ac67a59529a8245a9130d68f826c9c12","modified":1632560182644},{"_id":"themes/butterfly/source/css/var.styl","hash":"65d5653150627b83428c7cd9408e41c49157652f","modified":1632637383037},{"_id":"themes/butterfly/source/js/main.js","hash":"b244f28124a46d7f1e8ef76ba6e925289691f93b","modified":1632560182645},{"_id":"themes/butterfly/source/css/.DS_Store","hash":"5d1392368849a4cf8fcf4e5785888d703e3b4bbf","modified":1632709822754},{"_id":"themes/butterfly/source/js/tw_cn.js","hash":"00053ce73210274b3679f42607edef1206eebc68","modified":1632560182645},{"_id":"themes/butterfly/source/js/utils.js","hash":"8319b59c26ce8cd2b0ae7d030c4912215148fa92","modified":1632560182645},{"_id":"themes/butterfly/source/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1632560182644},{"_id":"themes/butterfly/source/img/algolia.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1632560182644},{"_id":"themes/butterfly/source/img/.DS_Store","hash":"7c75ba91cd813b8164c038f4a2910e1a7011bb82","modified":1632636263155},{"_id":"themes/butterfly/source/img/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1625476464949},{"_id":"themes/butterfly/source/img/apple-touch-icon-next.png","hash":"8b7e5d9f7b60baed4db48d8ae5c57b10a4ed2996","modified":1625570560312},{"_id":"themes/butterfly/source/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1632560182644},{"_id":"themes/butterfly/source/img/favicon.png","hash":"e30b7d837b4123552976ff87380dc679e8c613c9","modified":1625570380254},{"_id":"themes/butterfly/source/img/loading.gif","hash":"59ab4bff2c1b95fc8c00de9af99ac781125db443","modified":1629102906965},{"_id":"themes/butterfly/layout/includes/head/Open_Graph.pug","hash":"6c41f49a3e682067533dd9384e6e4511fc3a1349","modified":1632560182632},{"_id":"themes/butterfly/layout/includes/head/config.pug","hash":"4def0aab9e2172ad1f29abd1535d8e08ff23aa0b","modified":1632560182633},{"_id":"themes/butterfly/layout/includes/head/analytics.pug","hash":"90d01b88d0f406d00184960b1afe9230aec2ebe6","modified":1632560182633},{"_id":"themes/butterfly/layout/includes/head/config_site.pug","hash":"889ef16fa34a39e5533bc170e62f20f3450cc522","modified":1632560182633},{"_id":"themes/butterfly/layout/includes/head/google_adsense.pug","hash":"95a37e92b39c44bcbea4be7e29ddb3921c5b8220","modified":1632560182633},{"_id":"themes/butterfly/layout/includes/head/noscript.pug","hash":"d16ad2ee0ff5751fd7f8a5ce1b83935518674977","modified":1632560182633},{"_id":"themes/butterfly/layout/includes/head/pwa.pug","hash":"3d492cfe645d37c94d30512e0b230b0a09913148","modified":1632560182633},{"_id":"themes/butterfly/layout/includes/head/preconnect.pug","hash":"e55f8bdb876d5429a908498db1307b94094c0d06","modified":1632560182633},{"_id":"themes/butterfly/layout/includes/loading/loading-js.pug","hash":"4cfcf0100e37ce91864703cd44f1cb99cb5493ea","modified":1632560182633},{"_id":"themes/butterfly/layout/includes/loading/loading.pug","hash":"5276937fbcceb9d62879dc47be880cd469a27349","modified":1632560182634},{"_id":"themes/butterfly/layout/includes/header/index.pug","hash":"65fa23680af0daf64930a399c2f2ca37809a8149","modified":1632560182633},{"_id":"themes/butterfly/layout/includes/header/menu_item.pug","hash":"24370508ee87f14418e8f06e9d79ad8c52a342c4","modified":1632560182633},{"_id":"themes/butterfly/layout/includes/head/site_verification.pug","hash":"e2e8d681f183f00ce5ee239c42d2e36b3744daad","modified":1632560182633},{"_id":"themes/butterfly/layout/includes/header/nav.pug","hash":"c205b9fd72b2fe19e6d15c5b5ab0fb38c653032e","modified":1632560182633},{"_id":"themes/butterfly/layout/includes/header/post-info.pug","hash":"92f81a437c9db49f7ebcf608bc09488ecdb55a21","modified":1632560182633},{"_id":"themes/butterfly/layout/includes/header/social.pug","hash":"0d953e51d04a9294a64153c89c20f491a9ec42d4","modified":1632560182633},{"_id":"themes/butterfly/layout/includes/mixins/article-sort.pug","hash":"2fb74d0b0e4b98749427c5a1a1b0acb6c85fadc4","modified":1632560182634},{"_id":"themes/butterfly/layout/includes/mixins/post-ui.pug","hash":"4c3c5cb69b3aead8c232cb0fbc251929f28aad75","modified":1632560182634},{"_id":"themes/butterfly/layout/includes/page/default-page.pug","hash":"dbec869c62135695495703a29ad7655e9965d461","modified":1632560182634},{"_id":"themes/butterfly/layout/includes/page/tags.pug","hash":"93d4ebc7dc8228c7a10ddeb5a553d0dcdabbe145","modified":1632560182634},{"_id":"themes/butterfly/layout/includes/page/flink.pug","hash":"b53a2d4f9c37b375a4446d2273dcfb7712d91b3e","modified":1632560182634},{"_id":"themes/butterfly/layout/includes/page/categories.pug","hash":"1f30952fed73dec21b42e2e30b7fe2e84618d2e4","modified":1632560182634},{"_id":"themes/butterfly/layout/includes/post/post-copyright.pug","hash":"88e3b611b03149665e4113cfa39595c1a3fca7e5","modified":1632560182634},{"_id":"themes/butterfly/layout/includes/third-party/aplayer.pug","hash":"292646dfab135973b09f0fa9e3931e83da2ed30e","modified":1632560182634},{"_id":"themes/butterfly/layout/includes/post/reward.pug","hash":"5b404356f311d2ee36478291ca3553210867b738","modified":1632560182634},{"_id":"themes/butterfly/layout/includes/third-party/effect.pug","hash":"b9d54a01d7c2a7a183cb7209e99430ce7fea1fe3","modified":1632560182636},{"_id":"themes/butterfly/layout/includes/third-party/pangu.pug","hash":"d5fec7dedc52ab23865fb4db002755e9bdaadc9f","modified":1632560182637},{"_id":"themes/butterfly/layout/includes/widget/card_ad.pug","hash":"60dc48a7b5d89c2a49123c3fc5893ab9c57dd225","modified":1632560182638},{"_id":"themes/butterfly/layout/includes/third-party/pjax.pug","hash":"933cb710d2dbcea25c6426a57c6f49d2f48b792c","modified":1632560182637},{"_id":"themes/butterfly/layout/includes/third-party/prismjs.pug","hash":"1fbecfd299068f90d727f0c8c65e2a792fa6e3e2","modified":1632560182637},{"_id":"themes/butterfly/layout/includes/widget/card_announcement.pug","hash":"3d8e3706a056389176f55dd21956aabc78046761","modified":1632560182638},{"_id":"themes/butterfly/layout/includes/third-party/subtitle.pug","hash":"d50e5c22cd6bc3c378bc581918136746cfa3447f","modified":1632560182638},{"_id":"themes/butterfly/layout/includes/widget/card_archives.pug","hash":"86897010fe71503e239887fd8f6a4f5851737be9","modified":1632560182638},{"_id":"themes/butterfly/layout/includes/widget/card_author.pug","hash":"0366c658cdcff839aa1df2e2d252a03a53fd427e","modified":1632560182638},{"_id":"themes/butterfly/layout/includes/widget/card_bottom_self.pug","hash":"13dc8ce922e2e2332fe6ad5856ebb5dbf9ea4444","modified":1632560182638},{"_id":"themes/butterfly/layout/includes/widget/card_categories.pug","hash":"d1a416d0a8a7916d0b1a41d73adc66f8c811e493","modified":1632560182638},{"_id":"themes/butterfly/layout/includes/widget/card_newest_comment.pug","hash":"27afd2274bd5f2cbbf1bad9f0afe2b2b72c213ca","modified":1632560182638},{"_id":"themes/butterfly/layout/includes/widget/card_recent_post.pug","hash":"9c1229af6ab48961021886882c473514101fba21","modified":1632560182638},{"_id":"themes/butterfly/layout/includes/widget/card_tags.pug","hash":"438aea3e713ed16b7559b9a80a9c5ec0221263df","modified":1632560182638},{"_id":"themes/butterfly/layout/includes/widget/card_post_toc.pug","hash":"ae9336bf31cdad08ff586ead4295912a96563c76","modified":1632560182638},{"_id":"themes/butterfly/layout/includes/widget/card_webinfo.pug","hash":"0612aaee878f33ea8d3da0293c7dc3b6cd871466","modified":1632560182638},{"_id":"themes/butterfly/layout/includes/widget/card_top_self.pug","hash":"ae67c6d4130a6c075058a9c1faea1648bcc6f83e","modified":1632560182638},{"_id":"themes/butterfly/layout/includes/widget/index.pug","hash":"b5525891f6affd02c1ea3b2327c026882efe428b","modified":1632560182638},{"_id":"themes/butterfly/source/css/_global/function.styl","hash":"eda47f3e807a466ba8275627ea936c5100c43818","modified":1632560182640},{"_id":"themes/butterfly/source/css/_highlight/highlight.styl","hash":"85e72c70a0cef29e40be1968f5d23b06c6f8e3aa","modified":1632560182640},{"_id":"themes/butterfly/source/css/_highlight/theme.styl","hash":"fa4b87e7b29bdd1f09eb1a30e06ba74b224f0ba2","modified":1632560182641},{"_id":"themes/butterfly/source/css/_global/index.styl","hash":"7658e145e722c6bd8dd1e1b898d093cc9ed117eb","modified":1632624936153},{"_id":"themes/butterfly/source/css/_mode/readmode.styl","hash":"f59a9a0059d5261251bdd6de45aa97dd2d11e633","modified":1632560182642},{"_id":"themes/butterfly/source/css/_page/404.styl","hash":"b0488ceacde74af139d66c8db5cb36cc21737b9b","modified":1632560182642},{"_id":"themes/butterfly/source/css/_page/archives.styl","hash":"6874adc2e276443f354bbe50d0072e9bec37243c","modified":1632560182642},{"_id":"themes/butterfly/source/css/_mode/darkmode.styl","hash":"7a5d2937a7e5196d8659610c0b25e44656ddc019","modified":1632637024292},{"_id":"themes/butterfly/source/css/_page/categories.styl","hash":"e554549f0a0ae85362f0b0e8687981741f486f6b","modified":1632560182642},{"_id":"themes/butterfly/source/css/_page/flink.styl","hash":"2cc49d3f6a6beb9f7bff93e292f88aa5681da1d0","modified":1632560182642},{"_id":"themes/butterfly/source/css/_page/tags.styl","hash":"9a881c031f463c486bd25248c2814fd09f97892b","modified":1632560182642},{"_id":"themes/butterfly/source/css/_layout/chat.styl","hash":"29f48f9370f245e6e575b5836bccf47eb5688d8b","modified":1632560182641},{"_id":"themes/butterfly/source/css/_page/common.styl","hash":"97fec1e814f88237862f4d800a35362b802f6625","modified":1632629236001},{"_id":"themes/butterfly/source/css/_layout/footer.styl","hash":"dd8cdf639ba2b726437c77fa7aa8d5edbabe8f9b","modified":1632560182641},{"_id":"themes/butterfly/source/css/_page/homepage.styl","hash":"120cb840e5b422c604f5fc4c4fc3b01951a25aea","modified":1632629757454},{"_id":"themes/butterfly/source/css/_layout/head.styl","hash":"98235fcda3b87ad6f7e91eafbed94d0d6ae847ca","modified":1632560182641},{"_id":"themes/butterfly/source/css/_layout/loading.styl","hash":"7d18a7be9cfea65091de3ef00014063d2d649912","modified":1632560182641},{"_id":"themes/butterfly/source/css/_layout/pagination.styl","hash":"90fe01c968696a9f791cb2b84fca621cbbb56f47","modified":1632560182641},{"_id":"themes/butterfly/source/css/_layout/post.styl","hash":"d748951d9fbcd04dda839085af78b01b8fa04cba","modified":1632560182641},{"_id":"themes/butterfly/source/css/_layout/aside.styl","hash":"933bc99439661e0d1578105f12fe18d1e918da44","modified":1632629042727},{"_id":"themes/butterfly/source/css/_layout/comments.styl","hash":"f1b63892baafa48ab872bc79671d57aafd511f6c","modified":1632628843591},{"_id":"themes/butterfly/source/css/_layout/relatedposts.styl","hash":"0551c5893d1589a3d17ce161e50ecb1d724cc6e8","modified":1632560182641},{"_id":"themes/butterfly/source/css/_layout/reward.styl","hash":"ea1ba40dd5954c2ed718a126336fb7f94da4e66f","modified":1632560182641},{"_id":"themes/butterfly/source/css/_layout/sidebar.styl","hash":"2c5fb77c448ce0a734040c8ce532b28fed688899","modified":1632560182641},{"_id":"themes/butterfly/source/css/_layout/third-party.styl","hash":"978c397d0966eaf9e6e2afd13866f8f4900b509f","modified":1632560182642},{"_id":"themes/butterfly/source/css/_search/algolia.styl","hash":"917e0e399e117217184ca63d3eb5c4843bcccf7b","modified":1632560182642},{"_id":"themes/butterfly/source/css/_search/index.styl","hash":"f168f5c669978f633abe118cdcc4a12cfc883c01","modified":1632560182642},{"_id":"themes/butterfly/source/css/_tags/button.styl","hash":"1c3f9d7efc3b9dfcfa8926a1132d0c44ffc7d4b2","modified":1632560182642},{"_id":"themes/butterfly/source/css/_tags/gallery.styl","hash":"53ecae272e16223a436c497abbf25dd5f0fc4aaa","modified":1632560182643},{"_id":"themes/butterfly/source/css/_tags/hexo.styl","hash":"d0386ba6d8d63afc72b9673e8f3e89df6446ffc2","modified":1632560182643},{"_id":"themes/butterfly/source/css/_search/local-search.styl","hash":"6befe4c51b86d0c1de130beeecad9e28d6442713","modified":1632637178633},{"_id":"themes/butterfly/source/css/_tags/hide.styl","hash":"21964fdd6d74ffbea519418bab65024aee5f3736","modified":1632560182643},{"_id":"themes/butterfly/source/css/_tags/inlineImg.styl","hash":"df9d405c33a9a68946b530410f64096bcb72560c","modified":1632560182643},{"_id":"themes/butterfly/source/css/_layout/rightside.styl","hash":"c08d57733da9417164284f9c569ef612b334af9c","modified":1632628190379},{"_id":"themes/butterfly/source/css/_tags/label.styl","hash":"f741e85295ce15c70a6027ec15a542636dd5dcca","modified":1632560182643},{"_id":"themes/butterfly/source/css/_tags/note.styl","hash":"86fee274a62f7f034547342930f445c47378eb55","modified":1632560182643},{"_id":"themes/butterfly/source/css/_tags/tabs.styl","hash":"1756791581c0ec51cb03353a09dac4778d944349","modified":1632560182644},{"_id":"themes/butterfly/source/js/search/algolia.js","hash":"65b45e61586f7e66c3f338370bfd9daadd71a4b7","modified":1632560182645},{"_id":"themes/butterfly/source/css/_third-party/normalize.min.css","hash":"2c18a1c9604af475b4749def8f1959df88d8b276","modified":1632560182644},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/disqus.pug","hash":"d85c3737b5c9548553a78b757a7698df126a52cf","modified":1632560182635},{"_id":"themes/butterfly/source/js/search/local-search.js","hash":"b1429e9f80ef6b9a77434819ffb87d90bdad25e8","modified":1632560182645},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/fb.pug","hash":"7848ec58c6ec03243abf80a3b22b4dc10f3edf53","modified":1632560182635},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/twikoo.pug","hash":"ef1b2b5b980d6aeaa5d06b97d1afc9644b155a16","modified":1632560182635},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/waline.pug","hash":"400ce038548d6f9ddb486150c724c87b6923a88b","modified":1632560182635},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/valine.pug","hash":"bba9871f446c10ffcc8fa9023f5a2eb701a86bae","modified":1632560182635},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/index.pug","hash":"e3bf847553515174f6085df982f0623e9783db7a","modified":1632560182635},{"_id":"themes/butterfly/layout/includes/third-party/chat/chatra.pug","hash":"481cd5053bafb1a19f623554a27d3aa077ea59c3","modified":1632560182635},{"_id":"themes/butterfly/layout/includes/third-party/chat/crisp.pug","hash":"76634112c64023177260d1317ae39cef2a68e35f","modified":1632560182635},{"_id":"themes/butterfly/layout/includes/third-party/chat/daovoice.pug","hash":"cfe63e7d26a6665df6aa32ca90868ad48e05ec04","modified":1632560182635},{"_id":"themes/butterfly/layout/includes/third-party/chat/index.pug","hash":"3f05f8311ae559d768ee3d0925e84ed767c314d3","modified":1632560182635},{"_id":"themes/butterfly/layout/includes/third-party/chat/tidio.pug","hash":"24a926756c2300b9c561aaab6bd3a71fdd16e16d","modified":1632560182635},{"_id":"themes/butterfly/layout/includes/third-party/chat/gitter.pug","hash":"d1d2474420bf4edc2e43ccdff6f92b8b082143df","modified":1632560182635},{"_id":"themes/butterfly/layout/includes/third-party/comments/disqus.pug","hash":"a111407fdcafcf1099e26ffa69786f8822c5d9fb","modified":1632560182635},{"_id":"themes/butterfly/layout/includes/third-party/comments/facebook_comments.pug","hash":"c46a932257212f82e4a9974fbbc5de8878c8b383","modified":1632560182635},{"_id":"themes/butterfly/layout/includes/third-party/comments/gitalk.pug","hash":"0b7571919e8ad51285deda56a1868fccf8c563d7","modified":1632560182636},{"_id":"themes/butterfly/layout/includes/third-party/comments/disqusjs.pug","hash":"2e52c64e89f16267596a8465841dd46f51820982","modified":1632560182635},{"_id":"themes/butterfly/layout/includes/third-party/comments/index.pug","hash":"da9813f8dc0d388869c15413cf056012cfb69e1a","modified":1632560182636},{"_id":"themes/butterfly/layout/includes/third-party/comments/js.pug","hash":"bafb3d5710824caa59a56017afb058fd2b4eac65","modified":1632560182636},{"_id":"themes/butterfly/layout/includes/third-party/comments/livere.pug","hash":"52ea8aa26b84d3ad38ae28cdf0f163e9ca8dced7","modified":1632560182636},{"_id":"themes/butterfly/layout/includes/third-party/comments/twikoo.pug","hash":"16378d8646ea3f4ac99c18f0296dd85b13f9d775","modified":1632560182636},{"_id":"themes/butterfly/layout/includes/third-party/comments/utterances.pug","hash":"b871ea208e36398b4d668db9a9a0b61c79415381","modified":1632560182636},{"_id":"themes/butterfly/layout/includes/third-party/comments/valine.pug","hash":"2b45fe09d5b591dca156b76dae99981f8d8e1c61","modified":1632560182636},{"_id":"themes/butterfly/layout/includes/third-party/comments/waline.pug","hash":"36f3c603d2a2ecddaa6d2675a89d76ad94968f72","modified":1632560182636},{"_id":"themes/butterfly/layout/includes/third-party/math/index.pug","hash":"b8ae5fd7d74e1edcef21f5004fc96147e064d219","modified":1632560182636},{"_id":"themes/butterfly/layout/includes/third-party/math/mathjax.pug","hash":"a47d8f9f593091cc91192c0c49deaa2c0d2317fd","modified":1632560182636},{"_id":"themes/butterfly/layout/includes/third-party/math/mermaid.pug","hash":"3f3a3cd8bea2103dedd754f767aca5cb84d5f586","modified":1632560182636},{"_id":"themes/butterfly/layout/includes/third-party/math/katex.pug","hash":"f9b00ead54573ba6e6eb33481588af144aab648d","modified":1632560182636},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/github-issues.pug","hash":"34088a15655704d12e9b1807b47b3f6a860c9eec","modified":1632560182637},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/disqus-comment.pug","hash":"b443d6b16baf3ea250041342cc0361a42a412b7f","modified":1632560182637},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/index.pug","hash":"f6506ccfd1ce994b9e53aa95588d0b6dbad11411","modified":1632560182637},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/valine.pug","hash":"59b4c26a827ace5a54855881d199977103ff6f50","modified":1632560182637},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/waline.pug","hash":"a2bc2601b7e0ae5caf1fc51a07390562d928620a","modified":1632560182637},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/twikoo-comment.pug","hash":"cb38ffe911023092a90a28f2ba8317a92b22cd0c","modified":1632560182637},{"_id":"themes/butterfly/layout/includes/third-party/search/algolia.pug","hash":"d8f59e94eafc669c49349561dc5bbea3915aecb7","modified":1632560182637},{"_id":"themes/butterfly/layout/includes/third-party/search/index.pug","hash":"da3b9437d061ee68dbc383057db5c73034c49605","modified":1632560182637},{"_id":"themes/butterfly/layout/includes/third-party/search/local-search.pug","hash":"613280d61b8ab9612014ec016ae3d3698d36fd1a","modified":1632560182637},{"_id":"themes/butterfly/layout/includes/third-party/share/add-this.pug","hash":"2980f1889226ca981aa23b8eb1853fde26dcf89a","modified":1632560182637},{"_id":"themes/butterfly/layout/includes/third-party/share/index.pug","hash":"4c4a9c15215ae8ac5eadb0e086b278f76db9ee92","modified":1632560182638},{"_id":"themes/butterfly/layout/includes/third-party/share/share-js.pug","hash":"006acc91ce25fc7c7d778ca043e970f57dc46b83","modified":1632560182638},{"_id":"themes/butterfly/layout/includes/third-party/share/addtoany.pug","hash":"309f51bc5302e72fc469d54c577fbcfe57fb07a8","modified":1632560182637},{"_id":"themes/butterfly/source/css/_highlight/highlight/diff.styl","hash":"8c0726fb8d9a497d2f900b0be2845efaa68e3d87","modified":1632560182640},{"_id":"themes/butterfly/source/css/_highlight/highlight/index.styl","hash":"89cbcc8e087788ecec18b5fa58710afacdb7d080","modified":1632560182640},{"_id":"themes/butterfly/source/css/_highlight/prismjs/diff.styl","hash":"5972c61f5125068cbe0af279a0c93a54847fdc3b","modified":1632560182640},{"_id":"themes/butterfly/source/css/_highlight/prismjs/index.styl","hash":"e0e7065124ef0d99f8322a47bc47838982e04ad0","modified":1632560182641},{"_id":"themes/butterfly/source/css/_highlight/prismjs/line-number.styl","hash":"8970cc1916c982b64a1478792b2822d1d31e276d","modified":1632560182641},{"_id":"themes/butterfly/source/img/avatar.gif","hash":"7b8c80efd7035aca84264ff72875f346624cde06","modified":1625573955675},{"_id":"themes/butterfly/source/img/logo.svg","hash":"50b86b07d9fcec2b5660a4bf1b1769abcb1ae7b7","modified":1625570694993},{"_id":"themes/butterfly/source/img/weixin.png","hash":"75673fc76ffc552cb7fed5959242b4b9966434c2","modified":1632564089224},{"_id":"themes/butterfly/source/img/zhifubao.jpg","hash":"7759d421a5b22d4bf3182f4b52f2b0b557f7620c","modified":1632564179859},{"_id":"source/_posts/Anaconda环境日志/图4.jpg","hash":"a2d0ed9ac72ae6c73d83c3c279265013cab1506f","modified":1628345478307},{"_id":"themes/butterfly/source/img/类.png","hash":"da86c5884c4f0ed211838399372be8419e9c3a9f","modified":1632562250335},{"_id":"source/_posts/Anaconda环境日志/图2.png","hash":"462588e671814606e459dce9e848db714f2797e5","modified":1628345458342},{"_id":"themes/butterfly/source/img/封面原稿.png","hash":"722e9d2916d57dc2f5aa37e45a7bc509e8aa917a","modified":1632636233485},{"_id":"public/baidusitemap.xml","hash":"d509e794453a0becbd5c390e3bb61d945ec70fa0","modified":1632726303624},{"_id":"public/sitemap.xml","hash":"43d4ef312134679b6a2928a82d17cae967345f85","modified":1632726303624},{"_id":"public/search.xml","hash":"d3f069b19ad2b03ac02ba85bdeb2a17b8c248798","modified":1632726303624},{"_id":"public/tags/index.html","hash":"358d3528c5c928a72dba5475a85067dbf0c3778b","modified":1632726303624},{"_id":"public/about/index.html","hash":"d44aab62909eed579e6dc7945783b538b8891d86","modified":1632726303624},{"_id":"public/photos/index.html","hash":"01b9daa5e30e1a4c654390bf4fad723d255f8b75","modified":1632726303624},{"_id":"public/categories/index.html","hash":"f84b10d8508aea3915551b92048b0d4056006b01","modified":1632726303624},{"_id":"public/2021/08/07/Pytorch·API部署WEB·ONNX/index.html","hash":"9d656304a3e21f9fcc3f33b3d7f13d1d789f40c7","modified":1632710154143},{"_id":"public/2021/08/07/Pytorch·GPU训练-单-多/index.html","hash":"857fed9c906791a97efb7431f839aa3ab8c71d52","modified":1632710154143},{"_id":"public/2021/08/07/Keras·GPU训练（单-多）/index.html","hash":"7f7b897a7b0a0b4a39aac48f10c39935a5229485","modified":1632710154143},{"_id":"public/2021/08/07/TensorboardX训练可视化/index.html","hash":"c7d2ba64a7e471d71e20a16c6afa31d0ff0f3865","modified":1632710154143},{"_id":"public/2021/08/07/分割任务下数据集增广/index.html","hash":"6e3ffb4db8fc48e8b526c4dfd230fc2e235a5bfd","modified":1632710154143},{"_id":"public/2021/08/07/损失函数与语义分割任务/index.html","hash":"b19a4bf60f044136e91ac701c8941543c5e04c33","modified":1632710154143},{"_id":"public/2021/08/07/语义分割网络模型笔记/index.html","hash":"1e859646aef75a73e5ecddc603a0d3b68088e25b","modified":1632710154143},{"_id":"public/2021/08/07/Anaconda环境日志/index.html","hash":"c6528c4083f5032431e12c2c61a6ead7601a95f8","modified":1632726303624},{"_id":"public/2021/07/12/GIS的前沿现状/index.html","hash":"c64f9a4a692b3cda9e360ac69e4ac92d8f8aa924","modified":1632710154143},{"_id":"public/2021/07/12/GIS的应用/index.html","hash":"dcb4b91007bd7cc704db25bf8653ab70578a6c40","modified":1632710154143},{"_id":"public/2021/07/12/GISer的思想/index.html","hash":"e9cb5cdb9a092ea91f75c88a9ebc237d83c1cd71","modified":1632710154143},{"_id":"public/2021/07/12/GIS的简要定义/index.html","hash":"09787e87db38ca7ac5d1fa33be727bb359fc5ddb","modified":1632710154143},{"_id":"public/2021/07/06/美食之美/index.html","hash":"18c9cb29f4a3e1aad9b29f9fc052a463dd74f2d7","modified":1632710154143},{"_id":"public/archives/page/2/index.html","hash":"c98849007c1f4b3676b45b503b88da601d75157a","modified":1632726303624},{"_id":"public/archives/index.html","hash":"163eaf7d7803e43d5302f353401cf7455f77f052","modified":1632726303624},{"_id":"public/archives/2021/index.html","hash":"ca6ffaf0990a45236606f866b060333a7395285c","modified":1632726303624},{"_id":"public/archives/2021/page/2/index.html","hash":"0aef24d0d57aaf6bbd5991d1e0449dded784f33b","modified":1632726303624},{"_id":"public/archives/2021/07/index.html","hash":"b75acf35b7a84f0b78b34a133418dbdf453aca2b","modified":1632710154143},{"_id":"public/archives/2021/08/index.html","hash":"7f6e0ff4a9b92bf4c08f6bdc4491ffccbdae643a","modified":1632726303624},{"_id":"public/categories/计算机视觉特辑/index.html","hash":"1cd0b47a53c18c91c02e72c552613effa4e22397","modified":1632710154143},{"_id":"public/categories/生活随笔/index.html","hash":"ff16eef0d6010e3563fa6a504ce0e406f03bb149","modified":1632710154143},{"_id":"public/categories/地信原理特辑/index.html","hash":"7236e978a9bb105db8ec6b43de7a0f7ef46b8756","modified":1632710154143},{"_id":"public/index.html","hash":"45afe4583206ba18523675d2af2c7def17f45623","modified":1632726303624},{"_id":"public/tags/深度学习/index.html","hash":"9fcc3491f7b0f41fc862169d62312ce18855b2df","modified":1632710154143},{"_id":"public/page/2/index.html","hash":"89412acbeb8ec470615fd0a69b67aae0def154f5","modified":1632726303624},{"_id":"public/tags/语义分割/index.html","hash":"e9f4001aa452e7b32ec8950a74d31256d67c34cd","modified":1632710154143},{"_id":"public/tags/Anaconda/index.html","hash":"1a59b512117787c4125d66b5ba2567f6892736f3","modified":1632710154143},{"_id":"public/tags/Python/index.html","hash":"1f10dd9fe3501eeef4bf3ef1515198b68ea7201e","modified":1632710154143},{"_id":"public/tags/GIS/index.html","hash":"9aa64cabfda016934948ddde42c1d2e1624ba9a7","modified":1632710154143},{"_id":"public/tags/GISer/index.html","hash":"6bb8c3521a9dd1ba62a58b9bea7b7c3e10c633bc","modified":1632710154143},{"_id":"public/tags/思想/index.html","hash":"507ea6a81166b6a3947ab0a87beef0906aef680a","modified":1632710154143},{"_id":"public/tags/Keras/index.html","hash":"3b9550de286354167c758e4dcb26e3b735f8381b","modified":1632710154143},{"_id":"public/tags/Pytorch/index.html","hash":"8be1374350fbde44b6297b91624f3c54f805dd73","modified":1632710154143},{"_id":"public/tags/ONNX/index.html","hash":"c10f878e8809781c392a607df23e3f099389f4d4","modified":1632710154143},{"_id":"public/tags/Web开发/index.html","hash":"b8ea350a1d47b1c46a0689e8d7a66bcadd4ad8ca","modified":1632710154143},{"_id":"public/tags/可视化/index.html","hash":"c0f68ba5fe49a98a49012a3934e65b7c68649346","modified":1632710154143},{"_id":"public/tags/TensorboardX/index.html","hash":"2cab51c604e4a76cc64fe5c8c4f6d1b8e406b14b","modified":1632710154143},{"_id":"public/tags/数据/index.html","hash":"50362e34512bcd6541f94b04799ee391b095a19d","modified":1632710154143},{"_id":"public/tags/美食/index.html","hash":"8bb109d842d2e1e50b28339deddc218a49a75989","modified":1632710154143},{"_id":"public/CNAME","hash":"50464c10e057410560109446a31a76a6332b7ed8","modified":1632710154143},{"_id":"public/img/algolia.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1632710154143},{"_id":"public/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1632710154143},{"_id":"public/img/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1632710154143},{"_id":"public/img/favicon.png","hash":"e30b7d837b4123552976ff87380dc679e8c613c9","modified":1632710154143},{"_id":"public/img/apple-touch-icon-next.png","hash":"8b7e5d9f7b60baed4db48d8ae5c57b10a4ed2996","modified":1632710154143},{"_id":"public/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1632710154143},{"_id":"public/img/loading.gif","hash":"59ab4bff2c1b95fc8c00de9af99ac781125db443","modified":1632710154143},{"_id":"public/2021/08/07/Keras·GPU训练（单-多）/图2.png","hash":"c29cd7f6886ca460e0d11bc9ea059323ac847204","modified":1632710154143},{"_id":"public/2021/08/07/Keras·GPU训练（单-多）/图3.png","hash":"1c422fafc65871aba643217bd981f82fdfabd785","modified":1632710154143},{"_id":"public/2021/08/07/Keras·GPU训练（单-多）/图6.png","hash":"fa46d05d9e4363beba987a8d72585e88b57ecdfc","modified":1632710154143},{"_id":"public/2021/08/07/Keras·GPU训练（单-多）/图8.png","hash":"bc157f73febe73ca638dba7dfbfe387786974ffe","modified":1632710154143},{"_id":"public/2021/08/07/TensorboardX训练可视化/图2.png","hash":"186fafb5c7f43b516231e12c6cdff5d438eb21e4","modified":1632710154143},{"_id":"public/2021/08/07/Keras·GPU训练（单-多）/图7.png","hash":"51ca9fdfa66aac074da84e7ab5e98ea55dcd41f7","modified":1632710154143},{"_id":"public/2021/08/07/Keras·GPU训练（单-多）/图9.png","hash":"104d91db94ece58ccf45a85204c019b95c5324d3","modified":1632710154143},{"_id":"public/2021/08/07/TensorboardX训练可视化/图4.png","hash":"02bfbdff299f4e8da08b5e19fb1e66e9343a962c","modified":1632710154143},{"_id":"public/2021/08/07/TensorboardX训练可视化/图1.png","hash":"dfac55a597500d3a296b3ddb5ebcf8dc9f395f91","modified":1632710154143},{"_id":"public/2021/08/07/TensorboardX训练可视化/图6.png","hash":"086ebb091448f227d7264ba409ae0cc0980d9389","modified":1632710154143},{"_id":"public/2021/08/07/TensorboardX训练可视化/图7.png","hash":"7ff836bc832d52b5b44cec499f62118fd3702435","modified":1632710154143},{"_id":"public/2021/08/07/Keras·GPU训练（单-多）/图5.png","hash":"40d29cf2f8a172c8140a7fcfa7714bfa7dfd21fd","modified":1632710154143},{"_id":"public/2021/08/07/TensorboardX训练可视化/图8.png","hash":"4669aa6c432d9a709f3e94b81f98541a92a1b3a3","modified":1632710154143},{"_id":"public/2021/08/07/TensorboardX训练可视化/图9.gif","hash":"2cb5fe15b54329d2481c97cb092fe4b1e4d502b9","modified":1632710154143},{"_id":"public/2021/08/07/Pytorch·API部署WEB·ONNX/图10.png","hash":"ce2f03a12fa9f199ff47f2872a65abf8cd06c376","modified":1632710154143},{"_id":"public/2021/08/07/Pytorch·API部署WEB·ONNX/图1.png","hash":"f2eb62463dd9ef45312bf96ad0cf2cd79489bb80","modified":1632710154143},{"_id":"public/2021/08/07/Pytorch·API部署WEB·ONNX/图5.png","hash":"ff7ed6c6a7a7136e15f08a2e7f7011efb341907f","modified":1632710154143},{"_id":"public/2021/08/07/Pytorch·API部署WEB·ONNX/图3.png","hash":"48a5bc13c8f952ced8f02ef412c11a098f979b6a","modified":1632710154143},{"_id":"public/2021/08/07/Pytorch·API部署WEB·ONNX/图7.png","hash":"934c34b77d6bd2bd762a14fda27eb03dd9049cd2","modified":1632710154143},{"_id":"public/2021/08/07/Pytorch·API部署WEB·ONNX/图4.png","hash":"eb9a8257500202daf635f6d66468864a6381ca66","modified":1632710154143},{"_id":"public/2021/08/07/Pytorch·GPU训练-单-多/图1.png","hash":"26bd8275e2e18a9a5ab1b68e85c55f4dd5aed832","modified":1632710154143},{"_id":"public/2021/08/07/Pytorch·API部署WEB·ONNX/图8.png","hash":"f4abad45c4ee0c78c1aca02f2dba042bc8b3c792","modified":1632710154143},{"_id":"public/2021/08/07/Pytorch·API部署WEB·ONNX/图6.png","hash":"e86993a34016ac9dad9874b9b900b1f6b1e637fc","modified":1632710154143},{"_id":"public/2021/08/07/Pytorch·API部署WEB·ONNX/图2.png","hash":"b8c63e9788e937c7e075ad509998e146924b7807","modified":1632710154143},{"_id":"public/2021/08/07/Pytorch·API部署WEB·ONNX/图9.png","hash":"e014f6ede3b440aaa880c97d916760ccd909e7e4","modified":1632710154143},{"_id":"public/2021/08/07/损失函数与语义分割任务/图2.png","hash":"c7cf04aa61eeb3e200677eb937d922bc06f2286a","modified":1632710154143},{"_id":"public/2021/08/07/损失函数与语义分割任务/图4.png","hash":"74845a11a80d351223766426f11eea7a42c4668a","modified":1632710154143},{"_id":"public/2021/08/07/损失函数与语义分割任务/图3.png","hash":"c3db3bc33b9baa19b8d7018de9b3f3de1c3f11c0","modified":1632710154143},{"_id":"public/2021/08/07/损失函数与语义分割任务/图7.png","hash":"b18d25481683aeca98a837e11c1dd5f61a2a7572","modified":1632710154143},{"_id":"public/2021/08/07/损失函数与语义分割任务/图6.png","hash":"fd8d371b7e72fdd99d54ca55201fca1c62f0a837","modified":1632710154143},{"_id":"public/2021/08/07/TensorboardX训练可视化/图5.png","hash":"f640efd68535922f4435116babd6184086264f0c","modified":1632710154143},{"_id":"public/live2dw/assets/mtn/01.mtn","hash":"fb550833ae22c9954c3e01df37ed29b2d61700f2","modified":1632710154143},{"_id":"public/live2dw/assets/mtn/00_idle.mtn","hash":"b224c60e463b9f71ddbfc0c720e430496c175f4f","modified":1632710154143},{"_id":"public/live2dw/assets/mtn/02.mtn","hash":"7eafc52edc73b7cb80ae70d34b43c6ac778fa47b","modified":1632710154143},{"_id":"public/live2dw/assets/mtn/03.mtn","hash":"f900737c7a98441cbb2e05255427e6260e19ae68","modified":1632710154143},{"_id":"public/live2dw/assets/mtn/05.mtn","hash":"dd20ad24b5d1830a5d44b9bccb28f922eea5e0e5","modified":1632710154143},{"_id":"public/live2dw/assets/mtn/04.mtn","hash":"c7a25d3c5d783639bae18db2f3cd284b819c3c85","modified":1632710154143},{"_id":"public/live2dw/assets/mtn/07.mtn","hash":"b7f2e3a9fa4f3ffbb6e64a08f8d9f45ca1868ffb","modified":1632710154143},{"_id":"public/live2dw/assets/tororo.model.json","hash":"3b96ea33460642d288c98327444966d93a0c11ba","modified":1632710154143},{"_id":"public/live2dw/assets/mtn/08.mtn","hash":"4411c7651ff65195b113d95e7d5ebef8a59a37d9","modified":1632710154143},{"_id":"public/live2dw/assets/tororo.pose.json","hash":"81438bf69b32c7c11e311b4fe043730cdc7b7ec2","modified":1632710154143},{"_id":"public/live2dw/assets/mtn/06.mtn","hash":"ad404bd852d276cdd3d054c953e23f90e4e45ae1","modified":1632710154143},{"_id":"public/live2dw/lib/L2Dwidget.min.js","hash":"5f1a807437cc723bcadc3791d37add5ceed566a2","modified":1632710154143},{"_id":"public/img/avatar.gif","hash":"7b8c80efd7035aca84264ff72875f346624cde06","modified":1632710154143},{"_id":"public/img/weixin.png","hash":"75673fc76ffc552cb7fed5959242b4b9966434c2","modified":1632710154143},{"_id":"public/img/logo.svg","hash":"50b86b07d9fcec2b5660a4bf1b1769abcb1ae7b7","modified":1632710154143},{"_id":"public/2021/08/07/Anaconda环境日志/图1.png","hash":"c3ce12af475627eb519198b709538e900d0df8d7","modified":1632710154143},{"_id":"public/2021/08/07/Anaconda环境日志/图3.png","hash":"2add624209936c9c0ace5288888752a59fc2914b","modified":1632710154143},{"_id":"public/2021/07/12/GIS的应用/图1.png","hash":"442326c73998a3928bfae624f1e9cfc6a79c90cc","modified":1632710154143},{"_id":"public/2021/08/07/Keras·GPU训练（单-多）/图4.png","hash":"bdd2e24ce284e651b2be49aafe82633952473c2d","modified":1632710154143},{"_id":"public/2021/08/07/语义分割网络模型笔记/图1.jpg","hash":"d78a5357ee602caa99e9a515380b1a54ebdd8825","modified":1632710154143},{"_id":"public/2021/08/07/损失函数与语义分割任务/图5.png","hash":"b4875c91ad35be184411874c858e4913314bf5b5","modified":1632710154143},{"_id":"public/2021/08/07/语义分割网络模型笔记/图5.jpg","hash":"1d32fd03cc29ea18e146c446bce48a03c47a2091","modified":1632710154143},{"_id":"public/2021/08/07/语义分割网络模型笔记/图3.jpg","hash":"5a17e9d398a94f466879c0d106c42d30314b17a1","modified":1632710154143},{"_id":"public/live2dw/lib/L2Dwidget.min.js.map","hash":"3290fe2df45f065b51a1cd7b24ec325cbf9bb5ce","modified":1632710154143},{"_id":"public/about/index/weixin.jpg","hash":"bef1f685d45963a0308deac4b2395ae5ecd8cee2","modified":1632710154143},{"_id":"public/img/zhifubao.jpg","hash":"7759d421a5b22d4bf3182f4b52f2b0b557f7620c","modified":1632710154143},{"_id":"public/2021/08/07/TensorboardX训练可视化/图3.png","hash":"63b21576609627b3d9d4e4ad059e39a267b86d6e","modified":1632710154143},{"_id":"public/2021/08/07/分割任务下数据集增广/图1.png","hash":"b4f53494c472f15100bc82fb61c7cd3db340dea6","modified":1632710154143},{"_id":"public/2021/08/07/Keras·GPU训练（单-多）/图1.png","hash":"de643993afcf14f16330f442444ba14a8237653e","modified":1632710154143},{"_id":"public/2021/08/07/语义分割网络模型笔记/图6.jpg","hash":"b72c093509600dd35591e38ca27d62a0004474b2","modified":1632710154143},{"_id":"public/2021/08/07/语义分割网络模型笔记/图4.jpg","hash":"9d1d240f5ea3c8af9bcadaebc980ae4f1d5792f6","modified":1632710154143},{"_id":"public/2021/08/07/语义分割网络模型笔记/图2.jpg","hash":"424040a9ae9ebcc2ea6bca47cb7c9ca30767b4d5","modified":1632710154143},{"_id":"public/live2dw/assets/moc/tororo.moc","hash":"44289e62545a7046e0f5231103a851750b78524e","modified":1632710154143},{"_id":"public/live2dw/lib/L2Dwidget.0.min.js","hash":"35bb5b588b6de25c9be2dd51d3fd331feafac02d","modified":1632710154143},{"_id":"public/css/var.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1632710154143},{"_id":"public/js/utils.js","hash":"8319b59c26ce8cd2b0ae7d030c4912215148fa92","modified":1632710154143},{"_id":"public/js/search/algolia.js","hash":"65b45e61586f7e66c3f338370bfd9daadd71a4b7","modified":1632710154143},{"_id":"public/js/search/local-search.js","hash":"b1429e9f80ef6b9a77434819ffb87d90bdad25e8","modified":1632710154143},{"_id":"public/css/index.css","hash":"d5e6c0680d5f83f408bb8e1055f7cc3b58e4f114","modified":1632710154143},{"_id":"public/2021/07/12/GISer的思想/1.png","hash":"efda4cb11c6dc8b706648de170fed29cebe03d0b","modified":1632710154143},{"_id":"public/js/tw_cn.js","hash":"00053ce73210274b3679f42607edef1206eebc68","modified":1632710154143},{"_id":"public/js/main.js","hash":"b244f28124a46d7f1e8ef76ba6e925289691f93b","modified":1632710154143},{"_id":"public/about/index/zhifubao.jpg","hash":"7b71e940d3082e499ac071f95029e7fa8530f746","modified":1632710154143},{"_id":"public/2021/07/12/GIS的前沿现状/图1.png","hash":"c64839c1e4426e00fa0df83314f8c5fd9c3b1e7b","modified":1632710154143},{"_id":"public/live2dw/assets/moc/tororo.2048/texture_00.png","hash":"98af764b541083e87fc2f8e85f02d2db38c898cc","modified":1632710154143},{"_id":"public/2021/07/12/GIS的前沿现状/图2.png","hash":"103da15478f7457fbc3d5aad459edbdcf8fc7013","modified":1632710154143},{"_id":"public/2021/08/07/损失函数与语义分割任务/图1.png","hash":"f75108240d13f4bb3e2ec3a4932093ec5e2ec0b9","modified":1632710154143},{"_id":"public/live2dw/lib/L2Dwidget.0.min.js.map","hash":"35e71cc2a130199efb167b9a06939576602f0d75","modified":1632710154143},{"_id":"public/img/类.png","hash":"da86c5884c4f0ed211838399372be8419e9c3a9f","modified":1632710154143},{"_id":"public/2021/08/07/Anaconda环境日志/图4.jpg","hash":"a2d0ed9ac72ae6c73d83c3c279265013cab1506f","modified":1632710154143},{"_id":"public/2021/08/07/Anaconda环境日志/图2.png","hash":"462588e671814606e459dce9e848db714f2797e5","modified":1632710154143},{"_id":"public/img/封面原稿.png","hash":"722e9d2916d57dc2f5aa37e45a7bc509e8aa917a","modified":1632710154143},{"_id":"source/_posts/pklxqf.md","hash":"f240adecf38e94eb5bbfb898565de47bcf47232c","modified":1632726270450},{"_id":"source/_posts/grllbd.md","hash":"b26d4d24eb4015891dfaa159082267827a85c8d9","modified":1632726270513},{"_id":"source/_posts/ot04dg.md","hash":"ee32adecdb1934215e738a7e27b6a6bd396631c0","modified":1632726270476},{"_id":"source/_posts/mao648.md","hash":"b5b8b2316b57019dcd7efb0df3ec502520069b7a","modified":1632726270327},{"_id":"source/_posts/skxksy.md","hash":"dc843d2d7591b9f08ea9ca777fd6f1ef72432dbb","modified":1632726270415},{"_id":"source/_posts/tvbg16.md","hash":"a44be88452ad9f0c7de4e661198e5959e8348c29","modified":1632726270577},{"_id":"source/_posts/ug2u1x.md","hash":"53a7b58758ebdcd389b4307670539cf5881ef59d","modified":1632726270370},{"_id":"source/_posts/uqudv9.md","hash":"551203e5a883e15f53633b259afb03bebae026b1","modified":1632726270308},{"_id":"source/_posts/xkk3s7.md","hash":"567e12a6bbff9f089128d7384d7eff0c0d53eca9","modified":1632726270346},{"_id":"source/_posts/wgc27n.md","hash":"632aeed9bb4029b1c18971094eca311245f813fa","modified":1632726270435},{"_id":"source/_posts/xyq9rx.md","hash":"008596cb1f7851ec1b904f5806b6095aca6d527d","modified":1632726270387},{"_id":"source/_posts/zka8l2.md","hash":"934f988c53e3523c127e1ebaa02b761e105704f8","modified":1632726270267},{"_id":"source/_posts/xnqb2e.md","hash":"cf777ae9705afd5ed9bf1693f7bb50e9b52ae7b1","modified":1632726270558},{"_id":"source/_posts的副本/.DS_Store","hash":"6e516069ee4035d19c23c3e33405e8c847e1feab","modified":1632702858924},{"_id":"source/_posts的副本/GIS的应用.md","hash":"ac904026d34dfc630e7ce9736309889a50acd2ec","modified":1628344976488},{"_id":"source/_posts的副本/GIS的简要定义.md","hash":"5b0b717457f68436b991b602700f2033e60c9f3e","modified":1626188337577},{"_id":"source/_posts的副本/GISer的思想.md","hash":"9404c487714623df7a1a65d315b4e6a0f03b2e4f","modified":1626187926596},{"_id":"source/_posts的副本/Anaconda环境日志.md","hash":"cad70abf726e531dd0fc0aa2b492d0710b799fe3","modified":1628346020329},{"_id":"source/_posts的副本/Keras·GPU训练（单-多）.md","hash":"4e9232812ec3284d32ea37dbf4b7bf8f3efd7276","modified":1628351679097},{"_id":"source/_posts的副本/Pytorch·API部署WEB·ONNX.md","hash":"476c571d1bdb29ff1513ce1d76a1477eefe6a08c","modified":1628350754664},{"_id":"source/_posts的副本/TensorboardX训练可视化.md","hash":"00f72e075bb7638bb3daa70532fd503930b8362b","modified":1628349403553},{"_id":"source/_posts的副本/GIS的前沿现状.md","hash":"985d415129bb6a18f066e0bf44f15a423ab2de02","modified":1626188268923},{"_id":"source/_posts的副本/分割任务下数据集增广.md","hash":"7475d6c8f8bcf975b356e92b923b8e6505ccdcfc","modified":1629537478141},{"_id":"source/_posts的副本/Pytorch·GPU训练-单-多.md","hash":"4b1b133ac6f76d492c48769489e129b5a25f9457","modified":1628349856776},{"_id":"source/_posts的副本/损失函数与语义分割任务.md","hash":"d8ae29f792e107b065a796cf4e6ee8845a1ead5d","modified":1628347064497},{"_id":"source/_posts的副本/语义分割网络模型笔记.md","hash":"bc32aed0eb99d3b2a70e56307975705a94bc521c","modified":1628346505967},{"_id":"source/_posts的副本/美食之美.md","hash":"13383ede14d3aec9b00fc8ad99ba8411f2b84f13","modified":1632712003151},{"_id":"source/_posts的副本/GIS的应用/.DS_Store","hash":"1d375697da1d22c87c0b43707b6dd57dbf625635","modified":1626188039436},{"_id":"source/_posts的副本/Keras·GPU训练（单-多）/图2.png","hash":"c29cd7f6886ca460e0d11bc9ea059323ac847204","modified":1628351242161},{"_id":"source/_posts的副本/Keras·GPU训练（单-多）/图3.png","hash":"1c422fafc65871aba643217bd981f82fdfabd785","modified":1628351380834},{"_id":"source/_posts的副本/Keras·GPU训练（单-多）/图6.png","hash":"fa46d05d9e4363beba987a8d72585e88b57ecdfc","modified":1628351272179},{"_id":"source/_posts的副本/Keras·GPU训练（单-多）/图5.png","hash":"40d29cf2f8a172c8140a7fcfa7714bfa7dfd21fd","modified":1628351264026},{"_id":"source/_posts的副本/Keras·GPU训练（单-多）/图8.png","hash":"bc157f73febe73ca638dba7dfbfe387786974ffe","modified":1628351282945},{"_id":"source/_posts的副本/Keras·GPU训练（单-多）/图9.png","hash":"104d91db94ece58ccf45a85204c019b95c5324d3","modified":1628351287995},{"_id":"source/_posts的副本/Keras·GPU训练（单-多）/图7.png","hash":"51ca9fdfa66aac074da84e7ab5e98ea55dcd41f7","modified":1628351277991},{"_id":"source/_posts的副本/GIS的前沿现状/.DS_Store","hash":"ab04de3da934f6bf293d3645d8ad59951154d630","modified":1626188303052},{"_id":"source/_posts的副本/TensorboardX训练可视化/图1.png","hash":"dfac55a597500d3a296b3ddb5ebcf8dc9f395f91","modified":1628347470063},{"_id":"source/_posts的副本/Pytorch·GPU训练-单-多/图1.png","hash":"26bd8275e2e18a9a5ab1b68e85c55f4dd5aed832","modified":1628349734900},{"_id":"source/_posts的副本/TensorboardX训练可视化/图2.png","hash":"186fafb5c7f43b516231e12c6cdff5d438eb21e4","modified":1628347475826},{"_id":"source/_posts的副本/TensorboardX训练可视化/图6.png","hash":"086ebb091448f227d7264ba409ae0cc0980d9389","modified":1628347502350},{"_id":"source/_posts的副本/TensorboardX训练可视化/图4.png","hash":"02bfbdff299f4e8da08b5e19fb1e66e9343a962c","modified":1628347489642},{"_id":"source/_posts的副本/TensorboardX训练可视化/图5.png","hash":"f640efd68535922f4435116babd6184086264f0c","modified":1628347494753},{"_id":"source/_posts的副本/TensorboardX训练可视化/图8.png","hash":"4669aa6c432d9a709f3e94b81f98541a92a1b3a3","modified":1628347512618},{"_id":"source/_posts的副本/TensorboardX训练可视化/图7.png","hash":"7ff836bc832d52b5b44cec499f62118fd3702435","modified":1628347507661},{"_id":"source/_posts的副本/TensorboardX训练可视化/图9.gif","hash":"2cb5fe15b54329d2481c97cb092fe4b1e4d502b9","modified":1628347518909},{"_id":"source/_posts的副本/Pytorch·API部署WEB·ONNX/图3.png","hash":"48a5bc13c8f952ced8f02ef412c11a098f979b6a","modified":1628350119966},{"_id":"source/_posts的副本/Pytorch·API部署WEB·ONNX/图1.png","hash":"f2eb62463dd9ef45312bf96ad0cf2cd79489bb80","modified":1628350070462},{"_id":"source/_posts的副本/Pytorch·API部署WEB·ONNX/.DS_Store","hash":"ebf6aafb548470a2622beb2116b9f8b716890aff","modified":1628350786355},{"_id":"source/_posts的副本/Pytorch·API部署WEB·ONNX/图4.png","hash":"eb9a8257500202daf635f6d66468864a6381ca66","modified":1628350126629},{"_id":"source/_posts的副本/Pytorch·API部署WEB·ONNX/图10.png","hash":"ce2f03a12fa9f199ff47f2872a65abf8cd06c376","modified":1628350177252},{"_id":"source/_posts的副本/Pytorch·API部署WEB·ONNX/图5.png","hash":"ff7ed6c6a7a7136e15f08a2e7f7011efb341907f","modified":1628350150162},{"_id":"source/_posts的副本/Pytorch·API部署WEB·ONNX/图7.png","hash":"934c34b77d6bd2bd762a14fda27eb03dd9049cd2","modified":1628350160602},{"_id":"source/_posts的副本/Pytorch·API部署WEB·ONNX/图2.png","hash":"b8c63e9788e937c7e075ad509998e146924b7807","modified":1628350115081},{"_id":"source/_posts的副本/Pytorch·API部署WEB·ONNX/图6.png","hash":"e86993a34016ac9dad9874b9b900b1f6b1e637fc","modified":1628350155055},{"_id":"source/_posts的副本/Pytorch·API部署WEB·ONNX/图8.png","hash":"f4abad45c4ee0c78c1aca02f2dba042bc8b3c792","modified":1628350165845},{"_id":"source/_posts的副本/Pytorch·API部署WEB·ONNX/图9.png","hash":"e014f6ede3b440aaa880c97d916760ccd909e7e4","modified":1628350171134},{"_id":"source/_posts的副本/损失函数与语义分割任务/图2.png","hash":"c7cf04aa61eeb3e200677eb937d922bc06f2286a","modified":1628346823300},{"_id":"source/_posts的副本/损失函数与语义分割任务/图3.png","hash":"c3db3bc33b9baa19b8d7018de9b3f3de1c3f11c0","modified":1628346827957},{"_id":"source/_posts的副本/损失函数与语义分割任务/图4.png","hash":"74845a11a80d351223766426f11eea7a42c4668a","modified":1628346834309},{"_id":"source/_posts的副本/损失函数与语义分割任务/图6.png","hash":"fd8d371b7e72fdd99d54ca55201fca1c62f0a837","modified":1628346845394},{"_id":"source/_posts的副本/损失函数与语义分割任务/图7.png","hash":"b18d25481683aeca98a837e11c1dd5f61a2a7572","modified":1628346850433},{"_id":"source/_posts的副本/Anaconda环境日志/图1.png","hash":"c3ce12af475627eb519198b709538e900d0df8d7","modified":1628345446201},{"_id":"source/_posts的副本/Anaconda环境日志/图3.png","hash":"2add624209936c9c0ace5288888752a59fc2914b","modified":1628345467472},{"_id":"source/_posts的副本/GIS的应用/图1.png","hash":"442326c73998a3928bfae624f1e9cfc6a79c90cc","modified":1626188017715},{"_id":"source/_posts的副本/Keras·GPU训练（单-多）/图4.png","hash":"bdd2e24ce284e651b2be49aafe82633952473c2d","modified":1628351256628},{"_id":"source/_posts的副本/损失函数与语义分割任务/图5.png","hash":"b4875c91ad35be184411874c858e4913314bf5b5","modified":1628346839990},{"_id":"source/_posts的副本/语义分割网络模型笔记/图1.jpg","hash":"d78a5357ee602caa99e9a515380b1a54ebdd8825","modified":1628346205145},{"_id":"source/_posts的副本/语义分割网络模型笔记/图3.jpg","hash":"5a17e9d398a94f466879c0d106c42d30314b17a1","modified":1628346221111},{"_id":"source/_posts的副本/语义分割网络模型笔记/图5.jpg","hash":"1d32fd03cc29ea18e146c446bce48a03c47a2091","modified":1628346233798},{"_id":"source/_posts的副本/Keras·GPU训练（单-多）/图1.png","hash":"de643993afcf14f16330f442444ba14a8237653e","modified":1628351236803},{"_id":"source/_posts的副本/TensorboardX训练可视化/图3.png","hash":"63b21576609627b3d9d4e4ad059e39a267b86d6e","modified":1628347482895},{"_id":"source/_posts的副本/分割任务下数据集增广/图1.png","hash":"b4f53494c472f15100bc82fb61c7cd3db340dea6","modified":1628347262123},{"_id":"source/_posts的副本/语义分割网络模型笔记/图4.jpg","hash":"9d1d240f5ea3c8af9bcadaebc980ae4f1d5792f6","modified":1628346227640},{"_id":"source/_posts的副本/语义分割网络模型笔记/图2.jpg","hash":"424040a9ae9ebcc2ea6bca47cb7c9ca30767b4d5","modified":1628346214171},{"_id":"source/_posts的副本/语义分割网络模型笔记/图6.jpg","hash":"b72c093509600dd35591e38ca27d62a0004474b2","modified":1628346367698},{"_id":"source/_posts的副本/GISer的思想/1.png","hash":"efda4cb11c6dc8b706648de170fed29cebe03d0b","modified":1626098400679},{"_id":"source/_posts的副本/GIS的前沿现状/图1.png","hash":"c64839c1e4426e00fa0df83314f8c5fd9c3b1e7b","modified":1626188186762},{"_id":"source/_posts的副本/GIS的前沿现状/图2.png","hash":"103da15478f7457fbc3d5aad459edbdcf8fc7013","modified":1626188180572},{"_id":"source/_posts的副本/损失函数与语义分割任务/图1.png","hash":"f75108240d13f4bb3e2ec3a4932093ec5e2ec0b9","modified":1628346815002},{"_id":"source/_posts的副本/Anaconda环境日志/图4.jpg","hash":"a2d0ed9ac72ae6c73d83c3c279265013cab1506f","modified":1628345478307},{"_id":"source/_posts的副本/Anaconda环境日志/图2.png","hash":"462588e671814606e459dce9e848db714f2797e5","modified":1628345458342},{"_id":"public/2021/09/27/tvbg16/index.html","hash":"9c36f3c76b4c477cc5637d1140b25fbdf5247a1d","modified":1632726303624},{"_id":"public/2021/09/27/xnqb2e/index.html","hash":"90727b674c76ae5000ff0004b853f2d7fdafe2fc","modified":1632726303624},{"_id":"public/2021/09/27/ot04dg/index.html","hash":"d14f84993479d8df37da891398805dc3ef4d25b6","modified":1632726303624},{"_id":"public/2021/09/27/grllbd/index.html","hash":"b3c5a952801832414d27f73cb7e61a9ecde7d74e","modified":1632726303624},{"_id":"public/2021/09/27/skxksy/index.html","hash":"5b5bef4785a03e5ee3d581bac94a7571d495cdda","modified":1632726303624},{"_id":"public/2021/09/27/wgc27n/index.html","hash":"aa4bff7e67ce2cb6a1337c7297801a199d8d061e","modified":1632726303624},{"_id":"public/2021/09/27/xyq9rx/index.html","hash":"cf6045a014700fab2c2273c6a57013d61e86db60","modified":1632726303624},{"_id":"public/2021/09/27/ug2u1x/index.html","hash":"5e4216d5f8cc89231b04c8c397425a0cffa179fb","modified":1632726303624},{"_id":"public/2021/09/27/xkk3s7/index.html","hash":"fe570b8154b3dd5f296afb421ae9e77bd4521325","modified":1632726303624},{"_id":"public/2021/09/27/mao648/index.html","hash":"caae568d6d399bce781f7e9bcf42d1aa9f8343dc","modified":1632726303624},{"_id":"public/2021/09/27/uqudv9/index.html","hash":"db1187a79ead1971969cc8e2dbb9e1ad6bd4a9c1","modified":1632726303624},{"_id":"public/2021/09/27/zka8l2/index.html","hash":"4c6d9c0a46788a30b0f8adf916420eaf87403fdd","modified":1632726303624},{"_id":"public/archives/2021/09/index.html","hash":"44f78f9bd7d1b5f821b5b34c3e80efdff7e098e4","modified":1632726303624},{"_id":"public/archives/2021/09/page/2/index.html","hash":"24be7be54ea4ce80975926ed8ef976b0d3c8b805","modified":1632726303624},{"_id":"public/2021/09/27/pklxqf/index.html","hash":"9bd247be94bee48f4786922a39144d4e9131d7fa","modified":1632726303624}],"Category":[{"name":"计算机视觉特辑","_id":"cku21gy7n0004qua29s12cpwf"},{"name":"地信原理特辑","_id":"cku21gy7t000bqua24aqu1luu"},{"name":"生活随笔","_id":"cku21gy8k002jqua2cfxt0bbl"}],"Data":[],"Page":[{"title":"标","date":"2021-07-06T12:24:09.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: 标\ndate: 2021-07-06 20:24:09\ntype: \"tags\"\n---\n","updated":"2021-07-06T12:43:39.309Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cku21gy7c0000qua2ar933yel","content":"","site":{"data":{}},"cover":"/img/loading.gif","length":0,"excerpt":"","more":""},{"title":"categories","date":"2021-07-06T13:02:46.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2021-07-06 21:02:46\ntype: categories\n---\n","updated":"2021-07-06T13:03:20.275Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cku21gy7j0002qua24x7najyj","content":"","site":{"data":{}},"cover":"/img/loading.gif","length":0,"excerpt":"","more":""},{"title":"信·关于","date":"2021-07-06T08:36:26.000Z","type":"about","comments":0,"_content":"一、介\n    这里是林天文的博客，很想和大家分享知识，也想卑贱地要来点有趣的分享。\n二、警\n    本人博客仅此一家，不会设置主动的收费功能，如果有人以我的名义想你索要钱财，那都是害人不浅的坏蛋。我可不那么坏！当然，如果在本博客有所收获，也可以在文章页面投喂饭钱，谢谢，爱您嘻嘻。\n","source":"about/index.md","raw":"---\ntitle: 信·关于\ndate: 2021-07-6 16:36:26\ntype: \"about\"\ncomments: false\n---\n一、介\n    这里是林天文的博客，很想和大家分享知识，也想卑贱地要来点有趣的分享。\n二、警\n    本人博客仅此一家，不会设置主动的收费功能，如果有人以我的名义想你索要钱财，那都是害人不浅的坏蛋。我可不那么坏！当然，如果在本博客有所收获，也可以在文章页面投喂饭钱，谢谢，爱您嘻嘻。\n","updated":"2021-07-15T03:23:59.287Z","path":"about/index.html","layout":"page","_id":"cku21gy7q0006qua2cb9u2rs9","content":"<p>一、介<br>    这里是林天文的博客，很想和大家分享知识，也想卑贱地要来点有趣的分享。<br>二、警<br>    本人博客仅此一家，不会设置主动的收费功能，如果有人以我的名义想你索要钱财，那都是害人不浅的坏蛋。我可不那么坏！当然，如果在本博客有所收获，也可以在文章页面投喂饭钱，谢谢，爱您嘻嘻。</p>\n","site":{"data":{}},"cover":"/img/loading.gif","length":130,"excerpt":"","more":"<p>一、介<br>    这里是林天文的博客，很想和大家分享知识，也想卑贱地要来点有趣的分享。<br>二、警<br>    本人博客仅此一家，不会设置主动的收费功能，如果有人以我的名义想你索要钱财，那都是害人不浅的坏蛋。我可不那么坏！当然，如果在本博客有所收获，也可以在文章页面投喂饭钱，谢谢，爱您嘻嘻。</p>\n"},{"title":"百老汇","date":"2021-08-21T08:14:25.000Z","_content":"","source":"photos/index.md","raw":"---\ntitle: 百老汇\ndate: 2021-08-21 16:14:25\n---\n","updated":"2021-08-21T08:20:31.347Z","path":"photos/index.html","comments":1,"layout":"page","_id":"cku21gy7s0008qua2gpuy6chj","content":"","site":{"data":{}},"cover":"/img/loading.gif","length":0,"excerpt":"","more":""}],"Post":[{"_content":"---\n\n## title: Pytorch·GPU 训练(单/多)date: 2021-08-07 23:20:37\n\ntags: [深度学习, 语义分割, Pytorch]\ncategories: 计算机视觉特辑\n\n在一切开始前，请确定计算机拥有英伟达的显卡。\n\n（不是英特尔！不是英特尔！不是英特尔！）\nCUDA 安装参考该笔记：Keras·GPU 训练（单/多）\n\n1.依赖库版本：\n\n• Torch\n\n• Torchvision\n\n2.安装 PyTorch：\n\n• 进入 PyTorch 官网 Start Locally | PyTorch 选择版本，在 conda 运行所给安装语句\n\n![](Pytorch%C2%B7GPU%E8%AE%AD%E7%BB%83-%E5%8D%95-%E5%A4%9A/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n\n• 然后参照 Keras 的 gpu 环境配置，安装 CUDA11.1 和对应的 cudnn。\n\n3.单 gpu 训练\n\n• 输入如下语句，指定搜索 gpu 的起始位置\n\ndevice=torch.device(\"cuda:0\"iftorch.cuda.is_available()else\"cpu\")\n\n4.多 gpu 训练\n\n• Torch.nn.DataParallel\n\n○ 输入如下语句，指定搜索 gpu 的起始位置\n\ndevice=torch.device(\"cuda:0\"iftorch.cuda.is_available()else\"cpu\")\n\n○ 输入如下语句生成多 gpu 的 model\n\nPar_model = nn.DataParallel(MyNet())\n\nPar_model = Par_model.cuda(device)   #model 加载到 gpu\n","source":"_posts/mao648.md","raw":"---\n\n## title: Pytorch·GPU 训练(单/多)date: 2021-08-07 23:20:37\n\ntags: [深度学习, 语义分割, Pytorch]\ncategories: 计算机视觉特辑\n\n在一切开始前，请确定计算机拥有英伟达的显卡。\n\n（不是英特尔！不是英特尔！不是英特尔！）\nCUDA 安装参考该笔记：Keras·GPU 训练（单/多）\n\n1.依赖库版本：\n\n• Torch\n\n• Torchvision\n\n2.安装 PyTorch：\n\n• 进入 PyTorch 官网 Start Locally | PyTorch 选择版本，在 conda 运行所给安装语句\n\n![](Pytorch%C2%B7GPU%E8%AE%AD%E7%BB%83-%E5%8D%95-%E5%A4%9A/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n\n• 然后参照 Keras 的 gpu 环境配置，安装 CUDA11.1 和对应的 cudnn。\n\n3.单 gpu 训练\n\n• 输入如下语句，指定搜索 gpu 的起始位置\n\ndevice=torch.device(\"cuda:0\"iftorch.cuda.is_available()else\"cpu\")\n\n4.多 gpu 训练\n\n• Torch.nn.DataParallel\n\n○ 输入如下语句，指定搜索 gpu 的起始位置\n\ndevice=torch.device(\"cuda:0\"iftorch.cuda.is_available()else\"cpu\")\n\n○ 输入如下语句生成多 gpu 的 model\n\nPar_model = nn.DataParallel(MyNet())\n\nPar_model = Par_model.cuda(device)   #model 加载到 gpu\n","slug":"mao648","published":1,"date":"2021-09-27T07:04:30.327Z","updated":"2021-09-27T07:04:30.327Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cku2b35j20000d1a2e6bn6wex","content":"<hr>\n<h2 id=\"title-Pytorch·GPU-训练-单-多-date-2021-08-07-23-20-37\"><a href=\"#title-Pytorch·GPU-训练-单-多-date-2021-08-07-23-20-37\" class=\"headerlink\" title=\"title: Pytorch·GPU 训练(单/多)date: 2021-08-07 23:20:37\"></a>title: Pytorch·GPU 训练(单/多)date: 2021-08-07 23:20:37</h2><p>tags: [深度学习, 语义分割, Pytorch]<br>categories: 计算机视觉特辑</p>\n<p>在一切开始前，请确定计算机拥有英伟达的显卡。</p>\n<p>（不是英特尔！不是英特尔！不是英特尔！）<br>CUDA 安装参考该笔记：Keras·GPU 训练（单/多）</p>\n<p>1.依赖库版本：</p>\n<p>• Torch</p>\n<p>• Torchvision</p>\n<p>2.安装 PyTorch：</p>\n<p>• 进入 PyTorch 官网 Start Locally | PyTorch 选择版本，在 conda 运行所给安装语句</p>\n<p><img src=\"/2021/09/27/mao648/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n<p>• 然后参照 Keras 的 gpu 环境配置，安装 CUDA11.1 和对应的 cudnn。</p>\n<p>3.单 gpu 训练</p>\n<p>• 输入如下语句，指定搜索 gpu 的起始位置</p>\n<p>device=torch.device(“cuda:0”iftorch.cuda.is_available()else”cpu”)</p>\n<p>4.多 gpu 训练</p>\n<p>• Torch.nn.DataParallel</p>\n<p>○ 输入如下语句，指定搜索 gpu 的起始位置</p>\n<p>device=torch.device(“cuda:0”iftorch.cuda.is_available()else”cpu”)</p>\n<p>○ 输入如下语句生成多 gpu 的 model</p>\n<p>Par_model = nn.DataParallel(MyNet())</p>\n<p>Par_model = Par_model.cuda(device)   #model 加载到 gpu</p>\n","site":{"data":{}},"cover":"/img/loading.gif","length":592,"excerpt":"","more":"<hr>\n<h2 id=\"title-Pytorch·GPU-训练-单-多-date-2021-08-07-23-20-37\"><a href=\"#title-Pytorch·GPU-训练-单-多-date-2021-08-07-23-20-37\" class=\"headerlink\" title=\"title: Pytorch·GPU 训练(单/多)date: 2021-08-07 23:20:37\"></a>title: Pytorch·GPU 训练(单/多)date: 2021-08-07 23:20:37</h2><p>tags: [深度学习, 语义分割, Pytorch]<br>categories: 计算机视觉特辑</p>\n<p>在一切开始前，请确定计算机拥有英伟达的显卡。</p>\n<p>（不是英特尔！不是英特尔！不是英特尔！）<br>CUDA 安装参考该笔记：Keras·GPU 训练（单/多）</p>\n<p>1.依赖库版本：</p>\n<p>• Torch</p>\n<p>• Torchvision</p>\n<p>2.安装 PyTorch：</p>\n<p>• 进入 PyTorch 官网 Start Locally | PyTorch 选择版本，在 conda 运行所给安装语句</p>\n<p><img src=\"/2021/09/27/mao648/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n<p>• 然后参照 Keras 的 gpu 环境配置，安装 CUDA11.1 和对应的 cudnn。</p>\n<p>3.单 gpu 训练</p>\n<p>• 输入如下语句，指定搜索 gpu 的起始位置</p>\n<p>device=torch.device(“cuda:0”iftorch.cuda.is_available()else”cpu”)</p>\n<p>4.多 gpu 训练</p>\n<p>• Torch.nn.DataParallel</p>\n<p>○ 输入如下语句，指定搜索 gpu 的起始位置</p>\n<p>device=torch.device(“cuda:0”iftorch.cuda.is_available()else”cpu”)</p>\n<p>○ 输入如下语句生成多 gpu 的 model</p>\n<p>Par_model = nn.DataParallel(MyNet())</p>\n<p>Par_model = Par_model.cuda(device)   #model 加载到 gpu</p>\n"},{"_content":"---\n\n## title: 语义分割网络模型笔记 date: 2021-08-07 22:21:39\n\ntags: [深度学习, 语义分割]\ncategories: 计算机视觉特辑\n\n• 语义分割是计算机视觉的四大任务之一（四大任务：分类 a、定位 b、检测 b、分割 c+d），在语义分割中常用的公共数据集有 PASCAL VOC 2012（1.5k train 1.5k validate 20types with background）、MS COCO（83k train 41k validate 80k test 80types）\n![](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%9B%BE1.jpg#alt=%E5%9B%BE1)\n\n![](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%9B%BE6.jpg#alt=%E5%9B%BE6)\n\n• 语义分割基本思路：\n\n○ 基本思路：\n\n逐个像素分类。输入整张图片进入网络，输出大小和输入一致，通道数等于类别数，分别存放各个类别在某个像元位置的概率，即可逐个像素分类。\n\n○ 全卷积网络+反卷积网络 convolution and deconvolution network：\n\n为了使输出具有三维结构，全卷积网络中没有全连接层，只有卷积层和汇合层。但是随着卷积和汇合不断进行下去，图像的尺寸越来越小、通道数越来越多，就不能保证输出大小和输入一致，所以全卷积网络要使用反卷积和反汇合来增大空间大小。\n\n![](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%9B%BE2.jpg#alt=%E5%9B%BE2)\n\n○ 反卷积（或称转置卷积） deconvolution or transpose convolution：\n\n标准卷积的滤波器在输入的图像上滑动，每次和输入图像的局部区域点乘得到单个输出值，而反卷积的滤波器在输出图像上滑动，局部范围每个神经元值乘以滤波器对应值，得到一个输出的局部区域。标准卷积的后向过程和反卷积的前向过程完成的是同样的数学运算。而且同标准卷积滤波器一样，反卷积滤波器也是从数据中学到的。\n\n○ 反最大汇合 max-unpooling：\n\n通常全卷积网络是对称的结构，在最大汇合时需要记录最大值所处的局部区域范围，在对应的反最大汇合时将对应位置的输出置为输入，其余位置补零。反最大汇合可以弥补最大汇合时的空间信息丢失。反最大汇合的前向过程和最大汇合的后向过程完成的是同样的数学运算。\n\n![](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%9B%BE3.jpg#alt=%E5%9B%BE3)\n\n• 语义分割常用技巧：\n\n○ 膨胀\\空洞\\扩张卷积 dilated convolution：\n\n这是常用于分割任务以增大感受野的一个技巧。标准卷积操作中，每个输出神经元对应的局部区域的范围内是连续的。但是，扩张卷积向标准卷积运算中引入了一个新的超参数扩张量（dilation）用于描述输入局部区域在空间位置上的间距。（当扩张量为 1 时，扩张卷积退化回标准卷积）扩张卷积可以在参数量不变的情况下有效提高感受野，而与经典计算机视觉手工特征相比，大的感受野是深度学习方法能取得优异性能的重要原因之一。\n\n![](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%9B%BE4.jpg#alt=%E5%9B%BE4)\n\n○ 条件随机场 conditional random field(CRF)：\n\n这是一种概率图模型，常用于微调全卷积网络的输出结果，获得更好的细节信息。它的原理是更相近的像元更可能属于相同的类别。但是这样会要考虑两两像元之间的空间关系，会极大降低运行效率。\n\n○ 利用低层信息：\n\n全卷积中，可以记录低层的信息，在对应的反卷积网络中的对应层采用加和（如 FCN）或者沿通道方向拼接（如 U-net）的方法弥补全卷积网络操作中丢失的细节和边缘信息，后者效果通常更好（如图）\n\n![](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%9B%BE5.jpg#alt=%E5%9B%BE5)\n\n（以上参考https://zhuanlan.zhihu.com/p/31727402）\n","source":"_posts/grllbd.md","raw":"---\n\n## title: 语义分割网络模型笔记 date: 2021-08-07 22:21:39\n\ntags: [深度学习, 语义分割]\ncategories: 计算机视觉特辑\n\n• 语义分割是计算机视觉的四大任务之一（四大任务：分类 a、定位 b、检测 b、分割 c+d），在语义分割中常用的公共数据集有 PASCAL VOC 2012（1.5k train 1.5k validate 20types with background）、MS COCO（83k train 41k validate 80k test 80types）\n![](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%9B%BE1.jpg#alt=%E5%9B%BE1)\n\n![](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%9B%BE6.jpg#alt=%E5%9B%BE6)\n\n• 语义分割基本思路：\n\n○ 基本思路：\n\n逐个像素分类。输入整张图片进入网络，输出大小和输入一致，通道数等于类别数，分别存放各个类别在某个像元位置的概率，即可逐个像素分类。\n\n○ 全卷积网络+反卷积网络 convolution and deconvolution network：\n\n为了使输出具有三维结构，全卷积网络中没有全连接层，只有卷积层和汇合层。但是随着卷积和汇合不断进行下去，图像的尺寸越来越小、通道数越来越多，就不能保证输出大小和输入一致，所以全卷积网络要使用反卷积和反汇合来增大空间大小。\n\n![](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%9B%BE2.jpg#alt=%E5%9B%BE2)\n\n○ 反卷积（或称转置卷积） deconvolution or transpose convolution：\n\n标准卷积的滤波器在输入的图像上滑动，每次和输入图像的局部区域点乘得到单个输出值，而反卷积的滤波器在输出图像上滑动，局部范围每个神经元值乘以滤波器对应值，得到一个输出的局部区域。标准卷积的后向过程和反卷积的前向过程完成的是同样的数学运算。而且同标准卷积滤波器一样，反卷积滤波器也是从数据中学到的。\n\n○ 反最大汇合 max-unpooling：\n\n通常全卷积网络是对称的结构，在最大汇合时需要记录最大值所处的局部区域范围，在对应的反最大汇合时将对应位置的输出置为输入，其余位置补零。反最大汇合可以弥补最大汇合时的空间信息丢失。反最大汇合的前向过程和最大汇合的后向过程完成的是同样的数学运算。\n\n![](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%9B%BE3.jpg#alt=%E5%9B%BE3)\n\n• 语义分割常用技巧：\n\n○ 膨胀\\空洞\\扩张卷积 dilated convolution：\n\n这是常用于分割任务以增大感受野的一个技巧。标准卷积操作中，每个输出神经元对应的局部区域的范围内是连续的。但是，扩张卷积向标准卷积运算中引入了一个新的超参数扩张量（dilation）用于描述输入局部区域在空间位置上的间距。（当扩张量为 1 时，扩张卷积退化回标准卷积）扩张卷积可以在参数量不变的情况下有效提高感受野，而与经典计算机视觉手工特征相比，大的感受野是深度学习方法能取得优异性能的重要原因之一。\n\n![](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%9B%BE4.jpg#alt=%E5%9B%BE4)\n\n○ 条件随机场 conditional random field(CRF)：\n\n这是一种概率图模型，常用于微调全卷积网络的输出结果，获得更好的细节信息。它的原理是更相近的像元更可能属于相同的类别。但是这样会要考虑两两像元之间的空间关系，会极大降低运行效率。\n\n○ 利用低层信息：\n\n全卷积中，可以记录低层的信息，在对应的反卷积网络中的对应层采用加和（如 FCN）或者沿通道方向拼接（如 U-net）的方法弥补全卷积网络操作中丢失的细节和边缘信息，后者效果通常更好（如图）\n\n![](%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/%E5%9B%BE5.jpg#alt=%E5%9B%BE5)\n\n（以上参考https://zhuanlan.zhihu.com/p/31727402）\n","slug":"grllbd","published":1,"date":"2021-09-27T07:04:30.512Z","updated":"2021-09-27T07:04:30.513Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cku2b35j70001d1a2a16y5jue","content":"<hr>\n<h2 id=\"title-语义分割网络模型笔记-date-2021-08-07-22-21-39\"><a href=\"#title-语义分割网络模型笔记-date-2021-08-07-22-21-39\" class=\"headerlink\" title=\"title: 语义分割网络模型笔记 date: 2021-08-07 22:21:39\"></a>title: 语义分割网络模型笔记 date: 2021-08-07 22:21:39</h2><p>tags: [深度学习, 语义分割]<br>categories: 计算机视觉特辑</p>\n<p>• 语义分割是计算机视觉的四大任务之一（四大任务：分类 a、定位 b、检测 b、分割 c+d），在语义分割中常用的公共数据集有 PASCAL VOC 2012（1.5k train 1.5k validate 20types with background）、MS COCO（83k train 41k validate 80k test 80types）<br><img src=\"/2021/09/27/grllbd/%E5%9B%BE1.jpg#alt=%E5%9B%BE1\"></p>\n<p><img src=\"/2021/09/27/grllbd/%E5%9B%BE6.jpg#alt=%E5%9B%BE6\"></p>\n<p>• 语义分割基本思路：</p>\n<p>○ 基本思路：</p>\n<p>逐个像素分类。输入整张图片进入网络，输出大小和输入一致，通道数等于类别数，分别存放各个类别在某个像元位置的概率，即可逐个像素分类。</p>\n<p>○ 全卷积网络+反卷积网络 convolution and deconvolution network：</p>\n<p>为了使输出具有三维结构，全卷积网络中没有全连接层，只有卷积层和汇合层。但是随着卷积和汇合不断进行下去，图像的尺寸越来越小、通道数越来越多，就不能保证输出大小和输入一致，所以全卷积网络要使用反卷积和反汇合来增大空间大小。</p>\n<p><img src=\"/2021/09/27/grllbd/%E5%9B%BE2.jpg#alt=%E5%9B%BE2\"></p>\n<p>○ 反卷积（或称转置卷积） deconvolution or transpose convolution：</p>\n<p>标准卷积的滤波器在输入的图像上滑动，每次和输入图像的局部区域点乘得到单个输出值，而反卷积的滤波器在输出图像上滑动，局部范围每个神经元值乘以滤波器对应值，得到一个输出的局部区域。标准卷积的后向过程和反卷积的前向过程完成的是同样的数学运算。而且同标准卷积滤波器一样，反卷积滤波器也是从数据中学到的。</p>\n<p>○ 反最大汇合 max-unpooling：</p>\n<p>通常全卷积网络是对称的结构，在最大汇合时需要记录最大值所处的局部区域范围，在对应的反最大汇合时将对应位置的输出置为输入，其余位置补零。反最大汇合可以弥补最大汇合时的空间信息丢失。反最大汇合的前向过程和最大汇合的后向过程完成的是同样的数学运算。</p>\n<p><img src=\"/2021/09/27/grllbd/%E5%9B%BE3.jpg#alt=%E5%9B%BE3\"></p>\n<p>• 语义分割常用技巧：</p>\n<p>○ 膨胀\\空洞\\扩张卷积 dilated convolution：</p>\n<p>这是常用于分割任务以增大感受野的一个技巧。标准卷积操作中，每个输出神经元对应的局部区域的范围内是连续的。但是，扩张卷积向标准卷积运算中引入了一个新的超参数扩张量（dilation）用于描述输入局部区域在空间位置上的间距。（当扩张量为 1 时，扩张卷积退化回标准卷积）扩张卷积可以在参数量不变的情况下有效提高感受野，而与经典计算机视觉手工特征相比，大的感受野是深度学习方法能取得优异性能的重要原因之一。</p>\n<p><img src=\"/2021/09/27/grllbd/%E5%9B%BE4.jpg#alt=%E5%9B%BE4\"></p>\n<p>○ 条件随机场 conditional random field(CRF)：</p>\n<p>这是一种概率图模型，常用于微调全卷积网络的输出结果，获得更好的细节信息。它的原理是更相近的像元更可能属于相同的类别。但是这样会要考虑两两像元之间的空间关系，会极大降低运行效率。</p>\n<p>○ 利用低层信息：</p>\n<p>全卷积中，可以记录低层的信息，在对应的反卷积网络中的对应层采用加和（如 FCN）或者沿通道方向拼接（如 U-net）的方法弥补全卷积网络操作中丢失的细节和边缘信息，后者效果通常更好（如图）</p>\n<p><img src=\"/2021/09/27/grllbd/%E5%9B%BE5.jpg#alt=%E5%9B%BE5\"></p>\n<p>（以上参考<a href=\"https://zhuanlan.zhihu.com/p/31727402%EF%BC%89\">https://zhuanlan.zhihu.com/p/31727402）</a></p>\n","site":{"data":{}},"cover":"/img/loading.gif","length":1306,"excerpt":"","more":"<hr>\n<h2 id=\"title-语义分割网络模型笔记-date-2021-08-07-22-21-39\"><a href=\"#title-语义分割网络模型笔记-date-2021-08-07-22-21-39\" class=\"headerlink\" title=\"title: 语义分割网络模型笔记 date: 2021-08-07 22:21:39\"></a>title: 语义分割网络模型笔记 date: 2021-08-07 22:21:39</h2><p>tags: [深度学习, 语义分割]<br>categories: 计算机视觉特辑</p>\n<p>• 语义分割是计算机视觉的四大任务之一（四大任务：分类 a、定位 b、检测 b、分割 c+d），在语义分割中常用的公共数据集有 PASCAL VOC 2012（1.5k train 1.5k validate 20types with background）、MS COCO（83k train 41k validate 80k test 80types）<br><img src=\"/2021/09/27/grllbd/%E5%9B%BE1.jpg#alt=%E5%9B%BE1\"></p>\n<p><img src=\"/2021/09/27/grllbd/%E5%9B%BE6.jpg#alt=%E5%9B%BE6\"></p>\n<p>• 语义分割基本思路：</p>\n<p>○ 基本思路：</p>\n<p>逐个像素分类。输入整张图片进入网络，输出大小和输入一致，通道数等于类别数，分别存放各个类别在某个像元位置的概率，即可逐个像素分类。</p>\n<p>○ 全卷积网络+反卷积网络 convolution and deconvolution network：</p>\n<p>为了使输出具有三维结构，全卷积网络中没有全连接层，只有卷积层和汇合层。但是随着卷积和汇合不断进行下去，图像的尺寸越来越小、通道数越来越多，就不能保证输出大小和输入一致，所以全卷积网络要使用反卷积和反汇合来增大空间大小。</p>\n<p><img src=\"/2021/09/27/grllbd/%E5%9B%BE2.jpg#alt=%E5%9B%BE2\"></p>\n<p>○ 反卷积（或称转置卷积） deconvolution or transpose convolution：</p>\n<p>标准卷积的滤波器在输入的图像上滑动，每次和输入图像的局部区域点乘得到单个输出值，而反卷积的滤波器在输出图像上滑动，局部范围每个神经元值乘以滤波器对应值，得到一个输出的局部区域。标准卷积的后向过程和反卷积的前向过程完成的是同样的数学运算。而且同标准卷积滤波器一样，反卷积滤波器也是从数据中学到的。</p>\n<p>○ 反最大汇合 max-unpooling：</p>\n<p>通常全卷积网络是对称的结构，在最大汇合时需要记录最大值所处的局部区域范围，在对应的反最大汇合时将对应位置的输出置为输入，其余位置补零。反最大汇合可以弥补最大汇合时的空间信息丢失。反最大汇合的前向过程和最大汇合的后向过程完成的是同样的数学运算。</p>\n<p><img src=\"/2021/09/27/grllbd/%E5%9B%BE3.jpg#alt=%E5%9B%BE3\"></p>\n<p>• 语义分割常用技巧：</p>\n<p>○ 膨胀\\空洞\\扩张卷积 dilated convolution：</p>\n<p>这是常用于分割任务以增大感受野的一个技巧。标准卷积操作中，每个输出神经元对应的局部区域的范围内是连续的。但是，扩张卷积向标准卷积运算中引入了一个新的超参数扩张量（dilation）用于描述输入局部区域在空间位置上的间距。（当扩张量为 1 时，扩张卷积退化回标准卷积）扩张卷积可以在参数量不变的情况下有效提高感受野，而与经典计算机视觉手工特征相比，大的感受野是深度学习方法能取得优异性能的重要原因之一。</p>\n<p><img src=\"/2021/09/27/grllbd/%E5%9B%BE4.jpg#alt=%E5%9B%BE4\"></p>\n<p>○ 条件随机场 conditional random field(CRF)：</p>\n<p>这是一种概率图模型，常用于微调全卷积网络的输出结果，获得更好的细节信息。它的原理是更相近的像元更可能属于相同的类别。但是这样会要考虑两两像元之间的空间关系，会极大降低运行效率。</p>\n<p>○ 利用低层信息：</p>\n<p>全卷积中，可以记录低层的信息，在对应的反卷积网络中的对应层采用加和（如 FCN）或者沿通道方向拼接（如 U-net）的方法弥补全卷积网络操作中丢失的细节和边缘信息，后者效果通常更好（如图）</p>\n<p><img src=\"/2021/09/27/grllbd/%E5%9B%BE5.jpg#alt=%E5%9B%BE5\"></p>\n<p>（以上参考<a href=\"https://zhuanlan.zhihu.com/p/31727402%EF%BC%89\">https://zhuanlan.zhihu.com/p/31727402）</a></p>\n"},{"_content":"---\n\n## title: 损失函数与语义分割任务 date: 2021-08-07 22:29:17\n\ntags: [深度学习, 语义分割]\ncategories: 计算机视觉特辑\n\n1.基于分布的损失函数，基于区域的损失函数，基于边界的损失函数和基于复合的损失函数（ Distribution-based,Region-based,  Boundary-based,  and  Compounded）\n![](%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n\n2.分类标签不平衡问题：\n\n• 道路识别中，数据集拥有的正样本非常少，有时候背景负样本带来很多没必要的损失函数值的参数更新，类似噪声干扰了正样本训练，无论双分类训练还是多分类训练都存在该问题，更不用说多分类任务。\n\n• 解决方案\n\n损失函数本身对训练影响弱于数据集，但是，如果数据集中标签严重不平衡，还是应该选择二进制交叉熵(binary-cross entropy)之外的其他损失函数，如 DiceLoss 系列。\n\n○ 普通 DiceLoss 损失函数\n\n![](%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/%E5%9B%BE2.png#alt=%E5%9B%BE2)\n\n![](%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/%E5%9B%BE3.png#alt=%E5%9B%BE3)\n\n上面公式比较通俗，右侧部分称 Dice 系数（dice coefficient），由 1 减去就得到了 DiceLoss。对于分割任务而言，绝对值 X 和 Y 分别代表 ground_truth 和 predict_mask。一般和 IoU 一样作为测试的评价指数，实际训练效果非常糟糕。\n\n○ Log-Cosh Dice Loss\n\n普通 DiceLoss 由于其非凸性，它多次都无法获得最佳结果。Lovsz-softmax 损失旨在通过添加使用 Lovsz 扩展的平滑来解决非凸损失函数的问题。同时，Log-Cosh 方法已广泛用于基于回归的问题中，以平滑曲线。\n\n![](%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/%E5%9B%BE4.png#alt=%E5%9B%BE4)\n\n但是该损失函数无法在我的项目中运行（ExpLog_Dice）\n\n○ Tversky Loss\n\n![](%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/%E5%9B%BE5.png#alt=%E5%9B%BE5)\n\nTversky 系数是 Dice 系数和 Jaccard 系数的一种推广。当设置 α=β=0.5，此时 Tversky 系数就是 Dice 系数。而当设置 α=β=1 时，此时 Tversky 系数就是 Jaccard 系数。α 和 β 分别控制假阴性和假阳性。通过调整 α 和 β，可以控制假阳性和假阴性之间的平衡。\n\n![](%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/%E5%9B%BE6.png#alt=%E5%9B%BE6)\n\n○ Focal Tversky Loss\n\n![](%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/%E5%9B%BE7.png#alt=%E5%9B%BE7)\n\n参考文章： Loss Functions for Medical Image Segmentation: A Taxonomy | by JunMa | Medium [https://medium.com/@junma11/loss-functions-for-medical-image-segmentation-a-taxonomy-cefa5292eec0#:~:text=Generalized Dice loss is the multi-class extension of,negatives and false positives in generalized Dice loss](https://medium.com/@junma11/loss-functions-for-medical-image-segmentation-a-taxonomy-cefa5292eec0#:~:text=Generalized%20Dice%20loss%20is%20the%20multi-class%20extension%20of,negatives%20and%20false%20positives%20in%20generalized%20Dice%20loss).\n\nGitHub： GitHub - JunMa11/SegLoss: A collection of loss functions for medical image segmentation [https://github.com/JunMa11/SegLoss](https://github.com/JunMa11/SegLoss)\n\n论文：[https://arxiv.org/pdf/2006.14822.pdf](https://arxiv.org/pdf/2006.14822.pdf)\n","source":"_posts/ot04dg.md","raw":"---\n\n## title: 损失函数与语义分割任务 date: 2021-08-07 22:29:17\n\ntags: [深度学习, 语义分割]\ncategories: 计算机视觉特辑\n\n1.基于分布的损失函数，基于区域的损失函数，基于边界的损失函数和基于复合的损失函数（ Distribution-based,Region-based,  Boundary-based,  and  Compounded）\n![](%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n\n2.分类标签不平衡问题：\n\n• 道路识别中，数据集拥有的正样本非常少，有时候背景负样本带来很多没必要的损失函数值的参数更新，类似噪声干扰了正样本训练，无论双分类训练还是多分类训练都存在该问题，更不用说多分类任务。\n\n• 解决方案\n\n损失函数本身对训练影响弱于数据集，但是，如果数据集中标签严重不平衡，还是应该选择二进制交叉熵(binary-cross entropy)之外的其他损失函数，如 DiceLoss 系列。\n\n○ 普通 DiceLoss 损失函数\n\n![](%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/%E5%9B%BE2.png#alt=%E5%9B%BE2)\n\n![](%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/%E5%9B%BE3.png#alt=%E5%9B%BE3)\n\n上面公式比较通俗，右侧部分称 Dice 系数（dice coefficient），由 1 减去就得到了 DiceLoss。对于分割任务而言，绝对值 X 和 Y 分别代表 ground_truth 和 predict_mask。一般和 IoU 一样作为测试的评价指数，实际训练效果非常糟糕。\n\n○ Log-Cosh Dice Loss\n\n普通 DiceLoss 由于其非凸性，它多次都无法获得最佳结果。Lovsz-softmax 损失旨在通过添加使用 Lovsz 扩展的平滑来解决非凸损失函数的问题。同时，Log-Cosh 方法已广泛用于基于回归的问题中，以平滑曲线。\n\n![](%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/%E5%9B%BE4.png#alt=%E5%9B%BE4)\n\n但是该损失函数无法在我的项目中运行（ExpLog_Dice）\n\n○ Tversky Loss\n\n![](%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/%E5%9B%BE5.png#alt=%E5%9B%BE5)\n\nTversky 系数是 Dice 系数和 Jaccard 系数的一种推广。当设置 α=β=0.5，此时 Tversky 系数就是 Dice 系数。而当设置 α=β=1 时，此时 Tversky 系数就是 Jaccard 系数。α 和 β 分别控制假阴性和假阳性。通过调整 α 和 β，可以控制假阳性和假阴性之间的平衡。\n\n![](%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/%E5%9B%BE6.png#alt=%E5%9B%BE6)\n\n○ Focal Tversky Loss\n\n![](%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1/%E5%9B%BE7.png#alt=%E5%9B%BE7)\n\n参考文章： Loss Functions for Medical Image Segmentation: A Taxonomy | by JunMa | Medium [https://medium.com/@junma11/loss-functions-for-medical-image-segmentation-a-taxonomy-cefa5292eec0#:~:text=Generalized Dice loss is the multi-class extension of,negatives and false positives in generalized Dice loss](https://medium.com/@junma11/loss-functions-for-medical-image-segmentation-a-taxonomy-cefa5292eec0#:~:text=Generalized%20Dice%20loss%20is%20the%20multi-class%20extension%20of,negatives%20and%20false%20positives%20in%20generalized%20Dice%20loss).\n\nGitHub： GitHub - JunMa11/SegLoss: A collection of loss functions for medical image segmentation [https://github.com/JunMa11/SegLoss](https://github.com/JunMa11/SegLoss)\n\n论文：[https://arxiv.org/pdf/2006.14822.pdf](https://arxiv.org/pdf/2006.14822.pdf)\n","slug":"ot04dg","published":1,"date":"2021-09-27T07:04:30.476Z","updated":"2021-09-27T07:04:30.476Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cku2b35j90002d1a2bzkpbybq","content":"<hr>\n<h2 id=\"title-损失函数与语义分割任务-date-2021-08-07-22-29-17\"><a href=\"#title-损失函数与语义分割任务-date-2021-08-07-22-29-17\" class=\"headerlink\" title=\"title: 损失函数与语义分割任务 date: 2021-08-07 22:29:17\"></a>title: 损失函数与语义分割任务 date: 2021-08-07 22:29:17</h2><p>tags: [深度学习, 语义分割]<br>categories: 计算机视觉特辑</p>\n<p>1.基于分布的损失函数，基于区域的损失函数，基于边界的损失函数和基于复合的损失函数（ Distribution-based,Region-based,  Boundary-based,  and  Compounded）<br><img src=\"/2021/09/27/ot04dg/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n<p>2.分类标签不平衡问题：</p>\n<p>• 道路识别中，数据集拥有的正样本非常少，有时候背景负样本带来很多没必要的损失函数值的参数更新，类似噪声干扰了正样本训练，无论双分类训练还是多分类训练都存在该问题，更不用说多分类任务。</p>\n<p>• 解决方案</p>\n<p>损失函数本身对训练影响弱于数据集，但是，如果数据集中标签严重不平衡，还是应该选择二进制交叉熵(binary-cross entropy)之外的其他损失函数，如 DiceLoss 系列。</p>\n<p>○ 普通 DiceLoss 损失函数</p>\n<p><img src=\"/2021/09/27/ot04dg/%E5%9B%BE2.png#alt=%E5%9B%BE2\"></p>\n<p><img src=\"/2021/09/27/ot04dg/%E5%9B%BE3.png#alt=%E5%9B%BE3\"></p>\n<p>上面公式比较通俗，右侧部分称 Dice 系数（dice coefficient），由 1 减去就得到了 DiceLoss。对于分割任务而言，绝对值 X 和 Y 分别代表 ground_truth 和 predict_mask。一般和 IoU 一样作为测试的评价指数，实际训练效果非常糟糕。</p>\n<p>○ Log-Cosh Dice Loss</p>\n<p>普通 DiceLoss 由于其非凸性，它多次都无法获得最佳结果。Lovsz-softmax 损失旨在通过添加使用 Lovsz 扩展的平滑来解决非凸损失函数的问题。同时，Log-Cosh 方法已广泛用于基于回归的问题中，以平滑曲线。</p>\n<p><img src=\"/2021/09/27/ot04dg/%E5%9B%BE4.png#alt=%E5%9B%BE4\"></p>\n<p>但是该损失函数无法在我的项目中运行（ExpLog_Dice）</p>\n<p>○ Tversky Loss</p>\n<p><img src=\"/2021/09/27/ot04dg/%E5%9B%BE5.png#alt=%E5%9B%BE5\"></p>\n<p>Tversky 系数是 Dice 系数和 Jaccard 系数的一种推广。当设置 α=β=0.5，此时 Tversky 系数就是 Dice 系数。而当设置 α=β=1 时，此时 Tversky 系数就是 Jaccard 系数。α 和 β 分别控制假阴性和假阳性。通过调整 α 和 β，可以控制假阳性和假阴性之间的平衡。</p>\n<p><img src=\"/2021/09/27/ot04dg/%E5%9B%BE6.png#alt=%E5%9B%BE6\"></p>\n<p>○ Focal Tversky Loss</p>\n<p><img src=\"/2021/09/27/ot04dg/%E5%9B%BE7.png#alt=%E5%9B%BE7\"></p>\n<p>参考文章： Loss Functions for Medical Image Segmentation: A Taxonomy | by JunMa | Medium <a href=\"https://medium.com/@junma11/loss-functions-for-medical-image-segmentation-a-taxonomy-cefa5292eec0#:~:text=Generalized%20Dice%20loss%20is%20the%20multi-class%20extension%20of,negatives%20and%20false%20positives%20in%20generalized%20Dice%20loss\">https://medium.com/@junma11/loss-functions-for-medical-image-segmentation-a-taxonomy-cefa5292eec0#:~:text=Generalized Dice loss is the multi-class extension of,negatives and false positives in generalized Dice loss</a>.</p>\n<p>GitHub： GitHub - JunMa11/SegLoss: A collection of loss functions for medical image segmentation <a href=\"https://github.com/JunMa11/SegLoss\">https://github.com/JunMa11/SegLoss</a></p>\n<p>论文：<a href=\"https://arxiv.org/pdf/2006.14822.pdf\">https://arxiv.org/pdf/2006.14822.pdf</a></p>\n","site":{"data":{}},"cover":"/img/loading.gif","length":1267,"excerpt":"","more":"<hr>\n<h2 id=\"title-损失函数与语义分割任务-date-2021-08-07-22-29-17\"><a href=\"#title-损失函数与语义分割任务-date-2021-08-07-22-29-17\" class=\"headerlink\" title=\"title: 损失函数与语义分割任务 date: 2021-08-07 22:29:17\"></a>title: 损失函数与语义分割任务 date: 2021-08-07 22:29:17</h2><p>tags: [深度学习, 语义分割]<br>categories: 计算机视觉特辑</p>\n<p>1.基于分布的损失函数，基于区域的损失函数，基于边界的损失函数和基于复合的损失函数（ Distribution-based,Region-based,  Boundary-based,  and  Compounded）<br><img src=\"/2021/09/27/ot04dg/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n<p>2.分类标签不平衡问题：</p>\n<p>• 道路识别中，数据集拥有的正样本非常少，有时候背景负样本带来很多没必要的损失函数值的参数更新，类似噪声干扰了正样本训练，无论双分类训练还是多分类训练都存在该问题，更不用说多分类任务。</p>\n<p>• 解决方案</p>\n<p>损失函数本身对训练影响弱于数据集，但是，如果数据集中标签严重不平衡，还是应该选择二进制交叉熵(binary-cross entropy)之外的其他损失函数，如 DiceLoss 系列。</p>\n<p>○ 普通 DiceLoss 损失函数</p>\n<p><img src=\"/2021/09/27/ot04dg/%E5%9B%BE2.png#alt=%E5%9B%BE2\"></p>\n<p><img src=\"/2021/09/27/ot04dg/%E5%9B%BE3.png#alt=%E5%9B%BE3\"></p>\n<p>上面公式比较通俗，右侧部分称 Dice 系数（dice coefficient），由 1 减去就得到了 DiceLoss。对于分割任务而言，绝对值 X 和 Y 分别代表 ground_truth 和 predict_mask。一般和 IoU 一样作为测试的评价指数，实际训练效果非常糟糕。</p>\n<p>○ Log-Cosh Dice Loss</p>\n<p>普通 DiceLoss 由于其非凸性，它多次都无法获得最佳结果。Lovsz-softmax 损失旨在通过添加使用 Lovsz 扩展的平滑来解决非凸损失函数的问题。同时，Log-Cosh 方法已广泛用于基于回归的问题中，以平滑曲线。</p>\n<p><img src=\"/2021/09/27/ot04dg/%E5%9B%BE4.png#alt=%E5%9B%BE4\"></p>\n<p>但是该损失函数无法在我的项目中运行（ExpLog_Dice）</p>\n<p>○ Tversky Loss</p>\n<p><img src=\"/2021/09/27/ot04dg/%E5%9B%BE5.png#alt=%E5%9B%BE5\"></p>\n<p>Tversky 系数是 Dice 系数和 Jaccard 系数的一种推广。当设置 α=β=0.5，此时 Tversky 系数就是 Dice 系数。而当设置 α=β=1 时，此时 Tversky 系数就是 Jaccard 系数。α 和 β 分别控制假阴性和假阳性。通过调整 α 和 β，可以控制假阳性和假阴性之间的平衡。</p>\n<p><img src=\"/2021/09/27/ot04dg/%E5%9B%BE6.png#alt=%E5%9B%BE6\"></p>\n<p>○ Focal Tversky Loss</p>\n<p><img src=\"/2021/09/27/ot04dg/%E5%9B%BE7.png#alt=%E5%9B%BE7\"></p>\n<p>参考文章： Loss Functions for Medical Image Segmentation: A Taxonomy | by JunMa | Medium <a href=\"https://medium.com/@junma11/loss-functions-for-medical-image-segmentation-a-taxonomy-cefa5292eec0#:~:text=Generalized%20Dice%20loss%20is%20the%20multi-class%20extension%20of,negatives%20and%20false%20positives%20in%20generalized%20Dice%20loss\">https://medium.com/@junma11/loss-functions-for-medical-image-segmentation-a-taxonomy-cefa5292eec0#:~:text=Generalized Dice loss is the multi-class extension of,negatives and false positives in generalized Dice loss</a>.</p>\n<p>GitHub： GitHub - JunMa11/SegLoss: A collection of loss functions for medical image segmentation <a href=\"https://github.com/JunMa11/SegLoss\">https://github.com/JunMa11/SegLoss</a></p>\n<p>论文：<a href=\"https://arxiv.org/pdf/2006.14822.pdf\">https://arxiv.org/pdf/2006.14822.pdf</a></p>\n"},{"_content":"---\n\n## title: GIS 的应用 date: 2021-07-12 21:50:03\n\ntags: [GIS, GISer, 思想]\ncategories: 地信原理特辑\n\n1.输入（蓝）、处理（橙）、输出（绿）：\n\n• 输入处理输出是最基本最简化的应用逻辑\n\n```\n![图1](GIS的应用/图1.png)\n```\n\n2.GIS 模型+具体行业模型（集成）\n\n• 举例复杂应用：水务行业的排水管网 SWMM 模型（排水管网属于 GIS 模型，SWMM 属于税务行业模型）\n\n○ 对应 GISer 的思想：\n\n§ 给排水管建模\n\n§ 分层：根据用户需求分析得到的设备类型抽象图层：检查井、管线、汇水区……\n\n§ 抽象：\n\n□ 检查井，点类型：坐标、高程……\n\n□ 管线，线类型：上下游井、埋深、管长、管径……\n\n□ 汇水区，面类型：面积……\n\n○ 针对思想的复杂应用构建：\n\n§ 输入：\n\n□ 检查井：坐标、高程……<<DEM 高程提取\n\n□ 管线：上下游井、埋深、管长、管径……<<上下游网络拓扑\n\n□ 汇水区：面积……<<小流域 Basin 划分工具\n\n§ 处理：\n\n□ SWMM 模型引擎（和 GIS 关系不大，主要来源于行业模型）\n\n§ 输出（行业用户对应需求）：\n\n□ 检查井的水位变化序列的动态专题渲染\n\n□ 管线流量、充满度变化序列的动态专题渲染\n","source":"_posts/ug2u1x.md","raw":"---\n\n## title: GIS 的应用 date: 2021-07-12 21:50:03\n\ntags: [GIS, GISer, 思想]\ncategories: 地信原理特辑\n\n1.输入（蓝）、处理（橙）、输出（绿）：\n\n• 输入处理输出是最基本最简化的应用逻辑\n\n```\n![图1](GIS的应用/图1.png)\n```\n\n2.GIS 模型+具体行业模型（集成）\n\n• 举例复杂应用：水务行业的排水管网 SWMM 模型（排水管网属于 GIS 模型，SWMM 属于税务行业模型）\n\n○ 对应 GISer 的思想：\n\n§ 给排水管建模\n\n§ 分层：根据用户需求分析得到的设备类型抽象图层：检查井、管线、汇水区……\n\n§ 抽象：\n\n□ 检查井，点类型：坐标、高程……\n\n□ 管线，线类型：上下游井、埋深、管长、管径……\n\n□ 汇水区，面类型：面积……\n\n○ 针对思想的复杂应用构建：\n\n§ 输入：\n\n□ 检查井：坐标、高程……<<DEM 高程提取\n\n□ 管线：上下游井、埋深、管长、管径……<<上下游网络拓扑\n\n□ 汇水区：面积……<<小流域 Basin 划分工具\n\n§ 处理：\n\n□ SWMM 模型引擎（和 GIS 关系不大，主要来源于行业模型）\n\n§ 输出（行业用户对应需求）：\n\n□ 检查井的水位变化序列的动态专题渲染\n\n□ 管线流量、充满度变化序列的动态专题渲染\n","slug":"ug2u1x","published":1,"date":"2021-09-27T07:04:30.370Z","updated":"2021-09-27T07:04:30.370Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cku2b35ja0003d1a26qpu9m2c","content":"<hr>\n<h2 id=\"title-GIS-的应用-date-2021-07-12-21-50-03\"><a href=\"#title-GIS-的应用-date-2021-07-12-21-50-03\" class=\"headerlink\" title=\"title: GIS 的应用 date: 2021-07-12 21:50:03\"></a>title: GIS 的应用 date: 2021-07-12 21:50:03</h2><p>tags: [GIS, GISer, 思想]<br>categories: 地信原理特辑</p>\n<p>1.输入（蓝）、处理（橙）、输出（绿）：</p>\n<p>• 输入处理输出是最基本最简化的应用逻辑</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">![图1](GIS的应用/图1.png)</span><br></pre></td></tr></table></figure>\n\n<p>2.GIS 模型+具体行业模型（集成）</p>\n<p>• 举例复杂应用：水务行业的排水管网 SWMM 模型（排水管网属于 GIS 模型，SWMM 属于税务行业模型）</p>\n<p>○ 对应 GISer 的思想：</p>\n<p>§ 给排水管建模</p>\n<p>§ 分层：根据用户需求分析得到的设备类型抽象图层：检查井、管线、汇水区……</p>\n<p>§ 抽象：</p>\n<p>□ 检查井，点类型：坐标、高程……</p>\n<p>□ 管线，线类型：上下游井、埋深、管长、管径……</p>\n<p>□ 汇水区，面类型：面积……</p>\n<p>○ 针对思想的复杂应用构建：</p>\n<p>§ 输入：</p>\n<p>□ 检查井：坐标、高程……&lt;&lt;DEM 高程提取</p>\n<p>□ 管线：上下游井、埋深、管长、管径……&lt;&lt;上下游网络拓扑</p>\n<p>□ 汇水区：面积……&lt;&lt;小流域 Basin 划分工具</p>\n<p>§ 处理：</p>\n<p>□ SWMM 模型引擎（和 GIS 关系不大，主要来源于行业模型）</p>\n<p>§ 输出（行业用户对应需求）：</p>\n<p>□ 检查井的水位变化序列的动态专题渲染</p>\n<p>□ 管线流量、充满度变化序列的动态专题渲染</p>\n","site":{"data":{}},"cover":"/img/loading.gif","length":501,"excerpt":"","more":"<hr>\n<h2 id=\"title-GIS-的应用-date-2021-07-12-21-50-03\"><a href=\"#title-GIS-的应用-date-2021-07-12-21-50-03\" class=\"headerlink\" title=\"title: GIS 的应用 date: 2021-07-12 21:50:03\"></a>title: GIS 的应用 date: 2021-07-12 21:50:03</h2><p>tags: [GIS, GISer, 思想]<br>categories: 地信原理特辑</p>\n<p>1.输入（蓝）、处理（橙）、输出（绿）：</p>\n<p>• 输入处理输出是最基本最简化的应用逻辑</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">![图1](GIS的应用/图1.png)</span><br></pre></td></tr></table></figure>\n\n<p>2.GIS 模型+具体行业模型（集成）</p>\n<p>• 举例复杂应用：水务行业的排水管网 SWMM 模型（排水管网属于 GIS 模型，SWMM 属于税务行业模型）</p>\n<p>○ 对应 GISer 的思想：</p>\n<p>§ 给排水管建模</p>\n<p>§ 分层：根据用户需求分析得到的设备类型抽象图层：检查井、管线、汇水区……</p>\n<p>§ 抽象：</p>\n<p>□ 检查井，点类型：坐标、高程……</p>\n<p>□ 管线，线类型：上下游井、埋深、管长、管径……</p>\n<p>□ 汇水区，面类型：面积……</p>\n<p>○ 针对思想的复杂应用构建：</p>\n<p>§ 输入：</p>\n<p>□ 检查井：坐标、高程……&lt;&lt;DEM 高程提取</p>\n<p>□ 管线：上下游井、埋深、管长、管径……&lt;&lt;上下游网络拓扑</p>\n<p>□ 汇水区：面积……&lt;&lt;小流域 Basin 划分工具</p>\n<p>§ 处理：</p>\n<p>□ SWMM 模型引擎（和 GIS 关系不大，主要来源于行业模型）</p>\n<p>§ 输出（行业用户对应需求）：</p>\n<p>□ 检查井的水位变化序列的动态专题渲染</p>\n<p>□ 管线流量、充满度变化序列的动态专题渲染</p>\n"},{"_content":"---\n\n## title: 分割任务下数据集增广 date: 2021-08-07 22:39:14\n\ntags: [深度学习, 语义分割, 数据]\ncategories: 计算机视觉特辑\n\n1.常用增广算法\n\n• 图像变换：\n\n○ 颜色增强\n\n○ 亮度增强\n\n○ 颜色随机\n• 数量增广：\n\n○ 角度旋转\n\n• 重要图像变换：\n\n○ 随机缩放\n\n○ 椒盐化\n\n2.有序增广\n\n有各式各样的数据增广方式，但是有的增广给训练带来的效果更为显著，这样各种增广方式就不能简单各执行一次就好，而要按照一定次序逐一嵌套地执行，目前增广方式可分为三类重要等级（左至右重要性递增）：图像变换>>数量增广>>重要图像变换。\n\n• 第一步，对当前数据集完成各简单图像变换，如颜色增强、亮度增强、颜色随机。这一步进行了对泛化优化效果较差的增广，因为是第一次增广产生的数据量增长最少。\n\n• 第二部，对第一步增广的数据集执行角度旋转，这一步大量增加了数据量。\n\n• 最后，执行重要图像变换，如随机缩放、椒盐化，这是对泛化优化提供重要贡献的增广方法，这样对前面大量的增广数据完成全覆盖式重要图像变换。（值得强调的是，尺度上的变化对于感受野的优化是最大的，这也造就了随即缩放效益最优的地位，当然同时也会进一步数量增广）\n\n![](%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1%E4%B8%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A2%9E%E5%B9%BF/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n","source":"_posts/tvbg16.md","raw":"---\n\n## title: 分割任务下数据集增广 date: 2021-08-07 22:39:14\n\ntags: [深度学习, 语义分割, 数据]\ncategories: 计算机视觉特辑\n\n1.常用增广算法\n\n• 图像变换：\n\n○ 颜色增强\n\n○ 亮度增强\n\n○ 颜色随机\n• 数量增广：\n\n○ 角度旋转\n\n• 重要图像变换：\n\n○ 随机缩放\n\n○ 椒盐化\n\n2.有序增广\n\n有各式各样的数据增广方式，但是有的增广给训练带来的效果更为显著，这样各种增广方式就不能简单各执行一次就好，而要按照一定次序逐一嵌套地执行，目前增广方式可分为三类重要等级（左至右重要性递增）：图像变换>>数量增广>>重要图像变换。\n\n• 第一步，对当前数据集完成各简单图像变换，如颜色增强、亮度增强、颜色随机。这一步进行了对泛化优化效果较差的增广，因为是第一次增广产生的数据量增长最少。\n\n• 第二部，对第一步增广的数据集执行角度旋转，这一步大量增加了数据量。\n\n• 最后，执行重要图像变换，如随机缩放、椒盐化，这是对泛化优化提供重要贡献的增广方法，这样对前面大量的增广数据完成全覆盖式重要图像变换。（值得强调的是，尺度上的变化对于感受野的优化是最大的，这也造就了随即缩放效益最优的地位，当然同时也会进一步数量增广）\n\n![](%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1%E4%B8%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A2%9E%E5%B9%BF/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n","slug":"tvbg16","published":1,"date":"2021-09-27T07:04:30.577Z","updated":"2021-09-27T07:04:30.577Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cku2b35jc0004d1a297e1deci","content":"<hr>\n<h2 id=\"title-分割任务下数据集增广-date-2021-08-07-22-39-14\"><a href=\"#title-分割任务下数据集增广-date-2021-08-07-22-39-14\" class=\"headerlink\" title=\"title: 分割任务下数据集增广 date: 2021-08-07 22:39:14\"></a>title: 分割任务下数据集增广 date: 2021-08-07 22:39:14</h2><p>tags: [深度学习, 语义分割, 数据]<br>categories: 计算机视觉特辑</p>\n<p>1.常用增广算法</p>\n<p>• 图像变换：</p>\n<p>○ 颜色增强</p>\n<p>○ 亮度增强</p>\n<p>○ 颜色随机<br>• 数量增广：</p>\n<p>○ 角度旋转</p>\n<p>• 重要图像变换：</p>\n<p>○ 随机缩放</p>\n<p>○ 椒盐化</p>\n<p>2.有序增广</p>\n<p>有各式各样的数据增广方式，但是有的增广给训练带来的效果更为显著，这样各种增广方式就不能简单各执行一次就好，而要按照一定次序逐一嵌套地执行，目前增广方式可分为三类重要等级（左至右重要性递增）：图像变换&gt;&gt;数量增广&gt;&gt;重要图像变换。</p>\n<p>• 第一步，对当前数据集完成各简单图像变换，如颜色增强、亮度增强、颜色随机。这一步进行了对泛化优化效果较差的增广，因为是第一次增广产生的数据量增长最少。</p>\n<p>• 第二部，对第一步增广的数据集执行角度旋转，这一步大量增加了数据量。</p>\n<p>• 最后，执行重要图像变换，如随机缩放、椒盐化，这是对泛化优化提供重要贡献的增广方法，这样对前面大量的增广数据完成全覆盖式重要图像变换。（值得强调的是，尺度上的变化对于感受野的优化是最大的，这也造就了随即缩放效益最优的地位，当然同时也会进一步数量增广）</p>\n<p><img src=\"/2021/09/27/tvbg16/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n","site":{"data":{}},"cover":"/img/loading.gif","length":499,"excerpt":"","more":"<hr>\n<h2 id=\"title-分割任务下数据集增广-date-2021-08-07-22-39-14\"><a href=\"#title-分割任务下数据集增广-date-2021-08-07-22-39-14\" class=\"headerlink\" title=\"title: 分割任务下数据集增广 date: 2021-08-07 22:39:14\"></a>title: 分割任务下数据集增广 date: 2021-08-07 22:39:14</h2><p>tags: [深度学习, 语义分割, 数据]<br>categories: 计算机视觉特辑</p>\n<p>1.常用增广算法</p>\n<p>• 图像变换：</p>\n<p>○ 颜色增强</p>\n<p>○ 亮度增强</p>\n<p>○ 颜色随机<br>• 数量增广：</p>\n<p>○ 角度旋转</p>\n<p>• 重要图像变换：</p>\n<p>○ 随机缩放</p>\n<p>○ 椒盐化</p>\n<p>2.有序增广</p>\n<p>有各式各样的数据增广方式，但是有的增广给训练带来的效果更为显著，这样各种增广方式就不能简单各执行一次就好，而要按照一定次序逐一嵌套地执行，目前增广方式可分为三类重要等级（左至右重要性递增）：图像变换&gt;&gt;数量增广&gt;&gt;重要图像变换。</p>\n<p>• 第一步，对当前数据集完成各简单图像变换，如颜色增强、亮度增强、颜色随机。这一步进行了对泛化优化效果较差的增广，因为是第一次增广产生的数据量增长最少。</p>\n<p>• 第二部，对第一步增广的数据集执行角度旋转，这一步大量增加了数据量。</p>\n<p>• 最后，执行重要图像变换，如随机缩放、椒盐化，这是对泛化优化提供重要贡献的增广方法，这样对前面大量的增广数据完成全覆盖式重要图像变换。（值得强调的是，尺度上的变化对于感受野的优化是最大的，这也造就了随即缩放效益最优的地位，当然同时也会进一步数量增广）</p>\n<p><img src=\"/2021/09/27/tvbg16/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n"},{"_content":"---\n\n## title: GIS 的前沿现状 date: 2021-07-12 21:50:16\n\ntags: [GIS, GISer, 思想]\ncategories: 地信原理特辑\n\n1.总体来说 GIS 发展遇到了瓶颈\n\n• 烂概念+旧瓶装新酒：\n\n○ 数字城市\n\n○ 智慧城市\n\n○ 城市大脑\n\n○ CIM=BIM+GIS\n\n○ 云 GIS\n\n○ 大数据城市\n\n○ ……\n\n• 大部分概念可能有一些有趣的东西，但是都不足以成为技术革命。这些名词多少有点商业包装的成分，也侧面表明了 GIS 产业的瓶颈。\n2.GIS+VR/AR\n\n• 室内导航、实景导航等如华为 AR 地图——河图的简单运用\n\n![](GIS%E7%9A%84%E5%89%8D%E6%B2%BF%E7%8E%B0%E7%8A%B6/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n\n3.一个研究生非常适合研究（通过）的选题：GIS+物联网+行业模型（机理模型）\n\n• 物联网实现不难，行业模型大部分数据来源就可以利用物联网，成为数据来源；行业模型开放输入，作为静态模型存在；GIS 提供背后的空间数据分析能力，最终输出时间序列或者空间序列的展示。\n\n![](GIS%E7%9A%84%E5%89%8D%E6%B2%BF%E7%8E%B0%E7%8A%B6/%E5%9B%BE2.png#alt=%E5%9B%BE2)\n\n4.GIS+大数据分析（统计学模型，和人工智能无关）\n\n• 手机信令，北京第二次疫情病例行迹排查\n\n5.GIS+机器学习（非机理）\n\n• GIS 科班的学生对人工智能的认识基本很模糊，机器学习的神经网络是人工智能最典型的。可以理解神经网络过程是   黑箱>>灰箱>>白箱   的过程。黑箱：未知模型>>灰箱：神经网络训练>>白箱：机理模型——可表达模型（如水动力模型数学公式）。研究这方面，训练数据和空间分析挂钩是最好，不然 GIS 存在感太弱。\n\n6.RS+深度学习（非机理）\n\n• 遥感影像解译+卷机神经网络（CNN）是天生的一对\n\n• 栅格计算+图像识别\n\n7.GIS+BIM\n\n• 其实就是 CIM 平台开发，目前来讲做 BIM 的华而不实，自然 CIM 就是为了突破 BIM 的硬实力羸弱局面，但是很难，目前对于学生而言基本不现实，对于游戏开发或者大型地信企业是有用的。\n\n8.GIS 自身技术突破\n\n• 图形学\n\n• 三维切片（cesium），点云\n\n• 时空 GIS（停留在概念阶段）\n","source":"_posts/skxksy.md","raw":"---\n\n## title: GIS 的前沿现状 date: 2021-07-12 21:50:16\n\ntags: [GIS, GISer, 思想]\ncategories: 地信原理特辑\n\n1.总体来说 GIS 发展遇到了瓶颈\n\n• 烂概念+旧瓶装新酒：\n\n○ 数字城市\n\n○ 智慧城市\n\n○ 城市大脑\n\n○ CIM=BIM+GIS\n\n○ 云 GIS\n\n○ 大数据城市\n\n○ ……\n\n• 大部分概念可能有一些有趣的东西，但是都不足以成为技术革命。这些名词多少有点商业包装的成分，也侧面表明了 GIS 产业的瓶颈。\n2.GIS+VR/AR\n\n• 室内导航、实景导航等如华为 AR 地图——河图的简单运用\n\n![](GIS%E7%9A%84%E5%89%8D%E6%B2%BF%E7%8E%B0%E7%8A%B6/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n\n3.一个研究生非常适合研究（通过）的选题：GIS+物联网+行业模型（机理模型）\n\n• 物联网实现不难，行业模型大部分数据来源就可以利用物联网，成为数据来源；行业模型开放输入，作为静态模型存在；GIS 提供背后的空间数据分析能力，最终输出时间序列或者空间序列的展示。\n\n![](GIS%E7%9A%84%E5%89%8D%E6%B2%BF%E7%8E%B0%E7%8A%B6/%E5%9B%BE2.png#alt=%E5%9B%BE2)\n\n4.GIS+大数据分析（统计学模型，和人工智能无关）\n\n• 手机信令，北京第二次疫情病例行迹排查\n\n5.GIS+机器学习（非机理）\n\n• GIS 科班的学生对人工智能的认识基本很模糊，机器学习的神经网络是人工智能最典型的。可以理解神经网络过程是   黑箱>>灰箱>>白箱   的过程。黑箱：未知模型>>灰箱：神经网络训练>>白箱：机理模型——可表达模型（如水动力模型数学公式）。研究这方面，训练数据和空间分析挂钩是最好，不然 GIS 存在感太弱。\n\n6.RS+深度学习（非机理）\n\n• 遥感影像解译+卷机神经网络（CNN）是天生的一对\n\n• 栅格计算+图像识别\n\n7.GIS+BIM\n\n• 其实就是 CIM 平台开发，目前来讲做 BIM 的华而不实，自然 CIM 就是为了突破 BIM 的硬实力羸弱局面，但是很难，目前对于学生而言基本不现实，对于游戏开发或者大型地信企业是有用的。\n\n8.GIS 自身技术突破\n\n• 图形学\n\n• 三维切片（cesium），点云\n\n• 时空 GIS（停留在概念阶段）\n","slug":"skxksy","published":1,"date":"2021-09-27T07:04:30.415Z","updated":"2021-09-27T07:04:30.415Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cku2b35jd0005d1a2ghh3c6rq","content":"<hr>\n<h2 id=\"title-GIS-的前沿现状-date-2021-07-12-21-50-16\"><a href=\"#title-GIS-的前沿现状-date-2021-07-12-21-50-16\" class=\"headerlink\" title=\"title: GIS 的前沿现状 date: 2021-07-12 21:50:16\"></a>title: GIS 的前沿现状 date: 2021-07-12 21:50:16</h2><p>tags: [GIS, GISer, 思想]<br>categories: 地信原理特辑</p>\n<p>1.总体来说 GIS 发展遇到了瓶颈</p>\n<p>• 烂概念+旧瓶装新酒：</p>\n<p>○ 数字城市</p>\n<p>○ 智慧城市</p>\n<p>○ 城市大脑</p>\n<p>○ CIM=BIM+GIS</p>\n<p>○ 云 GIS</p>\n<p>○ 大数据城市</p>\n<p>○ ……</p>\n<p>• 大部分概念可能有一些有趣的东西，但是都不足以成为技术革命。这些名词多少有点商业包装的成分，也侧面表明了 GIS 产业的瓶颈。<br>2.GIS+VR/AR</p>\n<p>• 室内导航、实景导航等如华为 AR 地图——河图的简单运用</p>\n<p><img src=\"/2021/09/27/skxksy/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n<p>3.一个研究生非常适合研究（通过）的选题：GIS+物联网+行业模型（机理模型）</p>\n<p>• 物联网实现不难，行业模型大部分数据来源就可以利用物联网，成为数据来源；行业模型开放输入，作为静态模型存在；GIS 提供背后的空间数据分析能力，最终输出时间序列或者空间序列的展示。</p>\n<p><img src=\"/2021/09/27/skxksy/%E5%9B%BE2.png#alt=%E5%9B%BE2\"></p>\n<p>4.GIS+大数据分析（统计学模型，和人工智能无关）</p>\n<p>• 手机信令，北京第二次疫情病例行迹排查</p>\n<p>5.GIS+机器学习（非机理）</p>\n<p>• GIS 科班的学生对人工智能的认识基本很模糊，机器学习的神经网络是人工智能最典型的。可以理解神经网络过程是   黑箱&gt;&gt;灰箱&gt;&gt;白箱   的过程。黑箱：未知模型&gt;&gt;灰箱：神经网络训练&gt;&gt;白箱：机理模型——可表达模型（如水动力模型数学公式）。研究这方面，训练数据和空间分析挂钩是最好，不然 GIS 存在感太弱。</p>\n<p>6.RS+深度学习（非机理）</p>\n<p>• 遥感影像解译+卷机神经网络（CNN）是天生的一对</p>\n<p>• 栅格计算+图像识别</p>\n<p>7.GIS+BIM</p>\n<p>• 其实就是 CIM 平台开发，目前来讲做 BIM 的华而不实，自然 CIM 就是为了突破 BIM 的硬实力羸弱局面，但是很难，目前对于学生而言基本不现实，对于游戏开发或者大型地信企业是有用的。</p>\n<p>8.GIS 自身技术突破</p>\n<p>• 图形学</p>\n<p>• 三维切片（cesium），点云</p>\n<p>• 时空 GIS（停留在概念阶段）</p>\n","site":{"data":{}},"cover":"/img/loading.gif","length":790,"excerpt":"","more":"<hr>\n<h2 id=\"title-GIS-的前沿现状-date-2021-07-12-21-50-16\"><a href=\"#title-GIS-的前沿现状-date-2021-07-12-21-50-16\" class=\"headerlink\" title=\"title: GIS 的前沿现状 date: 2021-07-12 21:50:16\"></a>title: GIS 的前沿现状 date: 2021-07-12 21:50:16</h2><p>tags: [GIS, GISer, 思想]<br>categories: 地信原理特辑</p>\n<p>1.总体来说 GIS 发展遇到了瓶颈</p>\n<p>• 烂概念+旧瓶装新酒：</p>\n<p>○ 数字城市</p>\n<p>○ 智慧城市</p>\n<p>○ 城市大脑</p>\n<p>○ CIM=BIM+GIS</p>\n<p>○ 云 GIS</p>\n<p>○ 大数据城市</p>\n<p>○ ……</p>\n<p>• 大部分概念可能有一些有趣的东西，但是都不足以成为技术革命。这些名词多少有点商业包装的成分，也侧面表明了 GIS 产业的瓶颈。<br>2.GIS+VR/AR</p>\n<p>• 室内导航、实景导航等如华为 AR 地图——河图的简单运用</p>\n<p><img src=\"/2021/09/27/skxksy/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n<p>3.一个研究生非常适合研究（通过）的选题：GIS+物联网+行业模型（机理模型）</p>\n<p>• 物联网实现不难，行业模型大部分数据来源就可以利用物联网，成为数据来源；行业模型开放输入，作为静态模型存在；GIS 提供背后的空间数据分析能力，最终输出时间序列或者空间序列的展示。</p>\n<p><img src=\"/2021/09/27/skxksy/%E5%9B%BE2.png#alt=%E5%9B%BE2\"></p>\n<p>4.GIS+大数据分析（统计学模型，和人工智能无关）</p>\n<p>• 手机信令，北京第二次疫情病例行迹排查</p>\n<p>5.GIS+机器学习（非机理）</p>\n<p>• GIS 科班的学生对人工智能的认识基本很模糊，机器学习的神经网络是人工智能最典型的。可以理解神经网络过程是   黑箱&gt;&gt;灰箱&gt;&gt;白箱   的过程。黑箱：未知模型&gt;&gt;灰箱：神经网络训练&gt;&gt;白箱：机理模型——可表达模型（如水动力模型数学公式）。研究这方面，训练数据和空间分析挂钩是最好，不然 GIS 存在感太弱。</p>\n<p>6.RS+深度学习（非机理）</p>\n<p>• 遥感影像解译+卷机神经网络（CNN）是天生的一对</p>\n<p>• 栅格计算+图像识别</p>\n<p>7.GIS+BIM</p>\n<p>• 其实就是 CIM 平台开发，目前来讲做 BIM 的华而不实，自然 CIM 就是为了突破 BIM 的硬实力羸弱局面，但是很难，目前对于学生而言基本不现实，对于游戏开发或者大型地信企业是有用的。</p>\n<p>8.GIS 自身技术突破</p>\n<p>• 图形学</p>\n<p>• 三维切片（cesium），点云</p>\n<p>• 时空 GIS（停留在概念阶段）</p>\n"},{"_content":"---\n\n## title: Anaconda 环境日志 date: 2021-08-07 22:08:11\n\ntags: [深度学习, 语义分割, Anaconda, Python]\ncategories: 计算机视觉特辑\n\n1.创建虚拟环境\n\n• conda create -n your_env_name python=X.X\n\n2.更新 conda（慎用！！！，新 conda 可能用不了）\n\n• conda updata conda\n\n3.查看虚拟环境菜单和环境内已载入库\n\n• conda env list/conda info -e\n\n• conda list 4.激活虚拟环境\n\n• Conda activate your_env_name\n\n5.前人配置好的版本\n\n• Keras2.2.4\n\n• Tensorflow-gpu1.12.0\n\n6.如果遇到 conda 安装频繁报错，使用如下语句：\n\n• conda clean -i\n\n7.如果不幸要删除虚拟环境\n\n• conda remove -n your_env_name --all\n\n8.如果 pip 安装报错如下，可以检查一下是不是翻墙了\n\n![](Anaconda%E7%8E%AF%E5%A2%83%E6%97%A5%E5%BF%97/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n\n9.镜像 pip\n\n• pip install -i [https://pypi.tuna.tsinghua.edu.cn/simple](https://pypi.tuna.tsinghua.edu.cn/simple) opencv-python\n\n• pip install -i [https://pypi.douban.com/simple](https://pypi.douban.com/simple) opencv-python\n\n10.大电脑虚拟环境记录：\n\n• Main5：keras 开发框架\n\n• labelme：用于打开 labelme 工具\n\n• torch：Torch 开发框架\n\n⚠️tensorflow、keras、python 对应版本关系：\n\n![](Anaconda%E7%8E%AF%E5%A2%83%E6%97%A5%E5%BF%97/%E5%9B%BE2.png#alt=%E5%9B%BE2)\n\n⚠️Tensorflow、CUDA、python、cudnn 版本关系：\n\n![](Anaconda%E7%8E%AF%E5%A2%83%E6%97%A5%E5%BF%97/%E5%9B%BE3.png#alt=%E5%9B%BE3)\n\n⚠️ 前人配置环境全赏:\n\n![](Anaconda%E7%8E%AF%E5%A2%83%E6%97%A5%E5%BF%97/%E5%9B%BE4.jpg#alt=%E5%9B%BE4)\n","source":"_posts/pklxqf.md","raw":"---\n\n## title: Anaconda 环境日志 date: 2021-08-07 22:08:11\n\ntags: [深度学习, 语义分割, Anaconda, Python]\ncategories: 计算机视觉特辑\n\n1.创建虚拟环境\n\n• conda create -n your_env_name python=X.X\n\n2.更新 conda（慎用！！！，新 conda 可能用不了）\n\n• conda updata conda\n\n3.查看虚拟环境菜单和环境内已载入库\n\n• conda env list/conda info -e\n\n• conda list 4.激活虚拟环境\n\n• Conda activate your_env_name\n\n5.前人配置好的版本\n\n• Keras2.2.4\n\n• Tensorflow-gpu1.12.0\n\n6.如果遇到 conda 安装频繁报错，使用如下语句：\n\n• conda clean -i\n\n7.如果不幸要删除虚拟环境\n\n• conda remove -n your_env_name --all\n\n8.如果 pip 安装报错如下，可以检查一下是不是翻墙了\n\n![](Anaconda%E7%8E%AF%E5%A2%83%E6%97%A5%E5%BF%97/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n\n9.镜像 pip\n\n• pip install -i [https://pypi.tuna.tsinghua.edu.cn/simple](https://pypi.tuna.tsinghua.edu.cn/simple) opencv-python\n\n• pip install -i [https://pypi.douban.com/simple](https://pypi.douban.com/simple) opencv-python\n\n10.大电脑虚拟环境记录：\n\n• Main5：keras 开发框架\n\n• labelme：用于打开 labelme 工具\n\n• torch：Torch 开发框架\n\n⚠️tensorflow、keras、python 对应版本关系：\n\n![](Anaconda%E7%8E%AF%E5%A2%83%E6%97%A5%E5%BF%97/%E5%9B%BE2.png#alt=%E5%9B%BE2)\n\n⚠️Tensorflow、CUDA、python、cudnn 版本关系：\n\n![](Anaconda%E7%8E%AF%E5%A2%83%E6%97%A5%E5%BF%97/%E5%9B%BE3.png#alt=%E5%9B%BE3)\n\n⚠️ 前人配置环境全赏:\n\n![](Anaconda%E7%8E%AF%E5%A2%83%E6%97%A5%E5%BF%97/%E5%9B%BE4.jpg#alt=%E5%9B%BE4)\n","slug":"pklxqf","published":1,"date":"2021-09-27T07:04:30.450Z","updated":"2021-09-27T07:04:30.450Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cku2b35je0006d1a21vhm6kx3","content":"<hr>\n<h2 id=\"title-Anaconda-环境日志-date-2021-08-07-22-08-11\"><a href=\"#title-Anaconda-环境日志-date-2021-08-07-22-08-11\" class=\"headerlink\" title=\"title: Anaconda 环境日志 date: 2021-08-07 22:08:11\"></a>title: Anaconda 环境日志 date: 2021-08-07 22:08:11</h2><p>tags: [深度学习, 语义分割, Anaconda, Python]<br>categories: 计算机视觉特辑</p>\n<p>1.创建虚拟环境</p>\n<p>• conda create -n your_env_name python=X.X</p>\n<p>2.更新 conda（慎用！！！，新 conda 可能用不了）</p>\n<p>• conda updata conda</p>\n<p>3.查看虚拟环境菜单和环境内已载入库</p>\n<p>• conda env list/conda info -e</p>\n<p>• conda list 4.激活虚拟环境</p>\n<p>• Conda activate your_env_name</p>\n<p>5.前人配置好的版本</p>\n<p>• Keras2.2.4</p>\n<p>• Tensorflow-gpu1.12.0</p>\n<p>6.如果遇到 conda 安装频繁报错，使用如下语句：</p>\n<p>• conda clean -i</p>\n<p>7.如果不幸要删除虚拟环境</p>\n<p>• conda remove -n your_env_name –all</p>\n<p>8.如果 pip 安装报错如下，可以检查一下是不是翻墙了</p>\n<p><img src=\"/2021/09/27/pklxqf/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n<p>9.镜像 pip</p>\n<p>• pip install -i <a href=\"https://pypi.tuna.tsinghua.edu.cn/simple\">https://pypi.tuna.tsinghua.edu.cn/simple</a> opencv-python</p>\n<p>• pip install -i <a href=\"https://pypi.douban.com/simple\">https://pypi.douban.com/simple</a> opencv-python</p>\n<p>10.大电脑虚拟环境记录：</p>\n<p>• Main5：keras 开发框架</p>\n<p>• labelme：用于打开 labelme 工具</p>\n<p>• torch：Torch 开发框架</p>\n<p>⚠️tensorflow、keras、python 对应版本关系：</p>\n<p><img src=\"/2021/09/27/pklxqf/%E5%9B%BE2.png#alt=%E5%9B%BE2\"></p>\n<p>⚠️Tensorflow、CUDA、python、cudnn 版本关系：</p>\n<p><img src=\"/2021/09/27/pklxqf/%E5%9B%BE3.png#alt=%E5%9B%BE3\"></p>\n<p>⚠️ 前人配置环境全赏:</p>\n<p><img src=\"/2021/09/27/pklxqf/%E5%9B%BE4.jpg#alt=%E5%9B%BE4\"></p>\n","site":{"data":{}},"cover":"/img/loading.gif","length":693,"excerpt":"","more":"<hr>\n<h2 id=\"title-Anaconda-环境日志-date-2021-08-07-22-08-11\"><a href=\"#title-Anaconda-环境日志-date-2021-08-07-22-08-11\" class=\"headerlink\" title=\"title: Anaconda 环境日志 date: 2021-08-07 22:08:11\"></a>title: Anaconda 环境日志 date: 2021-08-07 22:08:11</h2><p>tags: [深度学习, 语义分割, Anaconda, Python]<br>categories: 计算机视觉特辑</p>\n<p>1.创建虚拟环境</p>\n<p>• conda create -n your_env_name python=X.X</p>\n<p>2.更新 conda（慎用！！！，新 conda 可能用不了）</p>\n<p>• conda updata conda</p>\n<p>3.查看虚拟环境菜单和环境内已载入库</p>\n<p>• conda env list/conda info -e</p>\n<p>• conda list 4.激活虚拟环境</p>\n<p>• Conda activate your_env_name</p>\n<p>5.前人配置好的版本</p>\n<p>• Keras2.2.4</p>\n<p>• Tensorflow-gpu1.12.0</p>\n<p>6.如果遇到 conda 安装频繁报错，使用如下语句：</p>\n<p>• conda clean -i</p>\n<p>7.如果不幸要删除虚拟环境</p>\n<p>• conda remove -n your_env_name –all</p>\n<p>8.如果 pip 安装报错如下，可以检查一下是不是翻墙了</p>\n<p><img src=\"/2021/09/27/pklxqf/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n<p>9.镜像 pip</p>\n<p>• pip install -i <a href=\"https://pypi.tuna.tsinghua.edu.cn/simple\">https://pypi.tuna.tsinghua.edu.cn/simple</a> opencv-python</p>\n<p>• pip install -i <a href=\"https://pypi.douban.com/simple\">https://pypi.douban.com/simple</a> opencv-python</p>\n<p>10.大电脑虚拟环境记录：</p>\n<p>• Main5：keras 开发框架</p>\n<p>• labelme：用于打开 labelme 工具</p>\n<p>• torch：Torch 开发框架</p>\n<p>⚠️tensorflow、keras、python 对应版本关系：</p>\n<p><img src=\"/2021/09/27/pklxqf/%E5%9B%BE2.png#alt=%E5%9B%BE2\"></p>\n<p>⚠️Tensorflow、CUDA、python、cudnn 版本关系：</p>\n<p><img src=\"/2021/09/27/pklxqf/%E5%9B%BE3.png#alt=%E5%9B%BE3\"></p>\n<p>⚠️ 前人配置环境全赏:</p>\n<p><img src=\"/2021/09/27/pklxqf/%E5%9B%BE4.jpg#alt=%E5%9B%BE4\"></p>\n"},{"_content":"---\n\n## title: Keras·gpu 训练（单\\多）date: 2021-08-07 23:40:18\n\ntags: [深度学习, 语义分割, Keras]\ncategories: 计算机视觉特辑\n\n在一切开始前，请确定计算机拥有英伟达的显卡。\n\n（不是英特尔！不是英特尔！不是英特尔！） 1.版本号：\n\n• keras2.2.4\n\n• Tensorflow-gpu1.12.0\n\n• CUDA9.0.176\n\n• cuDNN7.6.5 for CUDA 9.0\n\n• Scikit-image\n\n• Opencv-python\n\n2.CUDA 下载\n\n• 从 [https://developer.nvidia.com/cuda-toolkit-archive](https://developer.nvidia.com/cuda-toolkit-archive) 中打开下载中心，找到相应版本，点击版本号即可进入下载页面\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n\n• 选择要下载的平台、版本号等，点击 DOWNLOAD 即可\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE2.png#alt=%E5%9B%BE2)\n\n3.cuDNN 下载·安装\n\n• 首先，把 CUDA 安装好，从 NVIDIA cuDNN | NVIDIA Developer [https://developer.nvidia.com/zh-cn/cudnn](https://developer.nvidia.com/zh-cn/cudnn)\n\n中打开 cuDNN 中心，点击“下载 cuDNN”，登录之后填写问卷即可下载。最后 bin、include、lib 三个文件夹里的文件复制到 CUDA 的对应文件夹中就行了。\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE3.png#alt=%E5%9B%BE3)\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE4.png#alt=%E5%9B%BE4)\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE5.png#alt=%E5%9B%BE5)\n\n4.单 gpu 训练/多 gpu 训练\n\n• 单 gpu 非常简单，只需要写图中语句即可用 keras 实现单 gpu 训练\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE6.png#alt=%E5%9B%BE6)\n\n• 多 gpu 需要用到 muti 函数生成模型，代码如下\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE7.png#alt=%E5%9B%BE7)\n\n5.找不到第二条 gpu\n\n• 有时候 keras 识别不了电脑的第二条 gpu，执行 muti 会报错如下：\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE8.png#alt=%E5%9B%BE8)\n\n• 我这次是因为执行了这个语句造成的，这个语句只能供单 gpu 的 model 使用\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE9.png#alt=%E5%9B%BE9)\n","source":"_posts/xkk3s7.md","raw":"---\n\n## title: Keras·gpu 训练（单\\多）date: 2021-08-07 23:40:18\n\ntags: [深度学习, 语义分割, Keras]\ncategories: 计算机视觉特辑\n\n在一切开始前，请确定计算机拥有英伟达的显卡。\n\n（不是英特尔！不是英特尔！不是英特尔！） 1.版本号：\n\n• keras2.2.4\n\n• Tensorflow-gpu1.12.0\n\n• CUDA9.0.176\n\n• cuDNN7.6.5 for CUDA 9.0\n\n• Scikit-image\n\n• Opencv-python\n\n2.CUDA 下载\n\n• 从 [https://developer.nvidia.com/cuda-toolkit-archive](https://developer.nvidia.com/cuda-toolkit-archive) 中打开下载中心，找到相应版本，点击版本号即可进入下载页面\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n\n• 选择要下载的平台、版本号等，点击 DOWNLOAD 即可\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE2.png#alt=%E5%9B%BE2)\n\n3.cuDNN 下载·安装\n\n• 首先，把 CUDA 安装好，从 NVIDIA cuDNN | NVIDIA Developer [https://developer.nvidia.com/zh-cn/cudnn](https://developer.nvidia.com/zh-cn/cudnn)\n\n中打开 cuDNN 中心，点击“下载 cuDNN”，登录之后填写问卷即可下载。最后 bin、include、lib 三个文件夹里的文件复制到 CUDA 的对应文件夹中就行了。\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE3.png#alt=%E5%9B%BE3)\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE4.png#alt=%E5%9B%BE4)\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE5.png#alt=%E5%9B%BE5)\n\n4.单 gpu 训练/多 gpu 训练\n\n• 单 gpu 非常简单，只需要写图中语句即可用 keras 实现单 gpu 训练\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE6.png#alt=%E5%9B%BE6)\n\n• 多 gpu 需要用到 muti 函数生成模型，代码如下\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE7.png#alt=%E5%9B%BE7)\n\n5.找不到第二条 gpu\n\n• 有时候 keras 识别不了电脑的第二条 gpu，执行 muti 会报错如下：\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE8.png#alt=%E5%9B%BE8)\n\n• 我这次是因为执行了这个语句造成的，这个语句只能供单 gpu 的 model 使用\n\n![](Keras%C2%B7GPU%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95-%E5%A4%9A%EF%BC%89/%E5%9B%BE9.png#alt=%E5%9B%BE9)\n","slug":"xkk3s7","published":1,"date":"2021-09-27T07:04:30.346Z","updated":"2021-09-27T07:04:30.346Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cku2b35je0007d1a2cbp80mkn","content":"<hr>\n<h2 id=\"title-Keras·gpu-训练（单-多）date-2021-08-07-23-40-18\"><a href=\"#title-Keras·gpu-训练（单-多）date-2021-08-07-23-40-18\" class=\"headerlink\" title=\"title: Keras·gpu 训练（单\\多）date: 2021-08-07 23:40:18\"></a>title: Keras·gpu 训练（单\\多）date: 2021-08-07 23:40:18</h2><p>tags: [深度学习, 语义分割, Keras]<br>categories: 计算机视觉特辑</p>\n<p>在一切开始前，请确定计算机拥有英伟达的显卡。</p>\n<p>（不是英特尔！不是英特尔！不是英特尔！） 1.版本号：</p>\n<p>• keras2.2.4</p>\n<p>• Tensorflow-gpu1.12.0</p>\n<p>• CUDA9.0.176</p>\n<p>• cuDNN7.6.5 for CUDA 9.0</p>\n<p>• Scikit-image</p>\n<p>• Opencv-python</p>\n<p>2.CUDA 下载</p>\n<p>• 从 <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\">https://developer.nvidia.com/cuda-toolkit-archive</a> 中打开下载中心，找到相应版本，点击版本号即可进入下载页面</p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n<p>• 选择要下载的平台、版本号等，点击 DOWNLOAD 即可</p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE2.png#alt=%E5%9B%BE2\"></p>\n<p>3.cuDNN 下载·安装</p>\n<p>• 首先，把 CUDA 安装好，从 NVIDIA cuDNN | NVIDIA Developer <a href=\"https://developer.nvidia.com/zh-cn/cudnn\">https://developer.nvidia.com/zh-cn/cudnn</a></p>\n<p>中打开 cuDNN 中心，点击“下载 cuDNN”，登录之后填写问卷即可下载。最后 bin、include、lib 三个文件夹里的文件复制到 CUDA 的对应文件夹中就行了。</p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE3.png#alt=%E5%9B%BE3\"></p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE4.png#alt=%E5%9B%BE4\"></p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE5.png#alt=%E5%9B%BE5\"></p>\n<p>4.单 gpu 训练/多 gpu 训练</p>\n<p>• 单 gpu 非常简单，只需要写图中语句即可用 keras 实现单 gpu 训练</p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE6.png#alt=%E5%9B%BE6\"></p>\n<p>• 多 gpu 需要用到 muti 函数生成模型，代码如下</p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE7.png#alt=%E5%9B%BE7\"></p>\n<p>5.找不到第二条 gpu</p>\n<p>• 有时候 keras 识别不了电脑的第二条 gpu，执行 muti 会报错如下：</p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE8.png#alt=%E5%9B%BE8\"></p>\n<p>• 我这次是因为执行了这个语句造成的，这个语句只能供单 gpu 的 model 使用</p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE9.png#alt=%E5%9B%BE9\"></p>\n","site":{"data":{}},"cover":"/img/loading.gif","length":668,"excerpt":"","more":"<hr>\n<h2 id=\"title-Keras·gpu-训练（单-多）date-2021-08-07-23-40-18\"><a href=\"#title-Keras·gpu-训练（单-多）date-2021-08-07-23-40-18\" class=\"headerlink\" title=\"title: Keras·gpu 训练（单\\多）date: 2021-08-07 23:40:18\"></a>title: Keras·gpu 训练（单\\多）date: 2021-08-07 23:40:18</h2><p>tags: [深度学习, 语义分割, Keras]<br>categories: 计算机视觉特辑</p>\n<p>在一切开始前，请确定计算机拥有英伟达的显卡。</p>\n<p>（不是英特尔！不是英特尔！不是英特尔！） 1.版本号：</p>\n<p>• keras2.2.4</p>\n<p>• Tensorflow-gpu1.12.0</p>\n<p>• CUDA9.0.176</p>\n<p>• cuDNN7.6.5 for CUDA 9.0</p>\n<p>• Scikit-image</p>\n<p>• Opencv-python</p>\n<p>2.CUDA 下载</p>\n<p>• 从 <a href=\"https://developer.nvidia.com/cuda-toolkit-archive\">https://developer.nvidia.com/cuda-toolkit-archive</a> 中打开下载中心，找到相应版本，点击版本号即可进入下载页面</p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n<p>• 选择要下载的平台、版本号等，点击 DOWNLOAD 即可</p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE2.png#alt=%E5%9B%BE2\"></p>\n<p>3.cuDNN 下载·安装</p>\n<p>• 首先，把 CUDA 安装好，从 NVIDIA cuDNN | NVIDIA Developer <a href=\"https://developer.nvidia.com/zh-cn/cudnn\">https://developer.nvidia.com/zh-cn/cudnn</a></p>\n<p>中打开 cuDNN 中心，点击“下载 cuDNN”，登录之后填写问卷即可下载。最后 bin、include、lib 三个文件夹里的文件复制到 CUDA 的对应文件夹中就行了。</p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE3.png#alt=%E5%9B%BE3\"></p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE4.png#alt=%E5%9B%BE4\"></p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE5.png#alt=%E5%9B%BE5\"></p>\n<p>4.单 gpu 训练/多 gpu 训练</p>\n<p>• 单 gpu 非常简单，只需要写图中语句即可用 keras 实现单 gpu 训练</p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE6.png#alt=%E5%9B%BE6\"></p>\n<p>• 多 gpu 需要用到 muti 函数生成模型，代码如下</p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE7.png#alt=%E5%9B%BE7\"></p>\n<p>5.找不到第二条 gpu</p>\n<p>• 有时候 keras 识别不了电脑的第二条 gpu，执行 muti 会报错如下：</p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE8.png#alt=%E5%9B%BE8\"></p>\n<p>• 我这次是因为执行了这个语句造成的，这个语句只能供单 gpu 的 model 使用</p>\n<p><img src=\"/2021/09/27/xkk3s7/%E5%9B%BE9.png#alt=%E5%9B%BE9\"></p>\n"},{"_content":"---\n\n## title: TensorboardX 训练可视化 date: 2021-08-07 22:43:00\n\ntags: [深度学习, 语义分割, 可视化, TensorboardX]\ncategories: 计算机视觉特辑\n\n1.conda 环境配置：\n\n• tensorboardX2.2\n\n• Tensorboard2.5.0\n\n• PyTorch1.8.1\n\n• Torchvision0.9.1 2.查看记录\n\n• 首先学习以下 tensorboardX 怎么用。一般训练代码运行之后会同时生成 tensorboardX 的日志文件。这时复制日志文件所在文件夹路径，打开 Anaconda 命令行，切换环境至 torch，输入图中语句为日志文件夹创建 tensorboardX 默认的本地端口（格式：tensorboard --logdir PATH）\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n\n• 执行得到端口地址，复制到浏览器打开即可查看训练可视化内容\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE2.png#alt=%E5%9B%BE2)\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE3.png#alt=%E5%9B%BE3)\n\n• 关闭端口占用，只需长按 CTRL + C\n\n3.训练记录\n\n• 导入 SummaryWriter\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE4.png#alt=%E5%9B%BE4)\n\n• 在代码中初始化 SummaryWriter 实例，参数填记录的存储文件夹位置（有其他初始化方法，这里不常用）\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE5.png#alt=%E5%9B%BE5)\n\n• 训练常用记录类型：\n\n○ （scalar）单个数值\n\n§ 参数：\n\n□ Tag：该数据名称（如 train_acc），不同名称数据会用独立图表表示\n\n□ Scalar_value：数据值来源，一般是个 python 变量（如 train_acc）\n\n□ Global_step：存放当前 epoch 值\n\n□ walltime：默认值 time.time()，记录当下时间，一般填 None 不用\n\n§ 用法：如 writer.add_scalar()\n\n○ （scalars）多个数值\n\n多个数值的记录类型利用 python 字典生成日志\n\n§ 参数：\n\n□ Main_tag：该图表总的名称\n\n□ Tag_scalar_dict：各类值的字典（如下）\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE6.png#alt=%E5%9B%BE6)\n\n□ Global_step：存放当前 epoch 值\n\n□ walltime：默认值 time.time()，记录当下时间，一般填 None 不用\n\n§ 用法：如 writer.add_scalars()\n\n○ （graph）网络结构/运行图\n\n§ 参数：\n\n□ model：待可视化的网络模型\n\n□ Input_to_model：输入的一组真图片或者伪造的零值图片\n\n§ 用法：\n\n□ 首先使用 torch.randn(num,z,x,y)生成假数据，然后正常调用模型并切换至 train 状态\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE7.png#alt=%E5%9B%BE7)\n\n□ 然后使用 with 语句生成 SummaryWriter 实例并添加运行图\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE8.png#alt=%E5%9B%BE8)\n\n□ 当下文件夹目录会生成 runs 文件夹，这个文件路径为日志地址\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE9.gif#alt=%E5%9B%BE9)\n\n• 其他记录类型：\n\n• 一些问题\n\n○ 如果执行 add 操作后没有实时在网页可视化界面看到效果，试试重启 tensorboard\n","source":"_posts/zka8l2.md","raw":"---\n\n## title: TensorboardX 训练可视化 date: 2021-08-07 22:43:00\n\ntags: [深度学习, 语义分割, 可视化, TensorboardX]\ncategories: 计算机视觉特辑\n\n1.conda 环境配置：\n\n• tensorboardX2.2\n\n• Tensorboard2.5.0\n\n• PyTorch1.8.1\n\n• Torchvision0.9.1 2.查看记录\n\n• 首先学习以下 tensorboardX 怎么用。一般训练代码运行之后会同时生成 tensorboardX 的日志文件。这时复制日志文件所在文件夹路径，打开 Anaconda 命令行，切换环境至 torch，输入图中语句为日志文件夹创建 tensorboardX 默认的本地端口（格式：tensorboard --logdir PATH）\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n\n• 执行得到端口地址，复制到浏览器打开即可查看训练可视化内容\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE2.png#alt=%E5%9B%BE2)\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE3.png#alt=%E5%9B%BE3)\n\n• 关闭端口占用，只需长按 CTRL + C\n\n3.训练记录\n\n• 导入 SummaryWriter\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE4.png#alt=%E5%9B%BE4)\n\n• 在代码中初始化 SummaryWriter 实例，参数填记录的存储文件夹位置（有其他初始化方法，这里不常用）\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE5.png#alt=%E5%9B%BE5)\n\n• 训练常用记录类型：\n\n○ （scalar）单个数值\n\n§ 参数：\n\n□ Tag：该数据名称（如 train_acc），不同名称数据会用独立图表表示\n\n□ Scalar_value：数据值来源，一般是个 python 变量（如 train_acc）\n\n□ Global_step：存放当前 epoch 值\n\n□ walltime：默认值 time.time()，记录当下时间，一般填 None 不用\n\n§ 用法：如 writer.add_scalar()\n\n○ （scalars）多个数值\n\n多个数值的记录类型利用 python 字典生成日志\n\n§ 参数：\n\n□ Main_tag：该图表总的名称\n\n□ Tag_scalar_dict：各类值的字典（如下）\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE6.png#alt=%E5%9B%BE6)\n\n□ Global_step：存放当前 epoch 值\n\n□ walltime：默认值 time.time()，记录当下时间，一般填 None 不用\n\n§ 用法：如 writer.add_scalars()\n\n○ （graph）网络结构/运行图\n\n§ 参数：\n\n□ model：待可视化的网络模型\n\n□ Input_to_model：输入的一组真图片或者伪造的零值图片\n\n§ 用法：\n\n□ 首先使用 torch.randn(num,z,x,y)生成假数据，然后正常调用模型并切换至 train 状态\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE7.png#alt=%E5%9B%BE7)\n\n□ 然后使用 with 语句生成 SummaryWriter 实例并添加运行图\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE8.png#alt=%E5%9B%BE8)\n\n□ 当下文件夹目录会生成 runs 文件夹，这个文件路径为日志地址\n\n![](TensorboardX%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/%E5%9B%BE9.gif#alt=%E5%9B%BE9)\n\n• 其他记录类型：\n\n• 一些问题\n\n○ 如果执行 add 操作后没有实时在网页可视化界面看到效果，试试重启 tensorboard\n","slug":"zka8l2","published":1,"date":"2021-09-27T07:04:30.266Z","updated":"2021-09-27T07:04:30.267Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cku2b35jf0008d1a27okvapw6","content":"<hr>\n<h2 id=\"title-TensorboardX-训练可视化-date-2021-08-07-22-43-00\"><a href=\"#title-TensorboardX-训练可视化-date-2021-08-07-22-43-00\" class=\"headerlink\" title=\"title: TensorboardX 训练可视化 date: 2021-08-07 22:43:00\"></a>title: TensorboardX 训练可视化 date: 2021-08-07 22:43:00</h2><p>tags: [深度学习, 语义分割, 可视化, TensorboardX]<br>categories: 计算机视觉特辑</p>\n<p>1.conda 环境配置：</p>\n<p>• tensorboardX2.2</p>\n<p>• Tensorboard2.5.0</p>\n<p>• PyTorch1.8.1</p>\n<p>• Torchvision0.9.1 2.查看记录</p>\n<p>• 首先学习以下 tensorboardX 怎么用。一般训练代码运行之后会同时生成 tensorboardX 的日志文件。这时复制日志文件所在文件夹路径，打开 Anaconda 命令行，切换环境至 torch，输入图中语句为日志文件夹创建 tensorboardX 默认的本地端口（格式：tensorboard –logdir PATH）</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n<p>• 执行得到端口地址，复制到浏览器打开即可查看训练可视化内容</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE2.png#alt=%E5%9B%BE2\"></p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE3.png#alt=%E5%9B%BE3\"></p>\n<p>• 关闭端口占用，只需长按 CTRL + C</p>\n<p>3.训练记录</p>\n<p>• 导入 SummaryWriter</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE4.png#alt=%E5%9B%BE4\"></p>\n<p>• 在代码中初始化 SummaryWriter 实例，参数填记录的存储文件夹位置（有其他初始化方法，这里不常用）</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE5.png#alt=%E5%9B%BE5\"></p>\n<p>• 训练常用记录类型：</p>\n<p>○ （scalar）单个数值</p>\n<p>§ 参数：</p>\n<p>□ Tag：该数据名称（如 train_acc），不同名称数据会用独立图表表示</p>\n<p>□ Scalar_value：数据值来源，一般是个 python 变量（如 train_acc）</p>\n<p>□ Global_step：存放当前 epoch 值</p>\n<p>□ walltime：默认值 time.time()，记录当下时间，一般填 None 不用</p>\n<p>§ 用法：如 writer.add_scalar()</p>\n<p>○ （scalars）多个数值</p>\n<p>多个数值的记录类型利用 python 字典生成日志</p>\n<p>§ 参数：</p>\n<p>□ Main_tag：该图表总的名称</p>\n<p>□ Tag_scalar_dict：各类值的字典（如下）</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE6.png#alt=%E5%9B%BE6\"></p>\n<p>□ Global_step：存放当前 epoch 值</p>\n<p>□ walltime：默认值 time.time()，记录当下时间，一般填 None 不用</p>\n<p>§ 用法：如 writer.add_scalars()</p>\n<p>○ （graph）网络结构/运行图</p>\n<p>§ 参数：</p>\n<p>□ model：待可视化的网络模型</p>\n<p>□ Input_to_model：输入的一组真图片或者伪造的零值图片</p>\n<p>§ 用法：</p>\n<p>□ 首先使用 torch.randn(num,z,x,y)生成假数据，然后正常调用模型并切换至 train 状态</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE7.png#alt=%E5%9B%BE7\"></p>\n<p>□ 然后使用 with 语句生成 SummaryWriter 实例并添加运行图</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE8.png#alt=%E5%9B%BE8\"></p>\n<p>□ 当下文件夹目录会生成 runs 文件夹，这个文件路径为日志地址</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE9.gif#alt=%E5%9B%BE9\"></p>\n<p>• 其他记录类型：</p>\n<p>• 一些问题</p>\n<p>○ 如果执行 add 操作后没有实时在网页可视化界面看到效果，试试重启 tensorboard</p>\n","site":{"data":{}},"cover":"/img/loading.gif","length":1072,"excerpt":"","more":"<hr>\n<h2 id=\"title-TensorboardX-训练可视化-date-2021-08-07-22-43-00\"><a href=\"#title-TensorboardX-训练可视化-date-2021-08-07-22-43-00\" class=\"headerlink\" title=\"title: TensorboardX 训练可视化 date: 2021-08-07 22:43:00\"></a>title: TensorboardX 训练可视化 date: 2021-08-07 22:43:00</h2><p>tags: [深度学习, 语义分割, 可视化, TensorboardX]<br>categories: 计算机视觉特辑</p>\n<p>1.conda 环境配置：</p>\n<p>• tensorboardX2.2</p>\n<p>• Tensorboard2.5.0</p>\n<p>• PyTorch1.8.1</p>\n<p>• Torchvision0.9.1 2.查看记录</p>\n<p>• 首先学习以下 tensorboardX 怎么用。一般训练代码运行之后会同时生成 tensorboardX 的日志文件。这时复制日志文件所在文件夹路径，打开 Anaconda 命令行，切换环境至 torch，输入图中语句为日志文件夹创建 tensorboardX 默认的本地端口（格式：tensorboard –logdir PATH）</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n<p>• 执行得到端口地址，复制到浏览器打开即可查看训练可视化内容</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE2.png#alt=%E5%9B%BE2\"></p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE3.png#alt=%E5%9B%BE3\"></p>\n<p>• 关闭端口占用，只需长按 CTRL + C</p>\n<p>3.训练记录</p>\n<p>• 导入 SummaryWriter</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE4.png#alt=%E5%9B%BE4\"></p>\n<p>• 在代码中初始化 SummaryWriter 实例，参数填记录的存储文件夹位置（有其他初始化方法，这里不常用）</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE5.png#alt=%E5%9B%BE5\"></p>\n<p>• 训练常用记录类型：</p>\n<p>○ （scalar）单个数值</p>\n<p>§ 参数：</p>\n<p>□ Tag：该数据名称（如 train_acc），不同名称数据会用独立图表表示</p>\n<p>□ Scalar_value：数据值来源，一般是个 python 变量（如 train_acc）</p>\n<p>□ Global_step：存放当前 epoch 值</p>\n<p>□ walltime：默认值 time.time()，记录当下时间，一般填 None 不用</p>\n<p>§ 用法：如 writer.add_scalar()</p>\n<p>○ （scalars）多个数值</p>\n<p>多个数值的记录类型利用 python 字典生成日志</p>\n<p>§ 参数：</p>\n<p>□ Main_tag：该图表总的名称</p>\n<p>□ Tag_scalar_dict：各类值的字典（如下）</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE6.png#alt=%E5%9B%BE6\"></p>\n<p>□ Global_step：存放当前 epoch 值</p>\n<p>□ walltime：默认值 time.time()，记录当下时间，一般填 None 不用</p>\n<p>§ 用法：如 writer.add_scalars()</p>\n<p>○ （graph）网络结构/运行图</p>\n<p>§ 参数：</p>\n<p>□ model：待可视化的网络模型</p>\n<p>□ Input_to_model：输入的一组真图片或者伪造的零值图片</p>\n<p>§ 用法：</p>\n<p>□ 首先使用 torch.randn(num,z,x,y)生成假数据，然后正常调用模型并切换至 train 状态</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE7.png#alt=%E5%9B%BE7\"></p>\n<p>□ 然后使用 with 语句生成 SummaryWriter 实例并添加运行图</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE8.png#alt=%E5%9B%BE8\"></p>\n<p>□ 当下文件夹目录会生成 runs 文件夹，这个文件路径为日志地址</p>\n<p><img src=\"/2021/09/27/zka8l2/%E5%9B%BE9.gif#alt=%E5%9B%BE9\"></p>\n<p>• 其他记录类型：</p>\n<p>• 一些问题</p>\n<p>○ 如果执行 add 操作后没有实时在网页可视化界面看到效果，试试重启 tensorboard</p>\n"},{"_content":"---\n\n## title: Pytorch·API 部署 WEB·ONNXdate: 2021-08-07 23:25:16\n\ntags: [深度学习, 语义分割, Pytorch, ONNX, Web 开发]\ncategories: 计算机视觉特辑\n\n1.API 接口，主要是通过中间商，为目标端暴露功能函数作为输入、处理、输出的桥梁。\n2.WEB 预测 API 开发（基于 ONNX）\n\n• ONNX 是用于机器学习模型云端发布的文件格式，是适用于不同框架的规范工具，\n\n参考视频：How to run PyTorch models in the browser with ONNX.js - YouTube\n\n[https://www.youtube.com/watch?v=Vs730jsRgO8](https://www.youtube.com/watch?v=Vs730jsRgO8)\n\n项目地址：pytorch-to-javascript-with-onnx - CodeSandbox\n\n[https://codesandbox.io/s/vgzep?file=/index.html](https://codesandbox.io/s/vgzep?file=/index.html)\n\n• 训练并保存 model.state_dict()\n\n○ ONNX 支持 pt 格式的模型介绍文件（model.state_dict()），训练时输入以下语句保存，也可以是 pth 格式\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n\n○ 注意搭建模型的时候检查激活函数是否支持 ONNX，查询相关文档：\n\n[https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/operators.md](https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/operators.md)\n\n切换激活函数后还要更换对应的损失函数算法，比如 softmax 对应 nn.functional.cross_entropy()\n\n○ 另外，ONNX 仅支持单 GPU 训练的 model，切勿使用 DataParellel 等 model。\n\n○ 可以根据网络模型的 github 官网复制代码，搭建网络的时候要告知前端输入的图片尺寸，比如画布要绘制高清图可以预设网络输入尺寸为(280,280,4)，后期在池化层池化十倍即可。若 onnx 不能起作用，检查网络的 pytorch 语句是否有 bug（如池化时不能对图片切片而必须使用 torch.narrow(x)），可以查看 pytorch 官网文档或者中文文档的解释：\n\n[https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html) or [https://pytorch-cn.readthedocs.io/zh/latest/](https://pytorch-cn.readthedocs.io/zh/latest/)\n\n• pt 转 onnx 文件\n\n○ 载入 pt 文件，切换为 eval 模式\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE2.png#alt=%E5%9B%BE2)\n\n○ 预先构造空的待测影像矩阵，并同相关参数传入 torch.onnx.export()，导出 onnx 文件\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE3.png#alt=%E5%9B%BE3)\n\n• 前端对 ONNX 文件的获取和操作/API 接口搭建\n\n○ 首先，必须输入图中语句调用 ONNX 服务\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE4.png#alt=%E5%9B%BE4)\n\n○ 训练后生成的 ONNX 文件直接部署到前端项目文件夹，ONNX 提供如下 js 操作以执行预测：\n\n§ 新建 Session，并在 Session 中载入 model\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE5.png#alt=%E5%9B%BE5)\n\n§ 获取服务器待预测图片，传入 Tensor\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE6.png#alt=%E5%9B%BE6)\n\n§ 对图片执行预测\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE7.png#alt=%E5%9B%BE7)\n\n§ 获取 model 输出 Tensor\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE8.png#alt=%E5%9B%BE8)\n\n§ 从输出 Tensor 提取出概率分布矩阵\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE9.png#alt=%E5%9B%BE9)\n\n§ 提取概率最大对应的分类结果（转化为分类结果）\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE10.png#alt=%E5%9B%BE10)\n\n○ 每次后端准备搭建 API，以上都需要后端人员介绍给前端人员。\n\n3.上面内容只是以分类任务为例子，👉\n\n[https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/api.md](https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/api.md) 有更多 API 搭建手段提供给分类、检测、分割等不同计算机视觉任务，关于遥感智能解译还要进一步探索。\n","source":"_posts/uqudv9.md","raw":"---\n\n## title: Pytorch·API 部署 WEB·ONNXdate: 2021-08-07 23:25:16\n\ntags: [深度学习, 语义分割, Pytorch, ONNX, Web 开发]\ncategories: 计算机视觉特辑\n\n1.API 接口，主要是通过中间商，为目标端暴露功能函数作为输入、处理、输出的桥梁。\n2.WEB 预测 API 开发（基于 ONNX）\n\n• ONNX 是用于机器学习模型云端发布的文件格式，是适用于不同框架的规范工具，\n\n参考视频：How to run PyTorch models in the browser with ONNX.js - YouTube\n\n[https://www.youtube.com/watch?v=Vs730jsRgO8](https://www.youtube.com/watch?v=Vs730jsRgO8)\n\n项目地址：pytorch-to-javascript-with-onnx - CodeSandbox\n\n[https://codesandbox.io/s/vgzep?file=/index.html](https://codesandbox.io/s/vgzep?file=/index.html)\n\n• 训练并保存 model.state_dict()\n\n○ ONNX 支持 pt 格式的模型介绍文件（model.state_dict()），训练时输入以下语句保存，也可以是 pth 格式\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE1.png#alt=%E5%9B%BE1)\n\n○ 注意搭建模型的时候检查激活函数是否支持 ONNX，查询相关文档：\n\n[https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/operators.md](https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/operators.md)\n\n切换激活函数后还要更换对应的损失函数算法，比如 softmax 对应 nn.functional.cross_entropy()\n\n○ 另外，ONNX 仅支持单 GPU 训练的 model，切勿使用 DataParellel 等 model。\n\n○ 可以根据网络模型的 github 官网复制代码，搭建网络的时候要告知前端输入的图片尺寸，比如画布要绘制高清图可以预设网络输入尺寸为(280,280,4)，后期在池化层池化十倍即可。若 onnx 不能起作用，检查网络的 pytorch 语句是否有 bug（如池化时不能对图片切片而必须使用 torch.narrow(x)），可以查看 pytorch 官网文档或者中文文档的解释：\n\n[https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html) or [https://pytorch-cn.readthedocs.io/zh/latest/](https://pytorch-cn.readthedocs.io/zh/latest/)\n\n• pt 转 onnx 文件\n\n○ 载入 pt 文件，切换为 eval 模式\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE2.png#alt=%E5%9B%BE2)\n\n○ 预先构造空的待测影像矩阵，并同相关参数传入 torch.onnx.export()，导出 onnx 文件\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE3.png#alt=%E5%9B%BE3)\n\n• 前端对 ONNX 文件的获取和操作/API 接口搭建\n\n○ 首先，必须输入图中语句调用 ONNX 服务\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE4.png#alt=%E5%9B%BE4)\n\n○ 训练后生成的 ONNX 文件直接部署到前端项目文件夹，ONNX 提供如下 js 操作以执行预测：\n\n§ 新建 Session，并在 Session 中载入 model\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE5.png#alt=%E5%9B%BE5)\n\n§ 获取服务器待预测图片，传入 Tensor\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE6.png#alt=%E5%9B%BE6)\n\n§ 对图片执行预测\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE7.png#alt=%E5%9B%BE7)\n\n§ 获取 model 输出 Tensor\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE8.png#alt=%E5%9B%BE8)\n\n§ 从输出 Tensor 提取出概率分布矩阵\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE9.png#alt=%E5%9B%BE9)\n\n§ 提取概率最大对应的分类结果（转化为分类结果）\n\n![](Pytorch%C2%B7API%E9%83%A8%E7%BD%B2WEB%C2%B7ONNX/%E5%9B%BE10.png#alt=%E5%9B%BE10)\n\n○ 每次后端准备搭建 API，以上都需要后端人员介绍给前端人员。\n\n3.上面内容只是以分类任务为例子，👉\n\n[https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/api.md](https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/api.md) 有更多 API 搭建手段提供给分类、检测、分割等不同计算机视觉任务，关于遥感智能解译还要进一步探索。\n","slug":"uqudv9","published":1,"date":"2021-09-27T07:04:30.308Z","updated":"2021-09-27T07:04:30.308Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cku2b35jg0009d1a2bainfsb3","content":"<hr>\n<h2 id=\"title-Pytorch·API-部署-WEB·ONNXdate-2021-08-07-23-25-16\"><a href=\"#title-Pytorch·API-部署-WEB·ONNXdate-2021-08-07-23-25-16\" class=\"headerlink\" title=\"title: Pytorch·API 部署 WEB·ONNXdate: 2021-08-07 23:25:16\"></a>title: Pytorch·API 部署 WEB·ONNXdate: 2021-08-07 23:25:16</h2><p>tags: [深度学习, 语义分割, Pytorch, ONNX, Web 开发]<br>categories: 计算机视觉特辑</p>\n<p>1.API 接口，主要是通过中间商，为目标端暴露功能函数作为输入、处理、输出的桥梁。<br>2.WEB 预测 API 开发（基于 ONNX）</p>\n<p>• ONNX 是用于机器学习模型云端发布的文件格式，是适用于不同框架的规范工具，</p>\n<p>参考视频：How to run PyTorch models in the browser with ONNX.js - YouTube</p>\n<p><a href=\"https://www.youtube.com/watch?v=Vs730jsRgO8\">https://www.youtube.com/watch?v=Vs730jsRgO8</a></p>\n<p>项目地址：pytorch-to-javascript-with-onnx - CodeSandbox</p>\n<p><a href=\"https://codesandbox.io/s/vgzep?file=/index.html\">https://codesandbox.io/s/vgzep?file=/index.html</a></p>\n<p>• 训练并保存 model.state_dict()</p>\n<p>○ ONNX 支持 pt 格式的模型介绍文件（model.state_dict()），训练时输入以下语句保存，也可以是 pth 格式</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n<p>○ 注意搭建模型的时候检查激活函数是否支持 ONNX，查询相关文档：</p>\n<p><a href=\"https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/operators.md\">https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/operators.md</a></p>\n<p>切换激活函数后还要更换对应的损失函数算法，比如 softmax 对应 nn.functional.cross_entropy()</p>\n<p>○ 另外，ONNX 仅支持单 GPU 训练的 model，切勿使用 DataParellel 等 model。</p>\n<p>○ 可以根据网络模型的 github 官网复制代码，搭建网络的时候要告知前端输入的图片尺寸，比如画布要绘制高清图可以预设网络输入尺寸为(280,280,4)，后期在池化层池化十倍即可。若 onnx 不能起作用，检查网络的 pytorch 语句是否有 bug（如池化时不能对图片切片而必须使用 torch.narrow(x)），可以查看 pytorch 官网文档或者中文文档的解释：</p>\n<p><a href=\"https://pytorch.org/docs/stable/index.html\">https://pytorch.org/docs/stable/index.html</a> or <a href=\"https://pytorch-cn.readthedocs.io/zh/latest/\">https://pytorch-cn.readthedocs.io/zh/latest/</a></p>\n<p>• pt 转 onnx 文件</p>\n<p>○ 载入 pt 文件，切换为 eval 模式</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE2.png#alt=%E5%9B%BE2\"></p>\n<p>○ 预先构造空的待测影像矩阵，并同相关参数传入 torch.onnx.export()，导出 onnx 文件</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE3.png#alt=%E5%9B%BE3\"></p>\n<p>• 前端对 ONNX 文件的获取和操作/API 接口搭建</p>\n<p>○ 首先，必须输入图中语句调用 ONNX 服务</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE4.png#alt=%E5%9B%BE4\"></p>\n<p>○ 训练后生成的 ONNX 文件直接部署到前端项目文件夹，ONNX 提供如下 js 操作以执行预测：</p>\n<p>§ 新建 Session，并在 Session 中载入 model</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE5.png#alt=%E5%9B%BE5\"></p>\n<p>§ 获取服务器待预测图片，传入 Tensor</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE6.png#alt=%E5%9B%BE6\"></p>\n<p>§ 对图片执行预测</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE7.png#alt=%E5%9B%BE7\"></p>\n<p>§ 获取 model 输出 Tensor</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE8.png#alt=%E5%9B%BE8\"></p>\n<p>§ 从输出 Tensor 提取出概率分布矩阵</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE9.png#alt=%E5%9B%BE9\"></p>\n<p>§ 提取概率最大对应的分类结果（转化为分类结果）</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE10.png#alt=%E5%9B%BE10\"></p>\n<p>○ 每次后端准备搭建 API，以上都需要后端人员介绍给前端人员。</p>\n<p>3.上面内容只是以分类任务为例子，👉</p>\n<p><a href=\"https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/api.md\">https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/api.md</a> 有更多 API 搭建手段提供给分类、检测、分割等不同计算机视觉任务，关于遥感智能解译还要进一步探索。</p>\n","site":{"data":{}},"cover":"/img/loading.gif","length":1388,"excerpt":"","more":"<hr>\n<h2 id=\"title-Pytorch·API-部署-WEB·ONNXdate-2021-08-07-23-25-16\"><a href=\"#title-Pytorch·API-部署-WEB·ONNXdate-2021-08-07-23-25-16\" class=\"headerlink\" title=\"title: Pytorch·API 部署 WEB·ONNXdate: 2021-08-07 23:25:16\"></a>title: Pytorch·API 部署 WEB·ONNXdate: 2021-08-07 23:25:16</h2><p>tags: [深度学习, 语义分割, Pytorch, ONNX, Web 开发]<br>categories: 计算机视觉特辑</p>\n<p>1.API 接口，主要是通过中间商，为目标端暴露功能函数作为输入、处理、输出的桥梁。<br>2.WEB 预测 API 开发（基于 ONNX）</p>\n<p>• ONNX 是用于机器学习模型云端发布的文件格式，是适用于不同框架的规范工具，</p>\n<p>参考视频：How to run PyTorch models in the browser with ONNX.js - YouTube</p>\n<p><a href=\"https://www.youtube.com/watch?v=Vs730jsRgO8\">https://www.youtube.com/watch?v=Vs730jsRgO8</a></p>\n<p>项目地址：pytorch-to-javascript-with-onnx - CodeSandbox</p>\n<p><a href=\"https://codesandbox.io/s/vgzep?file=/index.html\">https://codesandbox.io/s/vgzep?file=/index.html</a></p>\n<p>• 训练并保存 model.state_dict()</p>\n<p>○ ONNX 支持 pt 格式的模型介绍文件（model.state_dict()），训练时输入以下语句保存，也可以是 pth 格式</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE1.png#alt=%E5%9B%BE1\"></p>\n<p>○ 注意搭建模型的时候检查激活函数是否支持 ONNX，查询相关文档：</p>\n<p><a href=\"https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/operators.md\">https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/operators.md</a></p>\n<p>切换激活函数后还要更换对应的损失函数算法，比如 softmax 对应 nn.functional.cross_entropy()</p>\n<p>○ 另外，ONNX 仅支持单 GPU 训练的 model，切勿使用 DataParellel 等 model。</p>\n<p>○ 可以根据网络模型的 github 官网复制代码，搭建网络的时候要告知前端输入的图片尺寸，比如画布要绘制高清图可以预设网络输入尺寸为(280,280,4)，后期在池化层池化十倍即可。若 onnx 不能起作用，检查网络的 pytorch 语句是否有 bug（如池化时不能对图片切片而必须使用 torch.narrow(x)），可以查看 pytorch 官网文档或者中文文档的解释：</p>\n<p><a href=\"https://pytorch.org/docs/stable/index.html\">https://pytorch.org/docs/stable/index.html</a> or <a href=\"https://pytorch-cn.readthedocs.io/zh/latest/\">https://pytorch-cn.readthedocs.io/zh/latest/</a></p>\n<p>• pt 转 onnx 文件</p>\n<p>○ 载入 pt 文件，切换为 eval 模式</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE2.png#alt=%E5%9B%BE2\"></p>\n<p>○ 预先构造空的待测影像矩阵，并同相关参数传入 torch.onnx.export()，导出 onnx 文件</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE3.png#alt=%E5%9B%BE3\"></p>\n<p>• 前端对 ONNX 文件的获取和操作/API 接口搭建</p>\n<p>○ 首先，必须输入图中语句调用 ONNX 服务</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE4.png#alt=%E5%9B%BE4\"></p>\n<p>○ 训练后生成的 ONNX 文件直接部署到前端项目文件夹，ONNX 提供如下 js 操作以执行预测：</p>\n<p>§ 新建 Session，并在 Session 中载入 model</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE5.png#alt=%E5%9B%BE5\"></p>\n<p>§ 获取服务器待预测图片，传入 Tensor</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE6.png#alt=%E5%9B%BE6\"></p>\n<p>§ 对图片执行预测</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE7.png#alt=%E5%9B%BE7\"></p>\n<p>§ 获取 model 输出 Tensor</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE8.png#alt=%E5%9B%BE8\"></p>\n<p>§ 从输出 Tensor 提取出概率分布矩阵</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE9.png#alt=%E5%9B%BE9\"></p>\n<p>§ 提取概率最大对应的分类结果（转化为分类结果）</p>\n<p><img src=\"/2021/09/27/uqudv9/%E5%9B%BE10.png#alt=%E5%9B%BE10\"></p>\n<p>○ 每次后端准备搭建 API，以上都需要后端人员介绍给前端人员。</p>\n<p>3.上面内容只是以分类任务为例子，👉</p>\n<p><a href=\"https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/api.md\">https://github.com/microsoft/onnxjs/blob/v0.1.8/docs/api.md</a> 有更多 API 搭建手段提供给分类、检测、分割等不同计算机视觉任务，关于遥感智能解译还要进一步探索。</p>\n"},{"_content":"---\n\n## title: GIS 的简要定义 date: 2021-07-12 21:49:33\n\ntags: [GIS, GISer, 思想]\ncategories: 地信原理特辑\n\n1.GIS 的四层定义：\n\n• 第一层面，表现层（最直观的，非科班出身的理解），就是一张地图的可视化；\n\n• 第二层面，应用层（最对口的，接触最多的领域），是一类软件，集合了对地理要素的编辑查询分析等功能；\n\n• 第三层面，服务层（最近走入人们的视野），读取地图数据后，提供一系列 API，面向开发者和最终用户提供数据处理、数据分析、数据发布等服务包；\n\n• 第四层面，数据层，GIS 主要的两个数据类型：矢量和栅格；这两个数据类型又有多种格式如 SHP、GDB、GRID 等。这一层体现了 GIS 的数据结构以及存储方式。 2.关于：\n\n• 从第一层到第四层 GIS 的认识是逐步加深的，GIS 作为一个系统，用于建模（modeling our world）人类认知的世界中的地理要素，存在着从现实到虚拟的概要简化，作为矢量格式或者栅格格式存储。将一系列底层功能如数据处理、数据分析、数据发布的代码整合到 API 发布给开发者，最后开发者包装数据编辑、查询、分析等功能形成软件平台提供给用户，数据最终都转换为数字化地图。\n","source":"_posts/wgc27n.md","raw":"---\n\n## title: GIS 的简要定义 date: 2021-07-12 21:49:33\n\ntags: [GIS, GISer, 思想]\ncategories: 地信原理特辑\n\n1.GIS 的四层定义：\n\n• 第一层面，表现层（最直观的，非科班出身的理解），就是一张地图的可视化；\n\n• 第二层面，应用层（最对口的，接触最多的领域），是一类软件，集合了对地理要素的编辑查询分析等功能；\n\n• 第三层面，服务层（最近走入人们的视野），读取地图数据后，提供一系列 API，面向开发者和最终用户提供数据处理、数据分析、数据发布等服务包；\n\n• 第四层面，数据层，GIS 主要的两个数据类型：矢量和栅格；这两个数据类型又有多种格式如 SHP、GDB、GRID 等。这一层体现了 GIS 的数据结构以及存储方式。 2.关于：\n\n• 从第一层到第四层 GIS 的认识是逐步加深的，GIS 作为一个系统，用于建模（modeling our world）人类认知的世界中的地理要素，存在着从现实到虚拟的概要简化，作为矢量格式或者栅格格式存储。将一系列底层功能如数据处理、数据分析、数据发布的代码整合到 API 发布给开发者，最后开发者包装数据编辑、查询、分析等功能形成软件平台提供给用户，数据最终都转换为数字化地图。\n","slug":"wgc27n","published":1,"date":"2021-09-27T07:04:30.435Z","updated":"2021-09-27T07:04:30.435Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cku2b35jg000ad1a2gmmz9a5x","content":"<hr>\n<h2 id=\"title-GIS-的简要定义-date-2021-07-12-21-49-33\"><a href=\"#title-GIS-的简要定义-date-2021-07-12-21-49-33\" class=\"headerlink\" title=\"title: GIS 的简要定义 date: 2021-07-12 21:49:33\"></a>title: GIS 的简要定义 date: 2021-07-12 21:49:33</h2><p>tags: [GIS, GISer, 思想]<br>categories: 地信原理特辑</p>\n<p>1.GIS 的四层定义：</p>\n<p>• 第一层面，表现层（最直观的，非科班出身的理解），就是一张地图的可视化；</p>\n<p>• 第二层面，应用层（最对口的，接触最多的领域），是一类软件，集合了对地理要素的编辑查询分析等功能；</p>\n<p>• 第三层面，服务层（最近走入人们的视野），读取地图数据后，提供一系列 API，面向开发者和最终用户提供数据处理、数据分析、数据发布等服务包；</p>\n<p>• 第四层面，数据层，GIS 主要的两个数据类型：矢量和栅格；这两个数据类型又有多种格式如 SHP、GDB、GRID 等。这一层体现了 GIS 的数据结构以及存储方式。 2.关于：</p>\n<p>• 从第一层到第四层 GIS 的认识是逐步加深的，GIS 作为一个系统，用于建模（modeling our world）人类认知的世界中的地理要素，存在着从现实到虚拟的概要简化，作为矢量格式或者栅格格式存储。将一系列底层功能如数据处理、数据分析、数据发布的代码整合到 API 发布给开发者，最后开发者包装数据编辑、查询、分析等功能形成软件平台提供给用户，数据最终都转换为数字化地图。</p>\n","site":{"data":{}},"cover":"/img/loading.gif","length":504,"excerpt":"","more":"<hr>\n<h2 id=\"title-GIS-的简要定义-date-2021-07-12-21-49-33\"><a href=\"#title-GIS-的简要定义-date-2021-07-12-21-49-33\" class=\"headerlink\" title=\"title: GIS 的简要定义 date: 2021-07-12 21:49:33\"></a>title: GIS 的简要定义 date: 2021-07-12 21:49:33</h2><p>tags: [GIS, GISer, 思想]<br>categories: 地信原理特辑</p>\n<p>1.GIS 的四层定义：</p>\n<p>• 第一层面，表现层（最直观的，非科班出身的理解），就是一张地图的可视化；</p>\n<p>• 第二层面，应用层（最对口的，接触最多的领域），是一类软件，集合了对地理要素的编辑查询分析等功能；</p>\n<p>• 第三层面，服务层（最近走入人们的视野），读取地图数据后，提供一系列 API，面向开发者和最终用户提供数据处理、数据分析、数据发布等服务包；</p>\n<p>• 第四层面，数据层，GIS 主要的两个数据类型：矢量和栅格；这两个数据类型又有多种格式如 SHP、GDB、GRID 等。这一层体现了 GIS 的数据结构以及存储方式。 2.关于：</p>\n<p>• 从第一层到第四层 GIS 的认识是逐步加深的，GIS 作为一个系统，用于建模（modeling our world）人类认知的世界中的地理要素，存在着从现实到虚拟的概要简化，作为矢量格式或者栅格格式存储。将一系列底层功能如数据处理、数据分析、数据发布的代码整合到 API 发布给开发者，最后开发者包装数据编辑、查询、分析等功能形成软件平台提供给用户，数据最终都转换为数字化地图。</p>\n"},{"_content":"---\n\n## title: 美食之美——《雅舍谈吃》date: 2021-07-06 23:07:04\n\ntags: 美食\ncategories: 生活随笔\nswiper_index: 8\nswiper_desc: 简单好用的 hexo 博客文章置顶插件！\nswiper_cover: /img/weixin.png\n\n```\n美，不尽收；食，不尽全。 ——题记\n```\n\n谈到美食，正如饥肠辘辘的人，心里面急迫等待着吃的味觉。但不是所谓，仅仅为着生存。追寻美食的目标，不像一种世俗的冲动，正如雅舍先生题序「我以为要求美味固是人欲，然而何曾有背于天理？如果天理不包括美味的要求在内，上天之人，在舌头上为什么要生那么多的味蕾」。\n   吃得好，是让人幸福，是对舌尖感受的升华，一旦提到耳熟能详的菜名，人的愉悦便像剑拔弩张，舌尖一触，百般馋舌。左右结构的「馋」字，右边两点比喻两腿迅猛，迅猛狡兔之肉，并非丰足，但人为了啖其美味，愿意逐此狡兔，故写作馋。西施舌、火腿、醋溜鱼、烤羊肉、烧鸭……谁若吃一口，也不至于在脑袋里臆想，两腿一跺，手掌一直，啪的一下，就算有事缠身也无暇顾及。然而，追求高级的味觉，就必然失去点东西，尤其时间宝贵，未必能每次都放纵自己。\n\n今年除夕，我从繁忙学业中抽身，冒着病毒的风险，踉跄到广东家中度过佳节。目的尚且鲜明，写在文章也就印证我所说为了吃美食。老母亲的美味已经不是天天所得，不回一趟家，不吃慈母饭。而明年除夕，已经计划好的美食清单，都在湖南临武，是为了外婆欢庆大寿，为了庆幸生能继续享用湘菜美味。诚然，计划是幸福的计划，我学业若是耽误，便影响我母亲回乡的心情。\n\n学业之中，不代表就缺乏美味。学校前门小吃街，第一条正中间的店面以「炒」为技艺，炒粉丝、炒饭、炒土豆粉、炒米线……每份炒制不少料、不过熟，香味浓郁滑入咽喉。旁边五碗小菜随取，勺筷俱全，好不心爽幸福。校外也有全州拌饭，馋嘴烤鱼、淮南牛肉汤、鸡蛋灌饼、特制酸奶等等玲琅满，如饕餮大餐，本人如数家珍，乐子甚广。\n\n但不是美食就令人幸福的，美食反被美食误，吃得不好，有可能还要怪罪一下美食。\n\n美食为了尝而点，而不是为了点而尝。打开一个外卖软件，铺天盖地的优惠券、广告条、满减促销，全然改变了美食服务的本质。美食不是为了优惠而美，但你看，我想起 26 元的双层牛肉芝士汉堡，打开外卖软件，又开始送我优惠券，一张是满 27 减 5，一张是满 80 减 9。我心念的汉堡计划被搁置一旁。现在，我购物车换了又换，想法改了又改，点了一家鸡架，满减很高，大份鸡架配油饼、薯条、龙串、千叶只要 81，满减后 43，优惠券折至 34，天啊，真捡大便宜了，一个人，母亲常骂我浪费，也怕生冷不好吃，终于胃饱难入。于是，半个月后的一天，我说：「我心心念念的大汉堡还没吃到呢」，于是又习惯地，打开了外卖软件……\n\n啖美味的人是得爽口，而不是失口德。点到为止，也是嘴的道规。母亲常嘱咐我「什么东西都不能吃多」，但你看，考试结束，我路过一家烤番薯，摊主一旁慢悠悠削菠萝，菠萝酸甜爽口，头脑一昏，要来四根。老板连忙感谢，我心生奇怪。一路上我大快朵颐，直到第三根已然不对劲，牙齿酸疼，满口酥麻。是啊，我知道我吃多了，但是当时人已然傻了，没办法，又已然泛起恶心。\n\n美食之美，若天仙之佳，似陋室之雅。雅舍谈吃，谈世俗之赏，谈高雅之堂。南京翠香阁的早茶、北京全聚德的烧鸭，绝不同于广东潮州的街边烂摊子，不带有地域的歧视，不带有阶层的歧视，雅舍或许能尽情在破烂中寻味潮州，若是「破烂」登上大雅之堂的高级餐席，也不感觉一丝违和。但是，真世上能为高价路边摊买单的人已然不多见，多人不敢斗胆，为了金钱的缺憾。别人跟我谈到吃，我更愿意关注在吃本身上，谈钱，色变。但美食不是阶级之物。凡人也能有凡人的美味，大雅之堂也未必不入凡人；大雅食材未必得凡人认可，没有凡人认可也不称美食。\n\n美，有庐山仙境，有墙头杏花，观不可尽收；食，有满汉全席，有菜汤小食，尝不可尽全。却有人生百态，五味杂陈。文章，有百科全书，有一本便笺，若不是看不完，怎么会藏书如山，孜孜不倦。吃饭，若不是吃不完，怎么会馋如饕餮，感想良多。我写的字，就是把味蕾的感想说出来。可，不是我的味蕾，是我熙熙攘攘间，听闻的许多味蕾……\n","source":"_posts/xnqb2e.md","raw":"---\n\n## title: 美食之美——《雅舍谈吃》date: 2021-07-06 23:07:04\n\ntags: 美食\ncategories: 生活随笔\nswiper_index: 8\nswiper_desc: 简单好用的 hexo 博客文章置顶插件！\nswiper_cover: /img/weixin.png\n\n```\n美，不尽收；食，不尽全。 ——题记\n```\n\n谈到美食，正如饥肠辘辘的人，心里面急迫等待着吃的味觉。但不是所谓，仅仅为着生存。追寻美食的目标，不像一种世俗的冲动，正如雅舍先生题序「我以为要求美味固是人欲，然而何曾有背于天理？如果天理不包括美味的要求在内，上天之人，在舌头上为什么要生那么多的味蕾」。\n   吃得好，是让人幸福，是对舌尖感受的升华，一旦提到耳熟能详的菜名，人的愉悦便像剑拔弩张，舌尖一触，百般馋舌。左右结构的「馋」字，右边两点比喻两腿迅猛，迅猛狡兔之肉，并非丰足，但人为了啖其美味，愿意逐此狡兔，故写作馋。西施舌、火腿、醋溜鱼、烤羊肉、烧鸭……谁若吃一口，也不至于在脑袋里臆想，两腿一跺，手掌一直，啪的一下，就算有事缠身也无暇顾及。然而，追求高级的味觉，就必然失去点东西，尤其时间宝贵，未必能每次都放纵自己。\n\n今年除夕，我从繁忙学业中抽身，冒着病毒的风险，踉跄到广东家中度过佳节。目的尚且鲜明，写在文章也就印证我所说为了吃美食。老母亲的美味已经不是天天所得，不回一趟家，不吃慈母饭。而明年除夕，已经计划好的美食清单，都在湖南临武，是为了外婆欢庆大寿，为了庆幸生能继续享用湘菜美味。诚然，计划是幸福的计划，我学业若是耽误，便影响我母亲回乡的心情。\n\n学业之中，不代表就缺乏美味。学校前门小吃街，第一条正中间的店面以「炒」为技艺，炒粉丝、炒饭、炒土豆粉、炒米线……每份炒制不少料、不过熟，香味浓郁滑入咽喉。旁边五碗小菜随取，勺筷俱全，好不心爽幸福。校外也有全州拌饭，馋嘴烤鱼、淮南牛肉汤、鸡蛋灌饼、特制酸奶等等玲琅满，如饕餮大餐，本人如数家珍，乐子甚广。\n\n但不是美食就令人幸福的，美食反被美食误，吃得不好，有可能还要怪罪一下美食。\n\n美食为了尝而点，而不是为了点而尝。打开一个外卖软件，铺天盖地的优惠券、广告条、满减促销，全然改变了美食服务的本质。美食不是为了优惠而美，但你看，我想起 26 元的双层牛肉芝士汉堡，打开外卖软件，又开始送我优惠券，一张是满 27 减 5，一张是满 80 减 9。我心念的汉堡计划被搁置一旁。现在，我购物车换了又换，想法改了又改，点了一家鸡架，满减很高，大份鸡架配油饼、薯条、龙串、千叶只要 81，满减后 43，优惠券折至 34，天啊，真捡大便宜了，一个人，母亲常骂我浪费，也怕生冷不好吃，终于胃饱难入。于是，半个月后的一天，我说：「我心心念念的大汉堡还没吃到呢」，于是又习惯地，打开了外卖软件……\n\n啖美味的人是得爽口，而不是失口德。点到为止，也是嘴的道规。母亲常嘱咐我「什么东西都不能吃多」，但你看，考试结束，我路过一家烤番薯，摊主一旁慢悠悠削菠萝，菠萝酸甜爽口，头脑一昏，要来四根。老板连忙感谢，我心生奇怪。一路上我大快朵颐，直到第三根已然不对劲，牙齿酸疼，满口酥麻。是啊，我知道我吃多了，但是当时人已然傻了，没办法，又已然泛起恶心。\n\n美食之美，若天仙之佳，似陋室之雅。雅舍谈吃，谈世俗之赏，谈高雅之堂。南京翠香阁的早茶、北京全聚德的烧鸭，绝不同于广东潮州的街边烂摊子，不带有地域的歧视，不带有阶层的歧视，雅舍或许能尽情在破烂中寻味潮州，若是「破烂」登上大雅之堂的高级餐席，也不感觉一丝违和。但是，真世上能为高价路边摊买单的人已然不多见，多人不敢斗胆，为了金钱的缺憾。别人跟我谈到吃，我更愿意关注在吃本身上，谈钱，色变。但美食不是阶级之物。凡人也能有凡人的美味，大雅之堂也未必不入凡人；大雅食材未必得凡人认可，没有凡人认可也不称美食。\n\n美，有庐山仙境，有墙头杏花，观不可尽收；食，有满汉全席，有菜汤小食，尝不可尽全。却有人生百态，五味杂陈。文章，有百科全书，有一本便笺，若不是看不完，怎么会藏书如山，孜孜不倦。吃饭，若不是吃不完，怎么会馋如饕餮，感想良多。我写的字，就是把味蕾的感想说出来。可，不是我的味蕾，是我熙熙攘攘间，听闻的许多味蕾……\n","slug":"xnqb2e","published":1,"date":"2021-09-27T07:04:30.558Z","updated":"2021-09-27T07:04:30.558Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cku2b35jh000bd1a256a00rlo","content":"<hr>\n<h2 id=\"title-美食之美——《雅舍谈吃》date-2021-07-06-23-07-04\"><a href=\"#title-美食之美——《雅舍谈吃》date-2021-07-06-23-07-04\" class=\"headerlink\" title=\"title: 美食之美——《雅舍谈吃》date: 2021-07-06 23:07:04\"></a>title: 美食之美——《雅舍谈吃》date: 2021-07-06 23:07:04</h2><p>tags: 美食<br>categories: 生活随笔<br>swiper_index: 8<br>swiper_desc: 简单好用的 hexo 博客文章置顶插件！<br>swiper_cover: /img/weixin.png</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">美，不尽收；食，不尽全。 ——题记</span><br></pre></td></tr></table></figure>\n\n<p>谈到美食，正如饥肠辘辘的人，心里面急迫等待着吃的味觉。但不是所谓，仅仅为着生存。追寻美食的目标，不像一种世俗的冲动，正如雅舍先生题序「我以为要求美味固是人欲，然而何曾有背于天理？如果天理不包括美味的要求在内，上天之人，在舌头上为什么要生那么多的味蕾」。<br>   吃得好，是让人幸福，是对舌尖感受的升华，一旦提到耳熟能详的菜名，人的愉悦便像剑拔弩张，舌尖一触，百般馋舌。左右结构的「馋」字，右边两点比喻两腿迅猛，迅猛狡兔之肉，并非丰足，但人为了啖其美味，愿意逐此狡兔，故写作馋。西施舌、火腿、醋溜鱼、烤羊肉、烧鸭……谁若吃一口，也不至于在脑袋里臆想，两腿一跺，手掌一直，啪的一下，就算有事缠身也无暇顾及。然而，追求高级的味觉，就必然失去点东西，尤其时间宝贵，未必能每次都放纵自己。</p>\n<p>今年除夕，我从繁忙学业中抽身，冒着病毒的风险，踉跄到广东家中度过佳节。目的尚且鲜明，写在文章也就印证我所说为了吃美食。老母亲的美味已经不是天天所得，不回一趟家，不吃慈母饭。而明年除夕，已经计划好的美食清单，都在湖南临武，是为了外婆欢庆大寿，为了庆幸生能继续享用湘菜美味。诚然，计划是幸福的计划，我学业若是耽误，便影响我母亲回乡的心情。</p>\n<p>学业之中，不代表就缺乏美味。学校前门小吃街，第一条正中间的店面以「炒」为技艺，炒粉丝、炒饭、炒土豆粉、炒米线……每份炒制不少料、不过熟，香味浓郁滑入咽喉。旁边五碗小菜随取，勺筷俱全，好不心爽幸福。校外也有全州拌饭，馋嘴烤鱼、淮南牛肉汤、鸡蛋灌饼、特制酸奶等等玲琅满，如饕餮大餐，本人如数家珍，乐子甚广。</p>\n<p>但不是美食就令人幸福的，美食反被美食误，吃得不好，有可能还要怪罪一下美食。</p>\n<p>美食为了尝而点，而不是为了点而尝。打开一个外卖软件，铺天盖地的优惠券、广告条、满减促销，全然改变了美食服务的本质。美食不是为了优惠而美，但你看，我想起 26 元的双层牛肉芝士汉堡，打开外卖软件，又开始送我优惠券，一张是满 27 减 5，一张是满 80 减 9。我心念的汉堡计划被搁置一旁。现在，我购物车换了又换，想法改了又改，点了一家鸡架，满减很高，大份鸡架配油饼、薯条、龙串、千叶只要 81，满减后 43，优惠券折至 34，天啊，真捡大便宜了，一个人，母亲常骂我浪费，也怕生冷不好吃，终于胃饱难入。于是，半个月后的一天，我说：「我心心念念的大汉堡还没吃到呢」，于是又习惯地，打开了外卖软件……</p>\n<p>啖美味的人是得爽口，而不是失口德。点到为止，也是嘴的道规。母亲常嘱咐我「什么东西都不能吃多」，但你看，考试结束，我路过一家烤番薯，摊主一旁慢悠悠削菠萝，菠萝酸甜爽口，头脑一昏，要来四根。老板连忙感谢，我心生奇怪。一路上我大快朵颐，直到第三根已然不对劲，牙齿酸疼，满口酥麻。是啊，我知道我吃多了，但是当时人已然傻了，没办法，又已然泛起恶心。</p>\n<p>美食之美，若天仙之佳，似陋室之雅。雅舍谈吃，谈世俗之赏，谈高雅之堂。南京翠香阁的早茶、北京全聚德的烧鸭，绝不同于广东潮州的街边烂摊子，不带有地域的歧视，不带有阶层的歧视，雅舍或许能尽情在破烂中寻味潮州，若是「破烂」登上大雅之堂的高级餐席，也不感觉一丝违和。但是，真世上能为高价路边摊买单的人已然不多见，多人不敢斗胆，为了金钱的缺憾。别人跟我谈到吃，我更愿意关注在吃本身上，谈钱，色变。但美食不是阶级之物。凡人也能有凡人的美味，大雅之堂也未必不入凡人；大雅食材未必得凡人认可，没有凡人认可也不称美食。</p>\n<p>美，有庐山仙境，有墙头杏花，观不可尽收；食，有满汉全席，有菜汤小食，尝不可尽全。却有人生百态，五味杂陈。文章，有百科全书，有一本便笺，若不是看不完，怎么会藏书如山，孜孜不倦。吃饭，若不是吃不完，怎么会馋如饕餮，感想良多。我写的字，就是把味蕾的感想说出来。可，不是我的味蕾，是我熙熙攘攘间，听闻的许多味蕾……</p>\n","site":{"data":{}},"cover":"/img/loading.gif","length":1698,"excerpt":"","more":"<hr>\n<h2 id=\"title-美食之美——《雅舍谈吃》date-2021-07-06-23-07-04\"><a href=\"#title-美食之美——《雅舍谈吃》date-2021-07-06-23-07-04\" class=\"headerlink\" title=\"title: 美食之美——《雅舍谈吃》date: 2021-07-06 23:07:04\"></a>title: 美食之美——《雅舍谈吃》date: 2021-07-06 23:07:04</h2><p>tags: 美食<br>categories: 生活随笔<br>swiper_index: 8<br>swiper_desc: 简单好用的 hexo 博客文章置顶插件！<br>swiper_cover: /img/weixin.png</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">美，不尽收；食，不尽全。 ——题记</span><br></pre></td></tr></table></figure>\n\n<p>谈到美食，正如饥肠辘辘的人，心里面急迫等待着吃的味觉。但不是所谓，仅仅为着生存。追寻美食的目标，不像一种世俗的冲动，正如雅舍先生题序「我以为要求美味固是人欲，然而何曾有背于天理？如果天理不包括美味的要求在内，上天之人，在舌头上为什么要生那么多的味蕾」。<br>   吃得好，是让人幸福，是对舌尖感受的升华，一旦提到耳熟能详的菜名，人的愉悦便像剑拔弩张，舌尖一触，百般馋舌。左右结构的「馋」字，右边两点比喻两腿迅猛，迅猛狡兔之肉，并非丰足，但人为了啖其美味，愿意逐此狡兔，故写作馋。西施舌、火腿、醋溜鱼、烤羊肉、烧鸭……谁若吃一口，也不至于在脑袋里臆想，两腿一跺，手掌一直，啪的一下，就算有事缠身也无暇顾及。然而，追求高级的味觉，就必然失去点东西，尤其时间宝贵，未必能每次都放纵自己。</p>\n<p>今年除夕，我从繁忙学业中抽身，冒着病毒的风险，踉跄到广东家中度过佳节。目的尚且鲜明，写在文章也就印证我所说为了吃美食。老母亲的美味已经不是天天所得，不回一趟家，不吃慈母饭。而明年除夕，已经计划好的美食清单，都在湖南临武，是为了外婆欢庆大寿，为了庆幸生能继续享用湘菜美味。诚然，计划是幸福的计划，我学业若是耽误，便影响我母亲回乡的心情。</p>\n<p>学业之中，不代表就缺乏美味。学校前门小吃街，第一条正中间的店面以「炒」为技艺，炒粉丝、炒饭、炒土豆粉、炒米线……每份炒制不少料、不过熟，香味浓郁滑入咽喉。旁边五碗小菜随取，勺筷俱全，好不心爽幸福。校外也有全州拌饭，馋嘴烤鱼、淮南牛肉汤、鸡蛋灌饼、特制酸奶等等玲琅满，如饕餮大餐，本人如数家珍，乐子甚广。</p>\n<p>但不是美食就令人幸福的，美食反被美食误，吃得不好，有可能还要怪罪一下美食。</p>\n<p>美食为了尝而点，而不是为了点而尝。打开一个外卖软件，铺天盖地的优惠券、广告条、满减促销，全然改变了美食服务的本质。美食不是为了优惠而美，但你看，我想起 26 元的双层牛肉芝士汉堡，打开外卖软件，又开始送我优惠券，一张是满 27 减 5，一张是满 80 减 9。我心念的汉堡计划被搁置一旁。现在，我购物车换了又换，想法改了又改，点了一家鸡架，满减很高，大份鸡架配油饼、薯条、龙串、千叶只要 81，满减后 43，优惠券折至 34，天啊，真捡大便宜了，一个人，母亲常骂我浪费，也怕生冷不好吃，终于胃饱难入。于是，半个月后的一天，我说：「我心心念念的大汉堡还没吃到呢」，于是又习惯地，打开了外卖软件……</p>\n<p>啖美味的人是得爽口，而不是失口德。点到为止，也是嘴的道规。母亲常嘱咐我「什么东西都不能吃多」，但你看，考试结束，我路过一家烤番薯，摊主一旁慢悠悠削菠萝，菠萝酸甜爽口，头脑一昏，要来四根。老板连忙感谢，我心生奇怪。一路上我大快朵颐，直到第三根已然不对劲，牙齿酸疼，满口酥麻。是啊，我知道我吃多了，但是当时人已然傻了，没办法，又已然泛起恶心。</p>\n<p>美食之美，若天仙之佳，似陋室之雅。雅舍谈吃，谈世俗之赏，谈高雅之堂。南京翠香阁的早茶、北京全聚德的烧鸭，绝不同于广东潮州的街边烂摊子，不带有地域的歧视，不带有阶层的歧视，雅舍或许能尽情在破烂中寻味潮州，若是「破烂」登上大雅之堂的高级餐席，也不感觉一丝违和。但是，真世上能为高价路边摊买单的人已然不多见，多人不敢斗胆，为了金钱的缺憾。别人跟我谈到吃，我更愿意关注在吃本身上，谈钱，色变。但美食不是阶级之物。凡人也能有凡人的美味，大雅之堂也未必不入凡人；大雅食材未必得凡人认可，没有凡人认可也不称美食。</p>\n<p>美，有庐山仙境，有墙头杏花，观不可尽收；食，有满汉全席，有菜汤小食，尝不可尽全。却有人生百态，五味杂陈。文章，有百科全书，有一本便笺，若不是看不完，怎么会藏书如山，孜孜不倦。吃饭，若不是吃不完，怎么会馋如饕餮，感想良多。我写的字，就是把味蕾的感想说出来。可，不是我的味蕾，是我熙熙攘攘间，听闻的许多味蕾……</p>\n"},{"_content":"---\n\n## title: GISer 的思想 date: 2021-07-12 21:49:50\n\ntags: [GIS, GISer, 思想]\ncategories: 地信原理特辑\n\n1.ESRI 出版的《Modeling Our World》讲解了成熟的 GIS 思想方法\n\n2.GIS 相比传统制图工具：\n\n• 制图工具只能在不同维度制作可视的现实还原，无法展开空间分析。GIS 相当于传统制图的下一发展阶段，为整个世界创造了空间分析的可视化制图方法。值得一提的是，以 CAD 为代表的传统制图和 GIS 齐头并进，各有千秋，应当进一步融合。 3.分层概念：\n\n• Feature In FeatureClass：一座城市有许多独立的、多种类的水厂，整个城市的各种水厂形成单独的群体。这就像当于面向对象中抽象的概念，各种 feature 继承了 featureClass 基类，但是 feature 和 featureClass 在 GIS 中都是作为独立一层存储的。\n\n4.抽象简化：\n\n• 空间抽象：现实信息组织入虚拟一般有抽象简化的过程，空间万物都是立体的，但是可以抽象成点线面存储在 GIS 中。\n\n• 属性抽象：就像猫狗都是生物但是不同种类一般，不同的空间要素不能在同一图层，因为字段组织不一致。但是他们抽象出来的特征可能一致，即可能属于同一图层。\n\n![](GISer%E7%9A%84%E6%80%9D%E6%83%B3/1.png#alt=%E5%9B%BE1)\n","source":"_posts/xyq9rx.md","raw":"---\n\n## title: GISer 的思想 date: 2021-07-12 21:49:50\n\ntags: [GIS, GISer, 思想]\ncategories: 地信原理特辑\n\n1.ESRI 出版的《Modeling Our World》讲解了成熟的 GIS 思想方法\n\n2.GIS 相比传统制图工具：\n\n• 制图工具只能在不同维度制作可视的现实还原，无法展开空间分析。GIS 相当于传统制图的下一发展阶段，为整个世界创造了空间分析的可视化制图方法。值得一提的是，以 CAD 为代表的传统制图和 GIS 齐头并进，各有千秋，应当进一步融合。 3.分层概念：\n\n• Feature In FeatureClass：一座城市有许多独立的、多种类的水厂，整个城市的各种水厂形成单独的群体。这就像当于面向对象中抽象的概念，各种 feature 继承了 featureClass 基类，但是 feature 和 featureClass 在 GIS 中都是作为独立一层存储的。\n\n4.抽象简化：\n\n• 空间抽象：现实信息组织入虚拟一般有抽象简化的过程，空间万物都是立体的，但是可以抽象成点线面存储在 GIS 中。\n\n• 属性抽象：就像猫狗都是生物但是不同种类一般，不同的空间要素不能在同一图层，因为字段组织不一致。但是他们抽象出来的特征可能一致，即可能属于同一图层。\n\n![](GISer%E7%9A%84%E6%80%9D%E6%83%B3/1.png#alt=%E5%9B%BE1)\n","slug":"xyq9rx","published":1,"date":"2021-09-27T07:04:30.387Z","updated":"2021-09-27T07:04:30.387Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cku2b35jh000cd1a24r8fbmc8","content":"<hr>\n<h2 id=\"title-GISer-的思想-date-2021-07-12-21-49-50\"><a href=\"#title-GISer-的思想-date-2021-07-12-21-49-50\" class=\"headerlink\" title=\"title: GISer 的思想 date: 2021-07-12 21:49:50\"></a>title: GISer 的思想 date: 2021-07-12 21:49:50</h2><p>tags: [GIS, GISer, 思想]<br>categories: 地信原理特辑</p>\n<p>1.ESRI 出版的《Modeling Our World》讲解了成熟的 GIS 思想方法</p>\n<p>2.GIS 相比传统制图工具：</p>\n<p>• 制图工具只能在不同维度制作可视的现实还原，无法展开空间分析。GIS 相当于传统制图的下一发展阶段，为整个世界创造了空间分析的可视化制图方法。值得一提的是，以 CAD 为代表的传统制图和 GIS 齐头并进，各有千秋，应当进一步融合。 3.分层概念：</p>\n<p>• Feature In FeatureClass：一座城市有许多独立的、多种类的水厂，整个城市的各种水厂形成单独的群体。这就像当于面向对象中抽象的概念，各种 feature 继承了 featureClass 基类，但是 feature 和 featureClass 在 GIS 中都是作为独立一层存储的。</p>\n<p>4.抽象简化：</p>\n<p>• 空间抽象：现实信息组织入虚拟一般有抽象简化的过程，空间万物都是立体的，但是可以抽象成点线面存储在 GIS 中。</p>\n<p>• 属性抽象：就像猫狗都是生物但是不同种类一般，不同的空间要素不能在同一图层，因为字段组织不一致。但是他们抽象出来的特征可能一致，即可能属于同一图层。</p>\n<p><img src=\"/2021/09/27/xyq9rx/1.png#alt=%E5%9B%BE1\"></p>\n","site":{"data":{}},"cover":"/img/loading.gif","length":520,"excerpt":"","more":"<hr>\n<h2 id=\"title-GISer-的思想-date-2021-07-12-21-49-50\"><a href=\"#title-GISer-的思想-date-2021-07-12-21-49-50\" class=\"headerlink\" title=\"title: GISer 的思想 date: 2021-07-12 21:49:50\"></a>title: GISer 的思想 date: 2021-07-12 21:49:50</h2><p>tags: [GIS, GISer, 思想]<br>categories: 地信原理特辑</p>\n<p>1.ESRI 出版的《Modeling Our World》讲解了成熟的 GIS 思想方法</p>\n<p>2.GIS 相比传统制图工具：</p>\n<p>• 制图工具只能在不同维度制作可视的现实还原，无法展开空间分析。GIS 相当于传统制图的下一发展阶段，为整个世界创造了空间分析的可视化制图方法。值得一提的是，以 CAD 为代表的传统制图和 GIS 齐头并进，各有千秋，应当进一步融合。 3.分层概念：</p>\n<p>• Feature In FeatureClass：一座城市有许多独立的、多种类的水厂，整个城市的各种水厂形成单独的群体。这就像当于面向对象中抽象的概念，各种 feature 继承了 featureClass 基类，但是 feature 和 featureClass 在 GIS 中都是作为独立一层存储的。</p>\n<p>4.抽象简化：</p>\n<p>• 空间抽象：现实信息组织入虚拟一般有抽象简化的过程，空间万物都是立体的，但是可以抽象成点线面存储在 GIS 中。</p>\n<p>• 属性抽象：就像猫狗都是生物但是不同种类一般，不同的空间要素不能在同一图层，因为字段组织不一致。但是他们抽象出来的特征可能一致，即可能属于同一图层。</p>\n<p><img src=\"/2021/09/27/xyq9rx/1.png#alt=%E5%9B%BE1\"></p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[{"name":"深度学习","_id":"cku21gy7p0005qua24la8047u"},{"name":"语义分割","_id":"cku21gy7t000cqua23mwebsxz"},{"name":"Anaconda","_id":"cku21gy7w000hqua2ehyeeqsu"},{"name":"Python","_id":"cku21gy82000mqua2at5i9si8"},{"name":"GIS","_id":"cku21gy86000rqua2gm3xejw9"},{"name":"GISer","_id":"cku21gy87000xqua2abtycd05"},{"name":"思想","_id":"cku21gy870012qua2cxj7ccmv"},{"name":"Keras","_id":"cku21gy8a001mqua27jja972k"},{"name":"Pytorch","_id":"cku21gy8b001pqua27pjh9ycc"},{"name":"ONNX","_id":"cku21gy8b001squa27t5nfb0u"},{"name":"Web开发","_id":"cku21gy8b001vqua21o897amw"},{"name":"可视化","_id":"cku21gy8c001zqua22a1o7t79"},{"name":"TensorboardX","_id":"cku21gy8e0022qua2cvmje6ri"},{"name":"数据","_id":"cku21gy8e0025qua20hrk1brq"},{"name":"美食","_id":"cku21gy8k002kqua2btsk5p92"}]}}